{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1062f1c9",
   "metadata": {},
   "source": [
    "# Lifelog Personalization Gatekeeper â€“ Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd6ec7",
   "metadata": {},
   "source": [
    "This notebook shows how to configure dataset paths, prepare prediction frames, and run the gatekeeper scorecards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd906ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[2]\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e3e0d",
   "metadata": {},
   "source": [
    "## Configure cache and dataset roots\n",
    "\n",
    "Override `GATEKEEPER_DATA_ROOT` and `GATEKEEPER_CACHE_DIR` if your datasets live elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9db3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = PROJECT_ROOT / \"memory/lifelog-personalization/datasets\"\n",
    "CACHE_ROOT = PROJECT_ROOT / \"memory/lifelog-personalization/.cache\"\n",
    "\n",
    "os.environ.setdefault(\"GATEKEEPER_DATA_ROOT\", str(DATA_ROOT))\n",
    "os.environ.setdefault(\"GATEKEEPER_CACHE_DIR\", str(CACHE_ROOT))\n",
    "\n",
    "print(\"Data root:\", os.environ[\"GATEKEEPER_DATA_ROOT\"])\n",
    "print(\"Cache root:\", os.environ[\"GATEKEEPER_CACHE_DIR\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f63dd",
   "metadata": {},
   "source": [
    "## Available configs\n",
    "\n",
    "Configs live under `gatekeeper/configs/`. Feel free to copy them into a run-specific directory before editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20442e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = PROJECT_ROOT / \"memory/lifelog-personalization/gatekeeper/configs\"\n",
    "for path in sorted(config_dir.glob(\"*.yaml\")):\n",
    "    print(path.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b693f436",
   "metadata": {},
   "source": [
    "## Build demo predictions\n",
    "\n",
    "Below we synthesize a minimal set of predictions to exercise the lifelog evaluation pipeline. Replace this block with real model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ac38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "config_path = config_dir / \"lifelog.yaml\"\n",
    "print(\"Using config:\", config_path.name)\n",
    "\n",
    "datasets = {\n",
    "    \"lsc24\": [\"kis\", \"adhoc\", \"qa\"],\n",
    "    \"ntcir18_lsat\": [\"dev\", \"test\"],\n",
    "    \"ntcir18_lqat\": [\"dev\"],\n",
    "    \"imageclef20\": [\"test\"],\n",
    "}\n",
    "retrievers = [\"vector_only\", \"graphrag_global\", \"raptor_hierarchy\"]\n",
    "\n",
    "predictions = {}\n",
    "for dataset, splits in datasets.items():\n",
    "    for split in splits:\n",
    "        for retriever in retrievers:\n",
    "            key = f\"{dataset}:{split}:{retriever}\"\n",
    "            rows = []\n",
    "            for idx in range(3):\n",
    "                ranked = [f\"doc_{i}\" for i in range(idx, idx + 5)]\n",
    "                relevant = ranked[:2]\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"query_id\": f\"{dataset}_{split}_{idx}\",\n",
    "                        \"ranked_ids\": ranked,\n",
    "                        \"relevant_ids\": relevant,\n",
    "                        \"timestamp\": \"2024-01-01T12:00:00\",\n",
    "                        \"predicted_timestamp\": \"2024-01-01T12:00:00\",\n",
    "                        \"entities\": [\"alice\", \"office\"],\n",
    "                        \"predicted_entities\": [\"alice\", \"office\"],\n",
    "                    }\n",
    "                )\n",
    "            predictions[key] = pd.DataFrame(rows)\n",
    "print(f\"Prepared {len(predictions)} prediction tables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e04df",
   "metadata": {},
   "source": [
    "## Run evaluation\n",
    "\n",
    "`evaluate_lifelog` returns a list of `EvaluationResult` objects. The helper below renders a Markdown scorecard that you can publish or attach to CI artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory.lifelog_personalization.gatekeeper.runners.run_lifelog import evaluate_lifelog\n",
    "from memory.lifelog_personalization.gatekeeper.reports.render_scorecard import render_scorecard\n",
    "\n",
    "results = evaluate_lifelog(config_path, predictions)\n",
    "for result in results[:3]:\n",
    "    print(result)\n",
    "\n",
    "artifacts_dir = PROJECT_ROOT / \"memory/lifelog-personalization/artifacts\"\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "report_path = artifacts_dir / \"lifelog_demo.md\"\n",
    "render_scorecard(results, title=\"Demo Lifelog Retrieval\", output=report_path)\n",
    "print(\"Scorecard saved to\", report_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1280fd8",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Replace the synthetic predictions with outputs from your retrievers.\n",
    "- Use the other configs (personalization, long-context, TTL, editing) following the same pattern.\n",
    "- Commit the generated scorecards into your evaluation reports or CI artifacts."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}