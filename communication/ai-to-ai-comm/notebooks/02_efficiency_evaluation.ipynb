{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# C2C Efficiency Evaluation\n",
    "\n",
    "**Research Question:** Is latent communication (C2C) faster/cheaper than natural language?\n",
    "\n",
    "**Date:** 2025-11-05\n",
    "\n",
    "**Metrics:**\n",
    "- Token count comparison (C2C vs text)\n",
    "- Latency comparison\n",
    "- Information density\n",
    "- Compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add parent directory to path for code imports\nimport sys\nsys.path.append('../code')\n\n# 2. Import required libraries\nimport time       # For timing measurements\nimport torch      # For PyTorch operations\nimport numpy as np  # For numerical operations\nimport pandas as pd  # For data analysis\nimport matplotlib.pyplot as plt  # For visualization\nfrom transformers import AutoModelForCausalLM, AutoTokenizer  # For model loading\n\n# 3. Import C2C communication modules\nfrom communication.ai_to_ai_comm import (\n    RosettaModel,            # Multi-model orchestrator\n    create_c2c_projector,    # Create projector network\n    generate_kv_cache_index, # Generate cache routing index\n)\n\n# 4. Import harness experiment tracking\nfrom harness import ExperimentConfig, get_tracker\n\n# 5. Check device availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Setup Models and Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts covering different communication scenarios\n",
    "test_prompts = [\n",
    "    \"Explain the concept of machine learning\",\n",
    "    \"Describe the feeling of excitement\",\n",
    "    \"What are the implications of quantum computing?\",\n",
    "    \"Summarize the theory of evolution\",\n",
    "    \"Compare democracy and authoritarianism\",\n",
    "]\n",
    "\n",
    "# Load models (reuse from quickstart or reload)\n",
    "base_model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "source_model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loaded {len(test_prompts)} test prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Benchmark C2C Communication\n",
    "\n",
    "Measure latency and token usage for C2C communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_c2c(rosetta, prompt, max_new_tokens=50):\n",
    "    \"\"\"Benchmark C2C communication for a single prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    cache_idx = generate_kv_cache_index(sequence_length=10, model_idx=1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = rosetta.generate(\n",
    "            cache_idx=cache_idx,\n",
    "            input_ids=inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    tokens_in = len(inputs.input_ids[0])\n",
    "    tokens_out = len(outputs[0]) - tokens_in\n",
    "    \n",
    "    return {\n",
    "        \"latency_s\": latency,\n",
    "        \"tokens_in\": tokens_in,\n",
    "        \"tokens_out\": tokens_out,\n",
    "        \"output_text\": output_text,\n",
    "    }\n",
    "\n",
    "# TODO: Setup RosettaModel (or import from previous notebook)\n",
    "# c2c_results = [benchmark_c2c(rosetta, prompt) for prompt in test_prompts]\n",
    "print(\"C2C benchmarking function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Benchmark Text-Based Communication\n",
    "\n",
    "Baseline: two-step text generation (source -> text -> base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_text_based(source_model, base_model, prompt, max_new_tokens=50):\n",
    "    \"\"\"Benchmark text-based communication.\n",
    "    \n",
    "    Step 1: Source model generates text response\n",
    "    Step 2: Base model processes that text\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Step 1: Source model generates\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        source_outputs = source_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    \n",
    "    intermediate_text = tokenizer.decode(source_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Step 2: Base model processes intermediate text\n",
    "    base_inputs = tokenizer(intermediate_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        base_outputs = base_model.generate(\n",
    "            base_inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    output_text = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    tokens_in = len(inputs.input_ids[0])\n",
    "    tokens_intermediate = len(source_outputs[0])\n",
    "    tokens_out = len(base_outputs[0])\n",
    "    total_tokens = tokens_intermediate + tokens_out\n",
    "    \n",
    "    return {\n",
    "        \"latency_s\": latency,\n",
    "        \"tokens_in\": tokens_in,\n",
    "        \"tokens_out\": tokens_out,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"output_text\": output_text,\n",
    "    }\n",
    "\n",
    "print(\"Text-based benchmarking function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Run Experiments\n",
    "\n",
    "Compare C2C vs text-based communication across all test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run after models are loaded\n",
    "# results = []\n",
    "# \n",
    "# for prompt in test_prompts:\n",
    "#     c2c_result = benchmark_c2c(rosetta, prompt)\n",
    "#     text_result = benchmark_text_based(source_model, base_model, prompt)\n",
    "#     \n",
    "#     results.append({\n",
    "#         \"prompt\": prompt,\n",
    "#         \"c2c_latency\": c2c_result[\"latency_s\"],\n",
    "#         \"text_latency\": text_result[\"latency_s\"],\n",
    "#         \"c2c_tokens\": c2c_result[\"tokens_out\"],\n",
    "#         \"text_tokens\": text_result[\"total_tokens\"],\n",
    "#         \"speedup\": text_result[\"latency_s\"] / c2c_result[\"latency_s\"],\n",
    "#         \"token_savings\": 1 - (c2c_result[\"tokens_out\"] / text_result[\"total_tokens\"]),\n",
    "#     })\n",
    "# \n",
    "# df = pd.DataFrame(results)\n",
    "# print(df)\n",
    "\n",
    "print(\"Experiment code ready (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment after running experiments\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# \n",
    "# # Latency comparison\n",
    "# axes[0].bar(['C2C', 'Text'], [df['c2c_latency'].mean(), df['text_latency'].mean()])\n",
    "# axes[0].set_ylabel('Latency (s)')\n",
    "# axes[0].set_title('Average Latency Comparison')\n",
    "# \n",
    "# # Token usage comparison\n",
    "# axes[1].bar(['C2C', 'Text'], [df['c2c_tokens'].mean(), df['text_tokens'].mean()])\n",
    "# axes[1].set_ylabel('Tokens')\n",
    "# axes[1].set_title('Average Token Usage')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# print(f\"\\nAverage Speedup: {df['speedup'].mean():.2f}x\")\n",
    "# print(f\"Average Token Savings: {df['token_savings'].mean()*100:.1f}%\")\n",
    "\n",
    "print(\"Visualization code ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Track Results\n",
    "\n",
    "Log to harness experiment tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentConfig(\n",
    "    experiment_name=\"c2c_efficiency_eval\",\n",
    "    task_type=\"c2c_evaluation\",\n",
    "    strategy=\"cache_to_cache\",\n",
    "    provider=\"local\",\n",
    "    model=f\"{base_model_name}_to_{source_model_name}\",\n",
    ")\n",
    "\n",
    "tracker = get_tracker()\n",
    "run_dir = tracker.start_experiment(config)\n",
    "\n",
    "# TODO: Log results after experiments run\n",
    "# for idx, row in df.iterrows():\n",
    "#     result = ExperimentResult(\n",
    "#         config=config,\n",
    "#         task_input=row['prompt'],\n",
    "#         output=f\"C2C: {row['c2c_latency']:.2f}s, Text: {row['text_latency']:.2f}s\",\n",
    "#         latency_s=row['c2c_latency'],\n",
    "#         eval_scores={\n",
    "#             \"speedup\": row['speedup'],\n",
    "#             \"token_savings\": row['token_savings'],\n",
    "#         },\n",
    "#         success=True,\n",
    "#     )\n",
    "#     tracker.log_result(result)\n",
    "\n",
    "summary = tracker.finish_experiment()\n",
    "print(f\"Experiment logged in: {run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "**Expected Results:**\n",
    "1. C2C should reduce token count (no intermediate text generation)\n",
    "2. Latency depends on projector efficiency vs text generation speed\n",
    "3. Information density may be higher in C2C (direct semantic transfer)\n",
    "\n",
    "**Research Questions:**\n",
    "- Does C2C provide measurable efficiency gains?\n",
    "- What is the overhead of cache projection vs text tokenization?\n",
    "- Does efficiency vary by task type or concept complexity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}