{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Cache-to-Cache (C2C) Communication - Quickstart\n",
    "\n",
    "**Experiment:** Demonstrate direct semantic communication between LLMs through KV-Cache projection\n",
    "\n",
    "**Date:** 2025-11-05\n",
    "\n",
    "**Research Question:** Can LLMs communicate more efficiently through latent representations (KV-Caches) instead of natural language?\n",
    "\n",
    "**Goals:**\n",
    "- Load and configure two different LLM architectures\n",
    "- Set up C2C projectors for cache transformation\n",
    "- Demonstrate cache-to-cache communication\n",
    "- Compare with baseline text-based communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the C2C communication modules and harness utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add repository root to path to import harness\nimport sys\nsys.path.append('../../../')  # Go up to repo root from notebooks/\n\n# 2. Import PyTorch for neural network operations\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# 3. Import C2C communication modules\nfrom communication.ai_to_ai_comm import (\n    RosettaModel,            # Multi-model C2C orchestrator\n    create_c2c_projector,    # Create KV-cache projector network\n    generate_kv_cache_index, # Generate cache routing index\n    print_cache_stats,       # Print cache usage statistics\n)\n\n# 4. Import harness experiment tracking\nfrom harness import ExperimentConfig, ExperimentResult, get_tracker\nfrom harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n\n# 5. Check for CUDA availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# 6. Show configuration\nprint(\"=\"*70)\nprint(\"ðŸ”§ C2C COMMUNICATION CONFIGURATION\")\nprint(\"=\"*70)\nprint(f\"ðŸ’» Device: {device}\")\nprint(f\"ðŸ“ Default Provider: {DEFAULT_PROVIDER}\")\nprint(f\"ðŸ¤– Default Model: {DEFAULT_MODEL or '(default)'}\")\nprint(\"=\"*70)\nprint(\"\\nNote: This notebook uses HuggingFace models directly\")\nprint(\"Configure models in cell 4 (Load Models section)\")\nprint(\"=\"*70)\n\n# 7. Print environment info\nprint(f\"\\nâœ… PyTorch version: {torch.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Models\n",
    "\n",
    "Load a base model and a source model for C2C communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# 1. Define which models to load (adjust based on available models)\nbase_model_name = \"Qwen/Qwen2-0.5B-Instruct\"    # Smaller base model\nsource_model_name = \"Qwen/Qwen2-1.5B-Instruct\"  # Larger source model\n\n# 2. Print loading status\nprint(f\"Loading models on {device}...\")\n\n# 3. Load tokenizer from base model\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n# 4. Set pad token if not present (required for batching)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# 5. Load base model with appropriate precision\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,  # FP16 on GPU, FP32 on CPU\n    device_map=device,  # Map to device\n)\n\n# 6. Load source model with appropriate precision\nsource_model = AutoModelForCausalLM.from_pretrained(\n    source_model_name,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device_map=device,\n)\n\n# 7. Set models to evaluation mode (disable dropout)\nbase_model.eval()\nsource_model.eval()\n\n# 8. Print model architectures\nprint(f\"\\nBase model: {base_model_name}\")\nprint(f\"  Layers: {base_model.config.num_hidden_layers}\")\nprint(f\"  Hidden size: {base_model.config.hidden_size}\")\n\nprint(f\"\\nSource model: {source_model_name}\")\nprint(f\"  Layers: {source_model.config.num_hidden_layers}\")\nprint(f\"  Hidden size: {source_model.config.hidden_size}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Create C2C Projectors\n",
    "\n",
    "Set up neural network projectors to transform KV-Caches between model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating C2C projectors...\")\n",
    "\n",
    "num_layers = base_model.config.num_hidden_layers\n",
    "projector_list = []\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    projector = create_c2c_projector(\n",
    "        source_model.config,\n",
    "        base_model.config,\n",
    "        hidden_dim=1024,\n",
    "        intermediate_dim=2048,\n",
    "        num_layers=3,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "    projector = projector.to(device)\n",
    "    projector_list.append(projector)\n",
    "\n",
    "print(f\"Created {len(projector_list)} projectors\")\n",
    "print(f\"Projector architecture: {projector_list[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Create RosettaModel\n",
    "\n",
    "Orchestrate multi-model cache-to-cache communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up RosettaModel...\")\n",
    "\n",
    "rosetta = RosettaModel(\n",
    "    [base_model, source_model],\n",
    "    projector_list=projector_list,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Configure which layers communicate\n",
    "# Map source model layer 0 -> base model layer 0\n",
    "for layer_idx in range(num_layers):\n",
    "    rosetta.set_projector_config(\n",
    "        projector_idx=layer_idx,\n",
    "        source_model_idx=1,  # source model\n",
    "        source_layer_idx=layer_idx,\n",
    "        target_model_idx=0,  # base model\n",
    "        target_layer_idx=layer_idx,\n",
    "    )\n",
    "\n",
    "print(\"RosettaModel configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Test C2C Communication\n",
    "\n",
    "Send a concept from source model to base model via cache projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test prompt\n",
    "test_prompt = \"The concept of artificial intelligence involves\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate cache index (use source model for first 10 tokens, then switch to base)\n",
    "cache_idx = generate_kv_cache_index(sequence_length=10, model_idx=1)\n",
    "\n",
    "print(f\"Test prompt: {test_prompt}\")\n",
    "print(f\"Cache index shape: {cache_idx.shape}\")\n",
    "print_cache_stats(cache_idx)\n",
    "\n",
    "# Generate with C2C communication\n",
    "with torch.no_grad():\n",
    "    outputs = rosetta.generate(\n",
    "        cache_idx=cache_idx,\n",
    "        input_ids=inputs.input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nGenerated text (with C2C):\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Baseline Comparison\n",
    "\n",
    "Compare C2C communication with standard text-based generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline (base model only):\")\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = base_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "baseline_text = tokenizer.decode(baseline_outputs[0], skip_special_tokens=True)\n",
    "print(baseline_text)\n",
    "\n",
    "print(\"\\nSource model only:\")\n",
    "with torch.no_grad():\n",
    "    source_outputs = source_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "source_text = tokenizer.decode(source_outputs[0], skip_special_tokens=True)\n",
    "print(source_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Track Experiment\n",
    "\n",
    "Log results using the harness experiment tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentConfig(\n",
    "    experiment_name=\"c2c_quickstart\",\n",
    "    task_type=\"c2c_communication\",\n",
    "    strategy=\"cache_to_cache\",\n",
    "    provider=\"local\",\n",
    "    model=f\"{base_model_name}_to_{source_model_name}\",\n",
    ")\n",
    "\n",
    "tracker = get_tracker()\n",
    "run_dir = tracker.start_experiment(config)\n",
    "\n",
    "result = ExperimentResult(\n",
    "    config=config,\n",
    "    task_input=test_prompt,\n",
    "    output=generated_text,\n",
    "    latency_s=0.0,  # TODO: measure actual latency\n",
    "    tokens_in=len(inputs.input_ids[0]),\n",
    "    tokens_out=len(outputs[0]) - len(inputs.input_ids[0]),\n",
    "    cost_usd=0.0,\n",
    "    eval_scores={},\n",
    "    eval_metadata={\n",
    "        \"base_model\": base_model_name,\n",
    "        \"source_model\": source_model_name,\n",
    "        \"num_projectors\": len(projector_list),\n",
    "        \"c2c_output\": generated_text,\n",
    "        \"baseline_output\": baseline_text,\n",
    "        \"source_output\": source_text,\n",
    "    },\n",
    "    success=True,\n",
    "    error=None,\n",
    ")\n",
    "\n",
    "tracker.log_result(result)\n",
    "summary = tracker.finish_experiment()\n",
    "print(f\"Experiment logged in: {run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Efficiency Analysis** (02_efficiency_comparison.ipynb): Compare token count, latency, and information density\n",
    "2. **Protocol Development** (03_communication_protocols.ipynb): Develop compression and error correction\n",
    "3. **Cross-Model Testing** (04_cross_model_generalization.ipynb): Test across different architectures\n",
    "\n",
    "## References\n",
    "\n",
    "- Cache-to-Cache paper: https://arxiv.org/abs/2510.03215\n",
    "- Project documentation: `/communication/ai-to-ai-comm/CLAUDE.md`\n",
    "- C2C README: `/communication/ai-to-ai-comm/C2C_README.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}