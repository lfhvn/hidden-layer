{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent vs Single Model Comparison\n\n",
    "**Experiment:** Compare debate and manager-worker strategies to single-model baseline\n\n",
    "**Date:** 2025-10-26\n\n",
    "**Goals:**\n",
    "- Run same tasks with multiple strategies\n",
    "- Compare accuracy, latency, cost\n",
    "- Identify where multi-agent helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add parent directory to path for code imports\nimport sys\nsys.path.append('../code')\n\n# 2. Import harness functions for running strategies and tracking experiments\nfrom harness import (\n    run_strategy,          # Run any multi-agent or single-agent strategy\n    ExperimentConfig,      # Configuration for experiment tracking\n    ExperimentResult,      # Structure for logging results\n    get_tracker,           # Get experiment tracker instance\n    compare_experiments    # Compare multiple experiment runs\n)\n\n# 3. Import pandas for data manipulation and matplotlib for plotting\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Define a test reasoning task as a dictionary\ntask = {\n    'id': 'reasoning_001',                                        # Unique task identifier\n    'input': 'A farmer has 17 sheep. All but 9 die. How many are left?',  # Trick question\n    'type': 'reasoning'                                           # Task category\n}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Single Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Configure experiment for single model baseline\nconfig_single = ExperimentConfig(\n    experiment_name='single_baseline',   # Name for this experiment\n    task_type='reasoning',               # Type of task being tested\n    strategy='single',                   # Using single-model strategy\n    provider='ollama'                    # Using Ollama local provider\n)\n\n# 2. Initialize experiment tracker\ntracker = get_tracker()\ntracker.start_experiment(config_single)\n\n# 3. Run single model strategy on the task\nresult_single = run_strategy(\n    'single',              # Strategy name\n    task['input'],         # Input text from task\n    provider='ollama'      # Provider to use\n)\n\n# 4. Print results for inspection\nprint(f\"Output: {result_single.output}\")\nprint(f\"Latency: {result_single.latency_s:.2f}s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Debate Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Run debate strategy with 3 debaters\nresult_debate = run_strategy(\n    'debate',              # Use debate strategy (2+ agents argue, judge decides)\n    task['input'],         # Same task input as baseline\n    n_debaters=3,          # Number of agents that will debate\n    provider='ollama'      # Use Ollama provider\n)\n\n# 2. Print the final output from the judge\nprint(f\"Output: {result_debate.output}\")\n\n# 3. Print latency (will be higher than single model)\nprint(f\"Latency: {result_debate.latency_s:.2f}s\")\n\n# 4. Print the individual arguments from each debater\nprint(f\"\\nDebater arguments:\")\nfor i, arg in enumerate(result_debate.metadata['arguments']):\n    print(f\"{i+1}. {arg[:100]}...\")  # Print first 100 chars of each argument"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Create a pandas DataFrame to compare the two strategies\ncomparison = pd.DataFrame([\n    {\n        'strategy': 'single',                                           # Single model baseline\n        'latency_s': result_single.latency_s,                          # Time taken in seconds\n        'tokens': result_single.tokens_in + result_single.tokens_out,  # Total tokens used\n        'cost': result_single.cost_usd                                  # Cost in USD\n    },\n    {\n        'strategy': 'debate',                                           # Multi-agent debate\n        'latency_s': result_debate.latency_s,                          # Time taken\n        'tokens': result_debate.tokens_in + result_debate.tokens_out,  # Total tokens\n        'cost': result_debate.cost_usd                                  # Cost\n    }\n])\n\n# 2. Print the comparison table\nprint(comparison)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}