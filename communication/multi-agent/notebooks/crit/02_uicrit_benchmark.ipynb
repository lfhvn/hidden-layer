{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Design Critique with UICrit\n",
    "\n",
    "This notebook shows how to use the **UICrit** benchmark dataset with CRIT.\n",
    "\n",
    "## About UICrit\n",
    "\n",
    "**UICrit** (UIST 2024, Google Research) contains:\n",
    "- 11,344 design critiques\n",
    "- 1,000 mobile UIs from RICO dataset\n",
    "- Expert human critiques from 7 experienced designers\n",
    "- LLM-generated critiques for comparison\n",
    "- Quality ratings (aesthetics, usability, learnability, efficiency)\n",
    "\n",
    "## Why Use UICrit?\n",
    "\n",
    "- **Compare to Experts**: See how your critiques match professional designers\n",
    "- **Quality Benchmarking**: Validate against quality ratings\n",
    "- **Large Scale**: Test on 1,000 real mobile UIs\n",
    "- **LLM Baseline**: Compare against existing LLM critiques\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add repository root to path to import modules\nimport sys\nsys.path.append('../../../../')  # Go up to repo root from notebooks/crit/\n\n# 2. Add local code directory for CRIT-specific code\nsys.path.append('../../code')  # Add multi-agent/code for crit module\n\n# 3. Import CRIT benchmark loading functions\nfrom crit import (\n    # Benchmark loaders\n    load_uicrit,                  # Load UICrit dataset\n    load_uicrit_for_comparison,   # Load UICrit with comparison data\n    print_benchmark_info,          # Print info about available benchmarks\n    compare_to_experts,            # Compare critique to expert critiques\n    \n    # Critique strategies\n    run_critique_strategy,         # Run any critique strategy\n    single_critic_strategy,        # Single critic\n    multi_perspective_critique,    # Multi-perspective\n    \n    # Evaluation\n    evaluate_critique,             # Evaluate critique quality\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Benchmark Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Print information about available benchmarks\n# This shows which benchmarks are available and how to install them\nprint_benchmark_info()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load UICrit Dataset\n",
    "\n",
    "First, clone the repository:\n",
    "```bash\n",
    "git clone https://github.com/google-research-datasets/uicrit.git\n",
    "```\n",
    "\n",
    "The CSV will be at: `./uicrit/uicrit_public.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Attempt to load the UICrit dataset\ntry:\n    # 2. Load dataset with specified parameters\n    uicrit = load_uicrit(\n        min_quality_rating=None,    # No filtering by quality\n        include_llm_critiques=True  # Include LLM-generated critiques for comparison\n    )\n    \n    # 3. Print dataset summary\n    print(f\"Loaded UICrit Dataset\")\n    print(f\"Source: {uicrit.source}\")\n    \n    # 4. Print statistics\n    print(f\"\\nStatistics:\")\n    print(f\"  UI Screens: {uicrit.metadata['ui_screens']}\")\n    print(f\"  Total Critiques: {uicrit.metadata['total_critiques']}\")\n    print(f\"  License: {uicrit.metadata['license']}\")\n    \n    # 5. Show a sample problem\n    print(f\"\\nSample UI Problem:\")\n    sample = uicrit.problems[0]\n    print(f\"RICO ID: {sample.metadata['rico_id']}\")\n    print(f\"Task: {sample.metadata['task']}\")\n    print(f\"Avg Quality: {sample.metadata['avg_quality']:.1f}/10\")\n    print(f\"Num Critiques: {sample.metadata['num_critiques']}\")\n    \n# 6. Handle case where UICrit is not installed\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}\")\n    print(\"\\nPlease install UICrit first:\")\n    print(\"git clone https://github.com/google-research-datasets/uicrit.git\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Run Your Critique Strategy on UICrit UIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample UI to critique\n",
    "try:\n",
    "    sample_problem = uicrit.problems[0]\n",
    "    \n",
    "    print(f\"Critiquing UI: {sample_problem.name}\")\n",
    "    print(f\"Task: {sample_problem.metadata['task']}\")\n",
    "    print(f\"\\nDesign Problem:\")\n",
    "    print(sample_problem.current_design)\n",
    "    \n",
    "    # Run your critique strategy\n",
    "    print(f\"\\nRunning multi-perspective critique...\")\n",
    "    your_critique = run_critique_strategy(\n",
    "        \"multi_perspective\",\n",
    "        sample_problem,\n",
    "        provider=\"ollama\",\n",
    "        synthesize=True,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nYOUR CRITIQUE:\")\n",
    "    print(\"=\"*60)\n",
    "    print(your_critique.synthesis)\n",
    "    print(f\"\\nRecommendations ({len(your_critique.recommendations)}):\")\n",
    "    for i, rec in enumerate(your_critique.recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"UICrit not loaded. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Compare to Expert Critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UICrit for comparison\n",
    "try:\n",
    "    comparison_data = load_uicrit_for_comparison(\n",
    "        sample_size=10  # Just 10 UIs for demo\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {len(comparison_data)} UIs for comparison\")\n",
    "    \n",
    "    # Show expert critiques for first UI\n",
    "    first_ui = comparison_data[0]\n",
    "    print(f\"\\nUI: {first_ui['task']}\")\n",
    "    print(f\"Quality Rating: {first_ui['quality_ratings']['design_quality']:.1f}/10\")\n",
    "    print(f\"\\nExpert Critiques ({len(first_ui['expert_critiques'])}):\")\n",
    "    for i, critique in enumerate(first_ui['expert_critiques'][:3], 1):\n",
    "        print(f\"{i}. {critique}\")\n",
    "    \n",
    "    if first_ui['llm_critiques']:\n",
    "        print(f\"\\nLLM Critiques ({len(first_ui['llm_critiques'])}):\")\n",
    "        for i, critique in enumerate(first_ui['llm_critiques'][:2], 1):\n",
    "            print(f\"{i}. {critique}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Systematic Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your strategy on multiple UIs and compare to experts\n",
    "try:\n",
    "    results = []\n",
    "    \n",
    "    # Test on first 5 UIs\n",
    "    for i, item in enumerate(comparison_data[:5], 1):\n",
    "        print(f\"\\nProcessing UI {i}/5: {item['task']}\")\n",
    "        \n",
    "        # Run your critique\n",
    "        your_critique = run_critique_strategy(\n",
    "            \"single\",  # Use single for faster testing\n",
    "            item['problem'],\n",
    "            provider=\"ollama\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        # Extract your critique text\n",
    "        your_critique_text = your_critique.synthesis or \"\"\n",
    "        your_recommendations = your_critique.recommendations\n",
    "        \n",
    "        # Compare to expert critiques\n",
    "        comparison = compare_to_experts(\n",
    "            your_recommendations,\n",
    "            item['expert_critiques'],\n",
    "            method=\"overlap\"\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'rico_id': item['rico_id'],\n",
    "            'task': item['task'],\n",
    "            'quality': item['quality_ratings']['design_quality'],\n",
    "            'your_critique': your_critique,\n",
    "            'expert_critiques': item['expert_critiques'],\n",
    "            'comparison': comparison\n",
    "        })\n",
    "        \n",
    "        print(f\"  Overlap with experts: {comparison['overlap_score']:.2%}\")\n",
    "    \n",
    "    # Aggregate results\n",
    "    avg_overlap = sum(r['comparison']['overlap_score'] for r in results) / len(results)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Average Overlap with Experts: {avg_overlap:.2%}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Comparison data not loaded. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Evaluate Critique Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your critique quality using CRIT metrics\n",
    "try:\n",
    "    if results:\n",
    "        sample_result = results[0]\n",
    "        \n",
    "        print(f\"Evaluating critique for: {sample_result['task']}\\n\")\n",
    "        \n",
    "        evaluation = evaluate_critique(\n",
    "            sample_result['your_critique'].metadata['problem'],\n",
    "            sample_result['your_critique'],\n",
    "            method=\"combined\",\n",
    "            judge_provider=\"ollama\"\n",
    "        )\n",
    "        \n",
    "        print(\"Evaluation Results:\")\n",
    "        print(f\"  Coverage Score: {evaluation['coverage']['overall_coverage']:.2f}\")\n",
    "        print(f\"  Quality Score: {evaluation['quality']['overall_quality']:.2f}\")\n",
    "        print(f\"  Depth Score: {evaluation['depth']['depth_score']:.2f}\")\n",
    "        print(f\"  Combined Score: {evaluation['combined_score']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nQuality Breakdown:\")\n",
    "        print(f\"  Specificity: {evaluation['quality']['specificity']:.2f}\")\n",
    "        print(f\"  Actionability: {evaluation['quality']['actionability']:.2f}\")\n",
    "        print(f\"  Relevance: {evaluation['quality']['relevance']:.2f}\")\n",
    "        print(f\"  Feasibility: {evaluation['quality']['feasibility']:.2f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"No results to evaluate yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Compare Your Strategies Against Expert Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple strategies and compare to experts\n",
    "try:\n",
    "    test_ui = comparison_data[0]\n",
    "    \n",
    "    strategies_to_test = {\n",
    "        \"single\": {},\n",
    "        \"multi_perspective\": {\"synthesize\": True},\n",
    "    }\n",
    "    \n",
    "    strategy_results = {}\n",
    "    \n",
    "    print(f\"Testing strategies on: {test_ui['task']}\\n\")\n",
    "    \n",
    "    for strategy_name, kwargs in strategies_to_test.items():\n",
    "        print(f\"Running {strategy_name}...\")\n",
    "        \n",
    "        critique = run_critique_strategy(\n",
    "            strategy_name,\n",
    "            test_ui['problem'],\n",
    "            provider=\"ollama\",\n",
    "            temperature=0.3,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Compare to experts\n",
    "        comparison = compare_to_experts(\n",
    "            critique.recommendations,\n",
    "            test_ui['expert_critiques'],\n",
    "            method=\"coverage\"\n",
    "        )\n",
    "        \n",
    "        strategy_results[strategy_name] = {\n",
    "            'critique': critique,\n",
    "            'expert_overlap': comparison['coverage_score']\n",
    "        }\n",
    "        \n",
    "        print(f\"  Expert Coverage: {comparison['coverage_score']:.2%}\")\n",
    "        print(f\"  Latency: {critique.latency_s:.2f}s\")\n",
    "        print()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nStrategy Comparison:\")\n",
    "    print(f\"{'Strategy':<20} {'Expert Coverage':<20} {'Recommendations':<20}\")\n",
    "    print(\"=\"*60)\n",
    "    for name, data in strategy_results.items():\n",
    "        print(f\"{name:<20} {data['expert_overlap']:<20.2%} {len(data['critique'].recommendations):<20}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harness import get_tracker, ExperimentConfig, ExperimentResult\n",
    "\n",
    "def track_uicrit_evaluation(problems, strategy=\"multi_perspective\", provider=\"ollama\"):\n",
    "    \"\"\"Run and track UICrit evaluation\"\"\"\n",
    "    \n",
    "    # Start tracking\n",
    "    tracker = get_tracker()\n",
    "    experiment_dir = tracker.start_experiment(ExperimentConfig(\n",
    "        experiment_name=f\"uicrit_{strategy}\",\n",
    "        strategy=strategy,\n",
    "        provider=provider,\n",
    "        metadata={\n",
    "            \"project\": \"crit\",\n",
    "            \"benchmark\": \"UICrit\",\n",
    "            \"problem_count\": len(problems)\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "    # Run critiques\n",
    "    for i, problem in enumerate(problems, 1):\n",
    "        print(f\"Processing {i}/{len(problems)}: {problem.metadata['task']}\")\n",
    "        \n",
    "        critique = run_critique_strategy(\n",
    "            strategy,\n",
    "            problem,\n",
    "            provider=provider,\n",
    "            synthesize=True if strategy == \"multi_perspective\" else None\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluation = evaluate_critique(problem, critique, method=\"combined\")\n",
    "        \n",
    "        # Log\n",
    "        tracker.log_result(ExperimentResult(\n",
    "            task_input=problem.to_critique_prompt(),\n",
    "            output=critique.synthesis or \"\",\n",
    "            strategy_name=strategy,\n",
    "            latency_s=critique.latency_s,\n",
    "            tokens_in=critique.total_tokens_in,\n",
    "            tokens_out=critique.total_tokens_out,\n",
    "            cost_usd=critique.total_cost_usd,\n",
    "            metadata={\n",
    "                \"rico_id\": problem.metadata['rico_id'],\n",
    "                \"task\": problem.metadata['task'],\n",
    "                \"quality_rating\": problem.metadata['avg_quality'],\n",
    "                \"combined_score\": evaluation.get('combined_score', 0),\n",
    "                \"recommendations\": critique.recommendations\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    summary = tracker.finish_experiment()\n",
    "    print(f\"\\nResults saved to: {experiment_dir}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "# summary = track_uicrit_evaluation(uicrit.problems[:10], strategy=\"multi_perspective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights from UICrit\n",
    "\n",
    "The UICrit paper found that LLMs achieved a **55% performance gain** using:\n",
    "- Few-shot prompting with expert examples\n",
    "- Visual prompting (when using multimodal models)\n",
    "- Structured output formats\n",
    "\n",
    "### Things to Try:\n",
    "\n",
    "1. **Few-Shot Learning**: Include expert critiques as examples\n",
    "2. **Multi-Perspective**: Use different expert viewpoints\n",
    "3. **Structured Output**: Ask for specific critique categories\n",
    "4. **Quality Correlation**: Test if low-rated UIs get more/better critiques\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Full Evaluation**: Run on all 1,000 UIs\n",
    "2. **Compare Strategies**: Test which approach best matches experts\n",
    "3. **Quality Analysis**: Correlate your critiques with quality ratings\n",
    "4. **Fine-Tuning**: Train on expert critiques to improve performance\n",
    "\n",
    "## References\n",
    "\n",
    "- UICrit: Duan et al., UIST 2024\n",
    "- GitHub: https://github.com/google-research-datasets/uicrit\n",
    "- Paper: https://arxiv.org/abs/2407.08850"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}