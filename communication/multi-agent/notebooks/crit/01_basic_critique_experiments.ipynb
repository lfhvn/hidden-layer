{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collective Design Critique Experiments\n",
    "\n",
    "This notebook demonstrates how to use collective design critique reasoning agents to solve challenging design problems.\n",
    "\n",
    "## Research Question\n",
    "\n",
    "**Can collective design critique from multiple perspectives produce better solutions than a single critic?**\n",
    "\n",
    "We're interested in:\n",
    "1. **Coverage**: Do multiple perspectives catch more issues?\n",
    "2. **Quality**: Are recommendations more specific and actionable?\n",
    "3. **Synthesis**: Can we effectively combine diverse viewpoints?\n",
    "4. **Iteration**: Does iterative refinement improve designs?\n",
    "5. **Adversarial Testing**: Does challenge/response find edge cases?\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../code')\n",
    "\n",
    "from crit import (\n",
    "    # Pre-defined problems\n",
    "    MOBILE_CHECKOUT,\n",
    "    DASHBOARD_LAYOUT,\n",
    "    REST_API_VERSIONING,\n",
    "    GRAPHQL_SCHEMA,\n",
    "    MICROSERVICES_SPLIT,\n",
    "    CACHING_STRATEGY,\n",
    "    PERMISSION_MODEL,\n",
    "    APPROVAL_WORKFLOW,\n",
    "    \n",
    "    # Strategies\n",
    "    run_critique_strategy,\n",
    "    single_critic_strategy,\n",
    "    multi_perspective_critique,\n",
    "    iterative_critique,\n",
    "    adversarial_critique,\n",
    "    \n",
    "    # Types\n",
    "    DesignDomain,\n",
    "    CritiquePerspective,\n",
    "    ALL_PROBLEMS,\n",
    "    get_problems_by_domain,\n",
    "    get_problems_by_difficulty,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluate_critique,\n",
    "    compare_strategies,\n",
    "    batch_evaluate,\n",
    ")\n",
    "\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Single Critic Baseline\n",
    "\n",
    "Let's start with a baseline - a single critic reviewing a mobile checkout flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's look at the design problem\n",
    "print(\"DESIGN PROBLEM:\")\n",
    "print(f\"Name: {MOBILE_CHECKOUT.name}\")\n",
    "print(f\"Domain: {MOBILE_CHECKOUT.domain.value}\")\n",
    "print(f\"Difficulty: {MOBILE_CHECKOUT.difficulty}\")\n",
    "print(f\"\\nDescription: {MOBILE_CHECKOUT.description}\")\n",
    "print(f\"\\nContext:\\n{MOBILE_CHECKOUT.context}\")\n",
    "print(f\"\\nCurrent Design:\\n{MOBILE_CHECKOUT.current_design}\")\n",
    "print(f\"\\nKnown Issues:\")\n",
    "for issue in MOBILE_CHECKOUT.known_issues:\n",
    "    print(f\"  - {issue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single critic strategy\n",
    "single_result = run_critique_strategy(\n",
    "    \"single\",\n",
    "    MOBILE_CHECKOUT,\n",
    "    provider=\"ollama\",\n",
    "    model=None,  # Uses default\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"SINGLE CRITIC FEEDBACK:\")\n",
    "print(single_result.synthesis)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nRecommendations ({len(single_result.recommendations)}):\")\n",
    "for i, rec in enumerate(single_result.recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "print(f\"\\nLatency: {single_result.latency_s:.2f}s\")\n",
    "print(f\"Cost: ${single_result.total_cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multi-Perspective Critique\n",
    "\n",
    "Now let's get feedback from multiple expert perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-perspective critique\n",
    "multi_result = run_critique_strategy(\n",
    "    \"multi_perspective\",\n",
    "    MOBILE_CHECKOUT,\n",
    "    provider=\"ollama\",\n",
    "    synthesize=True,  # Combine perspectives into unified feedback\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"MULTI-PERSPECTIVE CRITIQUES:\")\n",
    "print(\"\\nIndividual Perspectives:\")\n",
    "for critique in multi_result.critiques:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PERSPECTIVE: {critique['perspective'].upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(critique['critique'])\n",
    "\n",
    "if multi_result.synthesis:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SYNTHESIZED FEEDBACK:\")\n",
    "    print(\"=\"*60)\n",
    "    print(multi_result.synthesis)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Recommendations ({len(multi_result.recommendations)}):\")\n",
    "for i, rec in enumerate(multi_result.recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(f\"\\nLatency: {multi_result.latency_s:.2f}s\")\n",
    "print(f\"Cost: ${multi_result.total_cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Compare Single vs Multi-Perspective\n",
    "\n",
    "Let's evaluate and compare the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both approaches\n",
    "single_eval = evaluate_critique(\n",
    "    MOBILE_CHECKOUT,\n",
    "    single_result,\n",
    "    method=\"combined\",\n",
    "    judge_provider=\"ollama\"\n",
    ")\n",
    "\n",
    "multi_eval = evaluate_critique(\n",
    "    MOBILE_CHECKOUT,\n",
    "    multi_result,\n",
    "    method=\"combined\",\n",
    "    judge_provider=\"ollama\"\n",
    ")\n",
    "\n",
    "print(\"EVALUATION COMPARISON:\\n\")\n",
    "print(f\"{'Metric':<25} {'Single':<15} {'Multi-Perspective':<15}\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Coverage\n",
    "print(f\"{'Coverage Score':<25} {single_eval['coverage']['overall_coverage']:<15.2f} {multi_eval['coverage']['overall_coverage']:<15.2f}\")\n",
    "print(f\"{'Known Issues Found':<25} {single_eval['coverage']['known_issues_mentioned']:<15} {multi_eval['coverage']['known_issues_mentioned']:<15}\")\n",
    "print(f\"{'Criteria Addressed':<25} {single_eval['coverage']['criteria_mentioned']:<15} {multi_eval['coverage']['criteria_mentioned']:<15}\")\n",
    "\n",
    "# Quality\n",
    "print(f\"\\n{'Quality Score':<25} {single_eval['quality']['overall_quality']:<15.2f} {multi_eval['quality']['overall_quality']:<15.2f}\")\n",
    "print(f\"{'Specificity':<25} {single_eval['quality']['specificity']:<15.2f} {multi_eval['quality']['specificity']:<15.2f}\")\n",
    "print(f\"{'Actionability':<25} {single_eval['quality']['actionability']:<15.2f} {multi_eval['quality']['actionability']:<15.2f}\")\n",
    "print(f\"{'Relevance':<25} {single_eval['quality']['relevance']:<15.2f} {multi_eval['quality']['relevance']:<15.2f}\")\n",
    "\n",
    "# Depth\n",
    "print(f\"\\n{'Depth Score':<25} {single_eval['depth']['depth_score']:<15.2f} {multi_eval['depth']['depth_score']:<15.2f}\")\n",
    "print(f\"{'Perspectives Used':<25} {single_eval['depth']['critique_count']:<15} {multi_eval['depth']['critique_count']:<15}\")\n",
    "print(f\"{'Recommendations':<25} {single_eval['depth']['recommendations_count']:<15} {multi_eval['depth']['recommendations_count']:<15}\")\n",
    "\n",
    "# Overall\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"{'COMBINED SCORE':<25} {single_eval['combined_score']:<15.2f} {multi_eval['combined_score']:<15.2f}\")\n",
    "\n",
    "# Performance\n",
    "print(f\"\\n{'Latency (s)':<25} {single_result.latency_s:<15.2f} {multi_result.latency_s:<15.2f}\")\n",
    "print(f\"{'Cost (USD)':<25} ${single_result.total_cost_usd:<14.4f} ${multi_result.total_cost_usd:<14.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Iterative Critique\n",
    "\n",
    "Test if iterative refinement improves the design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run iterative critique (2 rounds)\n",
    "iterative_result = run_critique_strategy(\n",
    "    \"iterative\",\n",
    "    MOBILE_CHECKOUT,\n",
    "    iterations=2,\n",
    "    provider=\"ollama\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"ITERATIVE CRITIQUE RESULTS:\\n\")\n",
    "\n",
    "for critique in iterative_result.critiques:\n",
    "    iteration = critique.get('iteration', '?')\n",
    "    crit_type = critique.get('type', 'unknown')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ITERATION {iteration} - {crit_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if crit_type == 'critique':\n",
    "        print(critique['feedback'])\n",
    "    elif crit_type == 'revision':\n",
    "        print(\"REVISED DESIGN:\")\n",
    "        print(critique['revised_design'])\n",
    "\n",
    "if iterative_result.revised_design:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL REVISED DESIGN:\")\n",
    "    print(\"=\"*60)\n",
    "    print(iterative_result.revised_design)\n",
    "\n",
    "print(f\"\\nLatency: {iterative_result.latency_s:.2f}s\")\n",
    "print(f\"Cost: ${iterative_result.total_cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Adversarial Critique\n",
    "\n",
    "Use adversarial dialogue to challenge assumptions and find edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run adversarial critique\n",
    "adversarial_result = run_critique_strategy(\n",
    "    \"adversarial\",\n",
    "    MOBILE_CHECKOUT,\n",
    "    provider=\"ollama\",\n",
    "    temperature=0.4  # Slightly higher for more diverse debate\n",
    ")\n",
    "\n",
    "print(\"ADVERSARIAL CRITIQUE EXCHANGE:\\n\")\n",
    "\n",
    "for critique in adversarial_result.critiques:\n",
    "    agent = critique['agent']\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"AGENT: {agent.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(critique['content'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SYNTHESIS\")\n",
    "print(\"=\"*60)\n",
    "print(adversarial_result.synthesis)\n",
    "\n",
    "print(f\"\\nLatency: {adversarial_result.latency_s:.2f}s\")\n",
    "print(f\"Cost: ${adversarial_result.total_cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Compare All Strategies\n",
    "\n",
    "Run all strategies on the same problem and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all strategies\n",
    "strategies_to_test = {\n",
    "    \"single\": {},\n",
    "    \"multi_perspective\": {\"synthesize\": True},\n",
    "    \"iterative\": {\"iterations\": 2},\n",
    "    \"adversarial\": {},\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Running all strategies...\\n\")\n",
    "for strategy_name, kwargs in strategies_to_test.items():\n",
    "    print(f\"Running {strategy_name}...\")\n",
    "    result = run_critique_strategy(\n",
    "        strategy_name,\n",
    "        MOBILE_CHECKOUT,\n",
    "        provider=\"ollama\",\n",
    "        temperature=0.3,\n",
    "        **kwargs\n",
    "    )\n",
    "    results[strategy_name] = result\n",
    "    print(f\"  Completed in {result.latency_s:.2f}s\")\n",
    "\n",
    "print(\"\\nAll strategies completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare strategies\n",
    "comparison = compare_strategies(\n",
    "    MOBILE_CHECKOUT,\n",
    "    results,\n",
    "    judge_provider=\"ollama\"\n",
    ")\n",
    "\n",
    "print(\"STRATEGY COMPARISON RESULTS:\\n\")\n",
    "\n",
    "# Rankings\n",
    "if \"combined\" in comparison[\"rankings\"]:\n",
    "    print(\"Overall Ranking (Combined Score):\")\n",
    "    for i, item in enumerate(comparison[\"rankings\"][\"combined\"], 1):\n",
    "        print(f\"{i}. {item['strategy']:<20} Score: {item['score']:.3f}\")\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"\\n{'Strategy':<20} {'Coverage':<12} {'Quality':<12} {'Depth':<12} {'Combined':<12}\")\n",
    "print(\"=\"*68)\n",
    "\n",
    "for strategy_name, eval_result in comparison[\"evaluations\"].items():\n",
    "    coverage = eval_result.get(\"coverage\", {}).get(\"overall_coverage\", 0)\n",
    "    quality = eval_result.get(\"quality\", {}).get(\"overall_quality\", 0)\n",
    "    depth = eval_result.get(\"depth\", {}).get(\"depth_score\", 0)\n",
    "    combined = eval_result.get(\"combined_score\", 0)\n",
    "    \n",
    "    print(f\"{strategy_name:<20} {coverage:<12.3f} {quality:<12.3f} {depth:<12.3f} {combined:<12.3f}\")\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"\\n{'Strategy':<20} {'Latency (s)':<15} {'Cost (USD)':<15} {'Tokens':<15}\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "for strategy_name, perf in comparison[\"performance\"].items():\n",
    "    print(f\"{strategy_name:<20} {perf['latency_s']:<15.2f} ${perf['total_cost_usd']:<14.4f} {perf['total_tokens']:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Test on Different Problem Types\n",
    "\n",
    "Let's see how strategies perform across different design domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on different domains\n",
    "test_problems = [\n",
    "    MOBILE_CHECKOUT,    # UI/UX\n",
    "    REST_API_VERSIONING,  # API\n",
    "    CACHING_STRATEGY,   # System\n",
    "]\n",
    "\n",
    "domain_results = {}\n",
    "\n",
    "for problem in test_problems:\n",
    "    print(f\"\\nTesting on: {problem.name} ({problem.domain.value})\")\n",
    "    \n",
    "    # Use multi-perspective for each\n",
    "    result = run_critique_strategy(\n",
    "        \"multi_perspective\",\n",
    "        problem,\n",
    "        provider=\"ollama\",\n",
    "        synthesize=True,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    domain_results[problem.name] = {\n",
    "        \"problem\": problem,\n",
    "        \"critique_result\": result\n",
    "    }\n",
    "    \n",
    "    print(f\"  Perspectives used: {len(result.critiques)}\")\n",
    "    print(f\"  Recommendations: {len(result.recommendations)}\")\n",
    "    print(f\"  Latency: {result.latency_s:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch evaluate across domains\n",
    "batch_results = batch_evaluate(\n",
    "    list(domain_results.values()),\n",
    "    judge_provider=\"ollama\"\n",
    ")\n",
    "\n",
    "print(\"BATCH EVALUATION ACROSS DOMAINS:\\n\")\n",
    "print(f\"Total problems evaluated: {batch_results['count']}\")\n",
    "print(\"\\nAggregate Scores:\")\n",
    "for metric, score in batch_results['aggregates'].items():\n",
    "    print(f\"  {metric}: {score:.3f}\")\n",
    "\n",
    "print(\"\\nIndividual Problem Scores:\")\n",
    "print(f\"\\n{'Problem':<30} {'Coverage':<12} {'Quality':<12} {'Combined':<12}\")\n",
    "print(\"=\"*66)\n",
    "\n",
    "for eval_result in batch_results['evaluations']:\n",
    "    name = eval_result['problem_name']\n",
    "    coverage = eval_result.get(\"coverage\", {}).get(\"overall_coverage\", 0)\n",
    "    quality = eval_result.get(\"quality\", {}).get(\"overall_quality\", 0)\n",
    "    combined = eval_result.get(\"combined_score\", 0)\n",
    "    \n",
    "    print(f\"{name:<30} {coverage:<12.3f} {quality:<12.3f} {combined:<12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Custom Perspectives\n",
    "\n",
    "You can specify custom perspectives for domain-specific critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom perspectives for API design\n",
    "custom_perspectives = [\n",
    "    CritiquePerspective.USABILITY,\n",
    "    CritiquePerspective.CONSISTENCY,\n",
    "    CritiquePerspective.SCALABILITY,\n",
    "    CritiquePerspective.SECURITY,\n",
    "]\n",
    "\n",
    "custom_result = multi_perspective_critique(\n",
    "    REST_API_VERSIONING,\n",
    "    perspectives=custom_perspectives,\n",
    "    synthesize=True,\n",
    "    provider=\"ollama\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"CUSTOM PERSPECTIVE CRITIQUE:\\n\")\n",
    "print(f\"Perspectives used: {[p.value for p in custom_perspectives]}\")\n",
    "print(f\"\\nSynthesis:\")\n",
    "print(custom_result.synthesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Questions to Explore\n",
    "\n",
    "Use this toolkit to investigate:\n",
    "\n",
    "1. **Strategy Effectiveness**\n",
    "   - Which strategy finds the most issues?\n",
    "   - Which produces the most actionable recommendations?\n",
    "   - How does performance scale with problem complexity?\n",
    "\n",
    "2. **Perspective Value**\n",
    "   - Which perspectives are most valuable for different domains?\n",
    "   - How much overlap is there between perspectives?\n",
    "   - Can we identify \"essential\" vs \"nice-to-have\" perspectives?\n",
    "\n",
    "3. **Cost-Benefit Analysis**\n",
    "   - What's the ROI of multi-agent vs single-agent?\n",
    "   - Where is the point of diminishing returns?\n",
    "   - How to optimize for cost vs quality?\n",
    "\n",
    "4. **Synthesis Quality**\n",
    "   - Does synthesis improve upon individual critiques?\n",
    "   - What's lost in synthesis?\n",
    "   - Can we measure synthesis effectiveness?\n",
    "\n",
    "5. **Iteration Value**\n",
    "   - How much does each iteration improve the design?\n",
    "   - When do iterations stop adding value?\n",
    "   - Can we predict optimal iteration count?\n",
    "\n",
    "6. **Model Capabilities**\n",
    "   - How do different models compare on design critique?\n",
    "   - Do larger models give better critiques?\n",
    "   - Are specialized models better for specific domains?\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Create custom design problems** for your specific domain\n",
    "2. **Compare different models** (local vs API, small vs large)\n",
    "3. **Experiment with perspective combinations**\n",
    "4. **Test on real-world design challenges**\n",
    "5. **Integrate with experiment tracking** for long-term analysis\n",
    "6. **Build validated benchmark datasets** for consistent evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
