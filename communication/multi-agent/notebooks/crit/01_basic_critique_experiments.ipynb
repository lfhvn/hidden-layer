{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collective Design Critique Experiments\n",
    "\n",
    "This notebook demonstrates how to use collective design critique reasoning agents to solve challenging design problems.\n",
    "\n",
    "## Research Question\n",
    "\n",
    "**Can collective design critique from multiple perspectives produce better solutions than a single critic?**\n",
    "\n",
    "We're interested in:\n",
    "1. **Coverage**: Do multiple perspectives catch more issues?\n",
    "2. **Quality**: Are recommendations more specific and actionable?\n",
    "3. **Synthesis**: Can we effectively combine diverse viewpoints?\n",
    "4. **Iteration**: Does iterative refinement improve designs?\n",
    "5. **Adversarial Testing**: Does challenge/response find edge cases?\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add repository root to path to import modules\nimport sys\nsys.path.append('../../../../')  # Go up to repo root from notebooks/crit/\n\n# 2. Add local code directory for CRIT-specific code\nsys.path.append('../../code')  # Add multi-agent/code for crit module\n\n# 3. Import harness defaults to show configuration\nfrom harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n\nprint(\"=\"*70)\nprint(\"üîß CRIT NOTEBOOK CONFIGURATION\")\nprint(\"=\"*70)\nprint(f\"üìç Provider: {DEFAULT_PROVIDER}\")\nprint(f\"ü§ñ Model: {DEFAULT_MODEL or '(default for provider)'}\")\nprint(\"=\"*70)\nprint(\"\\nüí° TO CHANGE: Add after imports:\")\nprint(\"   PROVIDER = 'mlx'  # or 'ollama', 'anthropic', 'openai'\")\nprint(\"   MODEL = 'your-model'\")\nprint(\"   Then pass to: run_critique_strategy(..., provider=PROVIDER, model=MODEL)\")\nprint(\"=\"*70 + \"\\n\")\n\n# 4. Import CRIT module components\nfrom crit import (\n    # Pre-defined design problems to test critique strategies\n    MOBILE_CHECKOUT,          # Mobile checkout flow design\n    DASHBOARD_LAYOUT,         # Dashboard layout design\n    REST_API_VERSIONING,      # REST API versioning design\n    GRAPHQL_SCHEMA,           # GraphQL schema design\n    MICROSERVICES_SPLIT,      # Microservices architecture design\n    CACHING_STRATEGY,         # Caching strategy design\n    PERMISSION_MODEL,         # Permission model design\n    APPROVAL_WORKFLOW,        # Approval workflow design\n    \n    # Critique strategies\n    run_critique_strategy,    # Run any critique strategy\n    single_critic_strategy,   # Single critic baseline\n    multi_perspective_critique,  # Multiple perspective critique\n    iterative_critique,       # Iterative refinement critique\n    adversarial_critique,     # Adversarial debate critique\n    \n    # Types and collections\n    DesignDomain,             # Enum of design domains (UI/UX, API, System, Process)\n    CritiquePerspective,      # Enum of critique perspectives\n    ALL_PROBLEMS,             # All pre-defined problems\n    get_problems_by_domain,   # Filter problems by domain\n    get_problems_by_difficulty,  # Filter problems by difficulty\n    \n    # Evaluation functions\n    evaluate_critique,        # Evaluate a single critique\n    compare_strategies,       # Compare multiple strategies\n    batch_evaluate,           # Evaluate multiple critiques\n)\n\n# 5. Import JSON for data handling\nimport json\nfrom pprint import pprint"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Single Critic Baseline\n",
    "\n",
    "Let's start with a baseline - a single critic reviewing a mobile checkout flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Print the design problem details\nprint(\"DESIGN PROBLEM:\")\nprint(f\"Name: {MOBILE_CHECKOUT.name}\")\nprint(f\"Domain: {MOBILE_CHECKOUT.domain.value}\")\nprint(f\"Difficulty: {MOBILE_CHECKOUT.difficulty}\")\n\n# 2. Print the problem description\nprint(f\"\\nDescription: {MOBILE_CHECKOUT.description}\")\n\n# 3. Print the context for the design\nprint(f\"\\nContext:\\n{MOBILE_CHECKOUT.context}\")\n\n# 4. Print the current design being critiqued\nprint(f\"\\nCurrent Design:\\n{MOBILE_CHECKOUT.current_design}\")\n\n# 5. Print known issues (what we expect good critiques to find)\nprint(f\"\\nKnown Issues:\")\nfor issue in MOBILE_CHECKOUT.known_issues:\n    print(f\"  - {issue}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Run single critic strategy as baseline\nsingle_result = run_critique_strategy(\n    \"single\",              # Strategy: single critic\n    MOBILE_CHECKOUT,       # Problem to critique\n    provider=\"ollama\",     # Use Ollama local provider\n    model=None,            # Use default model\n    temperature=0.3        # Low temperature for consistent critiques\n)\n\n# 2. Print the critique feedback\nprint(\"SINGLE CRITIC FEEDBACK:\")\nprint(single_result.synthesis)\n\n# 3. Print separator\nprint(\"\\n\" + \"=\"*60)\n\n# 4. Print recommendations from the critique\nprint(f\"\\nRecommendations ({len(single_result.recommendations)}):\")\nfor i, rec in enumerate(single_result.recommendations, 1):\n    print(f\"{i}. {rec}\")\n\n# 5. Print performance metrics\nprint(f\"\\nLatency: {single_result.latency_s:.2f}s\")\nprint(f\"Cost: ${single_result.total_cost_usd:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multi-Perspective Critique\n",
    "\n",
    "Now let's get feedback from multiple expert perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Run multi-perspective critique strategy\nmulti_result = run_critique_strategy(\n    \"multi_perspective\",   # Strategy: multiple expert perspectives\n    MOBILE_CHECKOUT,       # Same problem\n    provider=\"ollama\",     # Use Ollama\n    synthesize=True,       # Combine perspectives into unified feedback\n    temperature=0.3        # Consistent temperature\n)\n\n# 2. Print individual perspectives\nprint(\"MULTI-PERSPECTIVE CRITIQUES:\")\nprint(\"\\nIndividual Perspectives:\")\nfor critique in multi_result.critiques:\n    print(f\"\\n{'='*60}\")\n    print(f\"PERSPECTIVE: {critique['perspective'].upper()}\")\n    print(f\"{'='*60}\")\n    print(critique['critique'])\n\n# 3. Print synthesized feedback if available\nif multi_result.synthesis:\n    print(\"\\n\" + \"=\"*60)\n    print(\"SYNTHESIZED FEEDBACK:\")\n    print(\"=\"*60)\n    print(multi_result.synthesis)\n\n# 4. Print recommendations\nprint(f\"\\n{'='*60}\")\nprint(f\"Recommendations ({len(multi_result.recommendations)}):\")\nfor i, rec in enumerate(multi_result.recommendations, 1):\n    print(f\"{i}. {rec}\")\n\n# 5. Print performance metrics\nprint(f\"\\nLatency: {multi_result.latency_s:.2f}s\")\nprint(f\"Cost: ${multi_result.total_cost_usd:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Compare Single vs Multi-Perspective\n",
    "\n",
    "Let's evaluate and compare the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both approaches\n",
    "single_eval = evaluate_critique(\n",
    "    MOBILE_CHECKOUT,\n",
    "    single_result,\n",
    "    method=\"combined\",\n",
    "    judge_provider=\"ollama\"\n",
    ")\n",
    "\n",
    "multi_eval = evaluate_critique(\n",
    "    MOBILE_CHECKOUT,\n",
    "    multi_result,\n",
    "    method=\"combined\",\n",
    "    judge_provider=\"ollama\"\n",
    ")\n",
    "\n",
    "print(\"EVALUATION COMPARISON:\\n\")\n",
    "print(f\"{'Metric':<25} {'Single':<15} {'Multi-Perspective':<15}\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Coverage\n",
    "print(f\"{'Coverage Score':<25} {single_eval['coverage']['overall_coverage']:<15.2f} {multi_eval['coverage']['overall_coverage']:<15.2f}\")\n",
    "print(f\"{'Known Issues Found':<25} {single_eval['coverage']['known_issues_mentioned']:<15} {multi_eval['coverage']['known_issues_mentioned']:<15}\")\n",
    "print(f\"{'Criteria Addressed':<25} {single_eval['coverage']['criteria_mentioned']:<15} {multi_eval['coverage']['criteria_mentioned']:<15}\")\n",
    "\n",
    "# Quality\n",
    "print(f\"\\n{'Quality Score':<25} {single_eval['quality']['overall_quality']:<15.2f} {multi_eval['quality']['overall_quality']:<15.2f}\")\n",
    "print(f\"{'Specificity':<25} {single_eval['quality']['specificity']:<15.2f} {multi_eval['quality']['specificity']:<15.2f}\")\n",
    "print(f\"{'Actionability':<25} {single_eval['quality']['actionability']:<15.2f} {multi_eval['quality']['actionability']:<15.2f}\")\n",
    "print(f\"{'Relevance':<25} {single_eval['quality']['relevance']:<15.2f} {multi_eval['quality']['relevance']:<15.2f}\")\n",
    "\n",
    "# Depth\n",
    "print(f\"\\n{'Depth Score':<25} {single_eval['depth']['depth_score']:<15.2f} {multi_eval['depth']['depth_score']:<15.2f}\")\n",
    "print(f\"{'Perspectives Used':<25} {single_eval['depth']['critique_count']:<15} {multi_eval['depth']['critique_count']:<15}\")\n",
    "print(f\"{'Recommendations':<25} {single_eval['depth']['recommendations_count']:<15} {multi_eval['depth']['recommendations_count']:<15}\")\n",
    "\n",
    "# Overall\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"{'COMBINED SCORE':<25} {single_eval['combined_score']:<15.2f} {multi_eval['combined_score']:<15.2f}\")\n",
    "\n",
    "# Performance\n",
    "print(f\"\\n{'Latency (s)':<25} {single_result.latency_s:<15.2f} {multi_result.latency_s:<15.2f}\")\n",
    "print(f\"{'Cost (USD)':<25} ${single_result.total_cost_usd:<14.4f} ${multi_result.total_cost_usd:<14.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Iterative Critique\n",
    "\n",
    "Test if iterative refinement improves the design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run iterative critique (2 rounds)\n",
    "iterative_result = run_critique_strategy(\n",
    "    \"iterative\",\n",
    "    MOBILE_CHECKOUT,\n",
    "    iterations=2,\n",
    "    provider=\"ollama\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"ITERATIVE CRITIQUE RESULTS:\\n\")\n",
    "\n",
    "for critique in iterative_result.critiques:\n",
    "    iteration = critique.get('iteration', '?')\n",
    "    crit_type = critique.get('type', 'unknown')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ITERATION {iteration} - {crit_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if crit_type == 'critique':\n",
    "        print(critique['feedback'])\n",
    "    elif crit_type == 'revision':\n",
    "        print(\"REVISED DESIGN:\")\n",
    "        print(critique['revised_design'])\n",
    "\n",
    "if iterative_result.revised_design:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL REVISED DESIGN:\")\n",
    "    print(\"=\"*60)\n",
    "    print(iterative_result.revised_design)\n",
    "\n",
    "print(f\"\\nLatency: {iterative_result.latency_s:.2f}s\")\n",
    "print(f\"Cost: ${iterative_result.total_cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Adversarial Critique\n",
    "\n",
    "Use adversarial dialogue to challenge assumptions and find edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run adversarial critique\n",
    "adversarial_result = run_critique_strategy(\n",
    "    \"adversarial\",\n",
    "    MOBILE_CHECKOUT,\n",
    "    provider=\"ollama\",\n",
    "    temperature=0.4  # Slightly higher for more diverse debate\n",
    ")\n",
    "\n",
    "print(\"ADVERSARIAL CRITIQUE EXCHANGE:\\n\")\n",
    "\n",
    "for critique in adversarial_result.critiques:\n",
    "    agent = critique['agent']\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"AGENT: {agent.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(critique['content'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SYNTHESIS\")\n",
    "print(\"=\"*60)\n",
    "print(adversarial_result.synthesis)\n",
    "\n",
    "print(f\"\\nLatency: {adversarial_result.latency_s:.2f}s\")\n",
    "print(f\"Cost: ${adversarial_result.total_cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Compare All Strategies\n",
    "\n",
    "Run all strategies on the same problem and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all strategies\n",
    "strategies_to_test = {\n",
    "    \"single\": {},\n",
    "    \"multi_perspective\": {\"synthesize\": True},\n",
    "    \"iterative\": {\"iterations\": 2},\n",
    "    \"adversarial\": {},\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Running all strategies...\\n\")\n",
    "for strategy_name, kwargs in strategies_to_test.items():\n",
    "    print(f\"Running {strategy_name}...\")\n",
    "    result = run_critique_strategy(\n",
    "        strategy_name,\n",
    "        MOBILE_CHECKOUT,\n",
    "        provider=\"ollama\",\n",
    "        temperature=0.3,\n",
    "        **kwargs\n",
    "    )\n",
    "    results[strategy_name] = result\n",
    "    print(f\"  Completed in {result.latency_s:.2f}s\")\n",
    "\n",
    "print(\"\\nAll strategies completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare strategies\n",
    "comparison = compare_strategies(\n",
    "    MOBILE_CHECKOUT,\n",
    "    results,\n",
    "    judge_provider=\"ollama\"\n",
    ")\n",
    "\n",
    "print(\"STRATEGY COMPARISON RESULTS:\\n\")\n",
    "\n",
    "# Rankings\n",
    "if \"combined\" in comparison[\"rankings\"]:\n",
    "    print(\"Overall Ranking (Combined Score):\")\n",
    "    for i, item in enumerate(comparison[\"rankings\"][\"combined\"], 1):\n",
    "        print(f\"{i}. {item['strategy']:<20} Score: {item['score']:.3f}\")\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"\\n{'Strategy':<20} {'Coverage':<12} {'Quality':<12} {'Depth':<12} {'Combined':<12}\")\n",
    "print(\"=\"*68)\n",
    "\n",
    "for strategy_name, eval_result in comparison[\"evaluations\"].items():\n",
    "    coverage = eval_result.get(\"coverage\", {}).get(\"overall_coverage\", 0)\n",
    "    quality = eval_result.get(\"quality\", {}).get(\"overall_quality\", 0)\n",
    "    depth = eval_result.get(\"depth\", {}).get(\"depth_score\", 0)\n",
    "    combined = eval_result.get(\"combined_score\", 0)\n",
    "    \n",
    "    print(f\"{strategy_name:<20} {coverage:<12.3f} {quality:<12.3f} {depth:<12.3f} {combined:<12.3f}\")\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"\\n{'Strategy':<20} {'Latency (s)':<15} {'Cost (USD)':<15} {'Tokens':<15}\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "for strategy_name, perf in comparison[\"performance\"].items():\n",
    "    print(f\"{strategy_name:<20} {perf['latency_s']:<15.2f} ${perf['total_cost_usd']:<14.4f} {perf['total_tokens']:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Test on Different Problem Types\n",
    "\n",
    "Let's see how strategies perform across different design domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on different domains\n",
    "test_problems = [\n",
    "    MOBILE_CHECKOUT,    # UI/UX\n",
    "    REST_API_VERSIONING,  # API\n",
    "    CACHING_STRATEGY,   # System\n",
    "]\n",
    "\n",
    "domain_results = {}\n",
    "\n",
    "for problem in test_problems:\n",
    "    print(f\"\\nTesting on: {problem.name} ({problem.domain.value})\")\n",
    "    \n",
    "    # Use multi-perspective for each\n",
    "    result = run_critique_strategy(\n",
    "        \"multi_perspective\",\n",
    "        problem,\n",
    "        provider=\"ollama\",\n",
    "        synthesize=True,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    domain_results[problem.name] = {\n",
    "        \"problem\": problem,\n",
    "        \"critique_result\": result\n",
    "    }\n",
    "    \n",
    "    print(f\"  Perspectives used: {len(result.critiques)}\")\n",
    "    print(f\"  Recommendations: {len(result.recommendations)}\")\n",
    "    print(f\"  Latency: {result.latency_s:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch evaluate across domains\n",
    "batch_results = batch_evaluate(\n",
    "    list(domain_results.values()),\n",
    "    judge_provider=\"ollama\"\n",
    ")\n",
    "\n",
    "print(\"BATCH EVALUATION ACROSS DOMAINS:\\n\")\n",
    "print(f\"Total problems evaluated: {batch_results['count']}\")\n",
    "print(\"\\nAggregate Scores:\")\n",
    "for metric, score in batch_results['aggregates'].items():\n",
    "    print(f\"  {metric}: {score:.3f}\")\n",
    "\n",
    "print(\"\\nIndividual Problem Scores:\")\n",
    "print(f\"\\n{'Problem':<30} {'Coverage':<12} {'Quality':<12} {'Combined':<12}\")\n",
    "print(\"=\"*66)\n",
    "\n",
    "for eval_result in batch_results['evaluations']:\n",
    "    name = eval_result['problem_name']\n",
    "    coverage = eval_result.get(\"coverage\", {}).get(\"overall_coverage\", 0)\n",
    "    quality = eval_result.get(\"quality\", {}).get(\"overall_quality\", 0)\n",
    "    combined = eval_result.get(\"combined_score\", 0)\n",
    "    \n",
    "    print(f\"{name:<30} {coverage:<12.3f} {quality:<12.3f} {combined:<12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Custom Perspectives\n",
    "\n",
    "You can specify custom perspectives for domain-specific critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom perspectives for API design\n",
    "custom_perspectives = [\n",
    "    CritiquePerspective.USABILITY,\n",
    "    CritiquePerspective.CONSISTENCY,\n",
    "    CritiquePerspective.SCALABILITY,\n",
    "    CritiquePerspective.SECURITY,\n",
    "]\n",
    "\n",
    "custom_result = multi_perspective_critique(\n",
    "    REST_API_VERSIONING,\n",
    "    perspectives=custom_perspectives,\n",
    "    synthesize=True,\n",
    "    provider=\"ollama\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"CUSTOM PERSPECTIVE CRITIQUE:\\n\")\n",
    "print(f\"Perspectives used: {[p.value for p in custom_perspectives]}\")\n",
    "print(f\"\\nSynthesis:\")\n",
    "print(custom_result.synthesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Questions to Explore\n",
    "\n",
    "Use this toolkit to investigate:\n",
    "\n",
    "1. **Strategy Effectiveness**\n",
    "   - Which strategy finds the most issues?\n",
    "   - Which produces the most actionable recommendations?\n",
    "   - How does performance scale with problem complexity?\n",
    "\n",
    "2. **Perspective Value**\n",
    "   - Which perspectives are most valuable for different domains?\n",
    "   - How much overlap is there between perspectives?\n",
    "   - Can we identify \"essential\" vs \"nice-to-have\" perspectives?\n",
    "\n",
    "3. **Cost-Benefit Analysis**\n",
    "   - What's the ROI of multi-agent vs single-agent?\n",
    "   - Where is the point of diminishing returns?\n",
    "   - How to optimize for cost vs quality?\n",
    "\n",
    "4. **Synthesis Quality**\n",
    "   - Does synthesis improve upon individual critiques?\n",
    "   - What's lost in synthesis?\n",
    "   - Can we measure synthesis effectiveness?\n",
    "\n",
    "5. **Iteration Value**\n",
    "   - How much does each iteration improve the design?\n",
    "   - When do iterations stop adding value?\n",
    "   - Can we predict optimal iteration count?\n",
    "\n",
    "6. **Model Capabilities**\n",
    "   - How do different models compare on design critique?\n",
    "   - Do larger models give better critiques?\n",
    "   - Are specialized models better for specific domains?\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Create custom design problems** for your specific domain\n",
    "2. **Compare different models** (local vs API, small vs large)\n",
    "3. **Experiment with perspective combinations**\n",
    "4. **Test on real-world design challenges**\n",
    "5. **Integrate with experiment tracking** for long-term analysis\n",
    "6. **Build validated benchmark datasets** for consistent evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}