{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Reasoning & Rationale Extraction\n",
    "\n",
    "**Purpose:** Demonstrate thinking budget and rationale extraction features\n",
    "\n",
    "**Date:** 2025-01-27  \n",
    "**Author:** Leif Haven Martinson  \n",
    "\n",
    "## Features\n",
    "\n",
    "1. **Thinking Budget** - Allocate extra tokens for internal reasoning\n",
    "2. **Rationale Extraction** - Get models to explain their reasoning\n",
    "3. **Multi-Agent Reasoning** - See how each agent reasons through problems\n",
    "\n",
    "## When to Use\n",
    "\n",
    "- ‚úÖ Complex problems requiring multi-step reasoning\n",
    "- ‚úÖ Strategic decisions with trade-offs\n",
    "- ‚úÖ Understanding why models give certain answers\n",
    "- ‚úÖ Debugging incorrect answers\n",
    "- ‚úÖ Building transparent AI systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add repository root AND parent directory to path\nimport sys\nsys.path.append('../../../')  # Go up to repo root (for harness)\nsys.path.append('../')          # Go up to multi-agent/ (for code module)\n\n# 2. Import multi-agent specific functions\nfrom code import (\n    llm_call_with_rationale,    # LLM call that extracts reasoning\n    ask_with_reasoning,          # Get both reasoning and answer\n    run_strategy_with_rationale, # Run strategy with reasoning extraction\n    run_strategy,                # Run any multi-agent strategy\n    STRATEGIES                   # Available strategies\n)\n\n# 3. Import core harness functions\nfrom harness import (\n    llm_call,                    # Basic LLM call\n    get_model_config             # Load pre-configured model settings\n)\nfrom harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n\n# 4. Show configuration and allow override\nprint(\"=\"*70)\nprint(\"üîß REASONING NOTEBOOK CONFIGURATION\")\nprint(\"=\"*70)\nprint(f\"üìç Default Provider: {DEFAULT_PROVIDER}\")\nprint(f\"ü§ñ Default Model: {DEFAULT_MODEL or '(default for provider)'}\")\nprint(\"=\"*70)\n\n# 5. TO CHANGE: Uncomment and edit these lines:\n# PROVIDER = \"mlx\"  # Options: \"mlx\", \"ollama\", \"anthropic\", \"openai\"\n# MODEL = \"your-model-name\"\n\n# Use defaults if not overridden\ntry:\n    PROVIDER\nexcept NameError:\n    PROVIDER = DEFAULT_PROVIDER\n    MODEL = DEFAULT_MODEL\n\nprint(f\"\\n‚úÖ Using: {PROVIDER} / {MODEL}\")\nprint(\"=\"*70 + \"\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Example 1: Thinking Budget\n",
    "\n",
    "Compare responses with and without thinking budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# 1. Define a math problem that benefits from reasoning\nproblem = \"\"\"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning \nand bakes muffins for her friends every day with four. She sells the remainder \nat the farmers' market daily for $2 per fresh duck egg. How much in dollars \ndoes she make every day at the farmers' market?\"\"\"\n\n# 2. First, generate WITHOUT thinking budget\nprint(\"üîµ WITHOUT thinking budget:\")\nprint(\"=\"*80)\n\nresponse_basic = llm_call(\n    problem,\n    provider=PROVIDER,\n    model=MODEL,\n    temperature=0.7\n)\n\n# 3. Print basic response details\nprint(response_basic.text)\nprint(f\"\\nTokens: {response_basic.tokens_out}\")\nprint(f\"Latency: {response_basic.latency_s:.2f}s\")\n\n# 4. Now generate WITH thinking budget (extra reasoning tokens)\nprint(\"\\n\\nüü¢ WITH thinking budget (2000 tokens):\")\nprint(\"=\"*80)\n\nresponse_thinking = llm_call(\n    problem,\n    provider=PROVIDER,\n    model=MODEL,\n    temperature=0.7,\n    thinking_budget=2000  # Allocate 2000 extra tokens for reasoning\n)\n\n# 5. Print response with thinking budget\nprint(response_thinking.text)\nprint(f\"\\nTokens: {response_thinking.tokens_out}\")\nprint(f\"Latency: {response_thinking.latency_s:.2f}s\")\n\n# 6. Compare the overhead of thinking budget\nprint(\"\\n\" + \"=\"*80)\nprint(\"üí° Key Insight: Thinking budget may improve accuracy on complex problems\")\nprint(f\"   Token overhead: {response_thinking.tokens_out - response_basic.tokens_out} tokens\")\nprint(f\"   Latency overhead: {response_thinking.latency_s - response_basic.latency_s:.2f}s\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Example 2: Rationale Extraction\n",
    "\n",
    "Get the model to explain its reasoning before giving the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategic decision problem\n",
    "decision = \"\"\"Our startup has $500k runway for 6 months. Should we:\n",
    "A) Hire 3 engineers and build faster\n",
    "B) Hire 1 engineer + 1 sales person to get revenue\n",
    "C) Keep current team and extend runway to 12 months\n",
    "\"\"\"\n",
    "\n",
    "print(\"üß† Getting reasoning + answer...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = ask_with_reasoning(\n",
    "    decision,\n",
    "    provider=PROVIDER,\n",
    "    model=MODEL,\n",
    "    thinking_budget=2000\n",
    ")\n",
    "\n",
    "print(\"REASONING:\")\n",
    "print(\"‚îÄ\"*80)\n",
    "print(result.rationale)\n",
    "\n",
    "print(\"\\n\\nFINAL ANSWER:\")\n",
    "print(\"‚îÄ\"*80)\n",
    "print(result.answer)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(f\"üíæ Metadata:\")\n",
    "print(f\"   Tokens: {result.llm_response.tokens_out}\")\n",
    "print(f\"   Latency: {result.llm_response.latency_s:.2f}s\")\n",
    "print(f\"   Provider: {result.llm_response.provider}\")\n",
    "print(f\"   Model: {result.llm_response.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Example 3: Multi-Agent Reasoning\n",
    "\n",
    "See how each agent in a multi-agent strategy reasons through the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical decision\n",
    "tech_decision = \"\"\"Should we use React or Vue for building our new dashboard?\n",
    "\n",
    "Context:\n",
    "- Team: 2 frontend developers (both know React, 1 knows Vue)\n",
    "- Timeline: 3 months to MVP\n",
    "- Dashboard will have real-time data visualization, complex forms, and user management\n",
    "- We prioritize development speed and maintainability\n",
    "\"\"\"\n",
    "\n",
    "print(\"ü§ù Running adaptive team with reasoning...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = run_strategy_with_rationale(\n",
    "    \"adaptive_team\",\n",
    "    tech_decision,\n",
    "    n_experts=3,\n",
    "    refinement_rounds=1,\n",
    "    thinking_budget=1500,\n",
    "    provider=PROVIDER,\n",
    "    model=MODEL,\n",
    "    verbose=True  # See each expert's analysis in real-time\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"üìä FINAL REASONING:\")\n",
    "print(\"=\"*80)\n",
    "print(result.rationale)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ FINAL RECOMMENDATION:\")\n",
    "print(\"=\"*80)\n",
    "print(result.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Example 4: Pre-configured Reasoning Models\n",
    "\n",
    "Use pre-configured model setups optimized for reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-configured reasoning model\n",
    "reasoning_config = get_model_config(\"gpt-oss-20b-reasoning\")\n",
    "\n",
    "if reasoning_config:\n",
    "    print(f\"‚úÖ Loaded config: {reasoning_config.name}\")\n",
    "    print(f\"   Model: {reasoning_config.model}\")\n",
    "    print(f\"   Thinking budget: {reasoning_config.thinking_budget}\")\n",
    "    print(f\"   Temperature: {reasoning_config.temperature}\")\n",
    "    print(f\"   Description: {reasoning_config.description}\")\n",
    "    \n",
    "    # Use it\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Using pre-configured reasoning model...\\n\")\n",
    "    \n",
    "    result = llm_call(\n",
    "        \"What is 15% of 240?\",\n",
    "        **reasoning_config.to_kwargs()\n",
    "    )\n",
    "    \n",
    "    print(result.text)\n",
    "    print(f\"\\nTokens: {result.tokens_out}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Config not found. Run this to create defaults:\")\n",
    "    print(\"   from harness import get_config_manager\")\n",
    "    print(\"   get_config_manager().create_default_configs()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Example 5: Comparing Strategies with Reasoning\n",
    "\n",
    "Test how different strategies reason through the same problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# run_strategy is already imported in cell 1\nproblem = \"\"\"A farmer has 12 chickens and rabbits combined. The animals have 38 legs total.\nHow many chickens and how many rabbits does the farmer have?\"\"\"\n\nstrategies = [\n    (\"single\", {}),\n    (\"self_consistency\", {\"n_samples\": 3}),\n    (\"adaptive_team\", {\"n_experts\": 2, \"refinement_rounds\": 0})\n]\n\nfor strategy_name, kwargs in strategies:\n    print(f\"\\n{'='*80}\")\n    print(f\"üîÑ Strategy: {strategy_name}\")\n    print(f\"{'='*80}\\n\")\n    \n    result = run_strategy(\n        strategy_name,\n        problem,\n        provider=PROVIDER,\n        model=MODEL,\n        thinking_budget=1000,  # Give all strategies thinking budget\n        **kwargs\n    )\n    \n    print(result.output)\n    print(f\"\\n‚è±Ô∏è  Latency: {result.latency_s:.2f}s\")\n    print(f\"üí∞ Tokens: {result.tokens_out}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Thinking Budget** (`thinking_budget=N`)\n",
    "   - Allocates extra tokens for internal reasoning\n",
    "   - Useful for complex, multi-step problems\n",
    "   - Trade-off: 2-3x more tokens, but potentially better accuracy\n",
    "\n",
    "2. **Rationale Extraction** (`ask_with_reasoning`, `llm_call_with_rationale`)\n",
    "   - Get models to explain their reasoning explicitly\n",
    "   - Separates reasoning from final answer\n",
    "   - Useful for transparency and debugging\n",
    "\n",
    "3. **Multi-Agent Reasoning** (`run_strategy_with_rationale`)\n",
    "   - See how each agent reasons through problems\n",
    "   - Understand consensus formation\n",
    "   - Identify where agents agree/disagree\n",
    "\n",
    "### When to Use\n",
    "\n",
    "| Feature | Best For | Avoid For |\n",
    "|---------|----------|----------|\n",
    "| Thinking Budget | Math, logic, multi-step reasoning | Simple factoid questions |\n",
    "| Rationale Extraction | Transparency, debugging, complex decisions | Speed-critical applications |\n",
    "| Multi-Agent Reasoning | Strategic decisions, complex problems | Well-defined, single-domain questions |\n",
    "\n",
    "### Cost Impact\n",
    "\n",
    "- **Thinking budget**: 2-3x token cost\n",
    "- **Rationale extraction**: 1.5-2x token cost (explicit reasoning)\n",
    "- **Combined**: 3-5x baseline cost\n",
    "\n",
    "**Recommendation**: Use for high-value decisions where accuracy matters more than cost.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Test on your specific problem domains\n",
    "2. Tune `thinking_budget` (500-4000 tokens)\n",
    "3. Measure accuracy improvement vs. cost increase\n",
    "4. Use rationale to improve prompts and strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}