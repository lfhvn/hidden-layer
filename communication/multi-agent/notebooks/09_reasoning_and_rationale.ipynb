{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Reasoning & Rationale Extraction\n",
    "\n",
    "**Purpose:** Demonstrate thinking budget and rationale extraction features\n",
    "\n",
    "**Date:** 2025-01-27  \n",
    "**Author:** Leif Haven Martinson  \n",
    "\n",
    "## Features\n",
    "\n",
    "1. **Thinking Budget** - Allocate extra tokens for internal reasoning\n",
    "2. **Rationale Extraction** - Get models to explain their reasoning\n",
    "3. **Multi-Agent Reasoning** - See how each agent reasons through problems\n",
    "\n",
    "## When to Use\n",
    "\n",
    "- ‚úÖ Complex problems requiring multi-step reasoning\n",
    "- ‚úÖ Strategic decisions with trade-offs\n",
    "- ‚úÖ Understanding why models give certain answers\n",
    "- ‚úÖ Debugging incorrect answers\n",
    "- ‚úÖ Building transparent AI systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from harness import (\n",
    "    llm_call,\n",
    "    llm_call_with_rationale,\n",
    "    ask_with_reasoning,\n",
    "    run_strategy_with_rationale,\n",
    "    get_model_config\n",
    ")\n",
    "from harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n",
    "\n",
    "PROVIDER = DEFAULT_PROVIDER\n",
    "MODEL = DEFAULT_MODEL\n",
    "\n",
    "print(f\"‚úÖ Setup complete\")\n",
    "print(f\"   Provider: {PROVIDER}\")\n",
    "print(f\"   Model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Example 1: Thinking Budget\n",
    "\n",
    "Compare responses with and without thinking budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math problem that benefits from reasoning\n",
    "problem = \"\"\"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning \n",
    "and bakes muffins for her friends every day with four. She sells the remainder \n",
    "at the farmers' market daily for $2 per fresh duck egg. How much in dollars \n",
    "does she make every day at the farmers' market?\"\"\"\n",
    "\n",
    "print(\"üîµ WITHOUT thinking budget:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response_basic = llm_call(\n",
    "    problem,\n",
    "    provider=PROVIDER,\n",
    "    model=MODEL,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(response_basic.text)\n",
    "print(f\"\\nTokens: {response_basic.tokens_out}\")\n",
    "print(f\"Latency: {response_basic.latency_s:.2f}s\")\n",
    "\n",
    "print(\"\\n\\nüü¢ WITH thinking budget (2000 tokens):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response_thinking = llm_call(\n",
    "    problem,\n",
    "    provider=PROVIDER,\n",
    "    model=MODEL,\n",
    "    temperature=0.7,\n",
    "    thinking_budget=2000  # Extra reasoning budget\n",
    ")\n",
    "\n",
    "print(response_thinking.text)\n",
    "print(f\"\\nTokens: {response_thinking.tokens_out}\")\n",
    "print(f\"Latency: {response_thinking.latency_s:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° Key Insight: Thinking budget may improve accuracy on complex problems\")\n",
    "print(f\"   Token overhead: {response_thinking.tokens_out - response_basic.tokens_out} tokens\")\n",
    "print(f\"   Latency overhead: {response_thinking.latency_s - response_basic.latency_s:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Example 2: Rationale Extraction\n",
    "\n",
    "Get the model to explain its reasoning before giving the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategic decision problem\n",
    "decision = \"\"\"Our startup has $500k runway for 6 months. Should we:\n",
    "A) Hire 3 engineers and build faster\n",
    "B) Hire 1 engineer + 1 sales person to get revenue\n",
    "C) Keep current team and extend runway to 12 months\n",
    "\"\"\"\n",
    "\n",
    "print(\"üß† Getting reasoning + answer...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = ask_with_reasoning(\n",
    "    decision,\n",
    "    provider=PROVIDER,\n",
    "    model=MODEL,\n",
    "    thinking_budget=2000\n",
    ")\n",
    "\n",
    "print(\"REASONING:\")\n",
    "print(\"‚îÄ\"*80)\n",
    "print(result.rationale)\n",
    "\n",
    "print(\"\\n\\nFINAL ANSWER:\")\n",
    "print(\"‚îÄ\"*80)\n",
    "print(result.answer)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(f\"üíæ Metadata:\")\n",
    "print(f\"   Tokens: {result.llm_response.tokens_out}\")\n",
    "print(f\"   Latency: {result.llm_response.latency_s:.2f}s\")\n",
    "print(f\"   Provider: {result.llm_response.provider}\")\n",
    "print(f\"   Model: {result.llm_response.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Example 3: Multi-Agent Reasoning\n",
    "\n",
    "See how each agent in a multi-agent strategy reasons through the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical decision\n",
    "tech_decision = \"\"\"Should we use React or Vue for building our new dashboard?\n",
    "\n",
    "Context:\n",
    "- Team: 2 frontend developers (both know React, 1 knows Vue)\n",
    "- Timeline: 3 months to MVP\n",
    "- Dashboard will have real-time data visualization, complex forms, and user management\n",
    "- We prioritize development speed and maintainability\n",
    "\"\"\"\n",
    "\n",
    "print(\"ü§ù Running adaptive team with reasoning...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = run_strategy_with_rationale(\n",
    "    \"adaptive_team\",\n",
    "    tech_decision,\n",
    "    n_experts=3,\n",
    "    refinement_rounds=1,\n",
    "    thinking_budget=1500,\n",
    "    provider=PROVIDER,\n",
    "    model=MODEL,\n",
    "    verbose=True  # See each expert's analysis in real-time\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"üìä FINAL REASONING:\")\n",
    "print(\"=\"*80)\n",
    "print(result.rationale)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ FINAL RECOMMENDATION:\")\n",
    "print(\"=\"*80)\n",
    "print(result.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Example 4: Pre-configured Reasoning Models\n",
    "\n",
    "Use pre-configured model setups optimized for reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-configured reasoning model\n",
    "reasoning_config = get_model_config(\"gpt-oss-20b-reasoning\")\n",
    "\n",
    "if reasoning_config:\n",
    "    print(f\"‚úÖ Loaded config: {reasoning_config.name}\")\n",
    "    print(f\"   Model: {reasoning_config.model}\")\n",
    "    print(f\"   Thinking budget: {reasoning_config.thinking_budget}\")\n",
    "    print(f\"   Temperature: {reasoning_config.temperature}\")\n",
    "    print(f\"   Description: {reasoning_config.description}\")\n",
    "    \n",
    "    # Use it\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Using pre-configured reasoning model...\\n\")\n",
    "    \n",
    "    result = llm_call(\n",
    "        \"What is 15% of 240?\",\n",
    "        **reasoning_config.to_kwargs()\n",
    "    )\n",
    "    \n",
    "    print(result.text)\n",
    "    print(f\"\\nTokens: {result.tokens_out}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Config not found. Run this to create defaults:\")\n",
    "    print(\"   from harness import get_config_manager\")\n",
    "    print(\"   get_config_manager().create_default_configs()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Example 5: Comparing Strategies with Reasoning\n",
    "\n",
    "Test how different strategies reason through the same problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from harness import run_strategy\n",
    "\n",
    "problem = \"\"\"A farmer has 12 chickens and rabbits combined. The animals have 38 legs total.\n",
    "How many chickens and how many rabbits does the farmer have?\"\"\"\n",
    "\n",
    "strategies = [\n",
    "    (\"single\", {}),\n",
    "    (\"self_consistency\", {\"n_samples\": 3}),\n",
    "    (\"adaptive_team\", {\"n_experts\": 2, \"refinement_rounds\": 0})\n",
    "]\n",
    "\n",
    "for strategy_name, kwargs in strategies:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîÑ Strategy: {strategy_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    result = run_strategy(\n",
    "        strategy_name,\n",
    "        problem,\n",
    "        provider=PROVIDER,\n",
    "        model=MODEL,\n",
    "        thinking_budget=1000,  # Give all strategies thinking budget\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    print(result.output)\n",
    "    print(f\"\\n‚è±Ô∏è  Latency: {result.latency_s:.2f}s\")\n",
    "    print(f\"üí∞ Tokens: {result.tokens_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Thinking Budget** (`thinking_budget=N`)\n",
    "   - Allocates extra tokens for internal reasoning\n",
    "   - Useful for complex, multi-step problems\n",
    "   - Trade-off: 2-3x more tokens, but potentially better accuracy\n",
    "\n",
    "2. **Rationale Extraction** (`ask_with_reasoning`, `llm_call_with_rationale`)\n",
    "   - Get models to explain their reasoning explicitly\n",
    "   - Separates reasoning from final answer\n",
    "   - Useful for transparency and debugging\n",
    "\n",
    "3. **Multi-Agent Reasoning** (`run_strategy_with_rationale`)\n",
    "   - See how each agent reasons through problems\n",
    "   - Understand consensus formation\n",
    "   - Identify where agents agree/disagree\n",
    "\n",
    "### When to Use\n",
    "\n",
    "| Feature | Best For | Avoid For |\n",
    "|---------|----------|----------|\n",
    "| Thinking Budget | Math, logic, multi-step reasoning | Simple factoid questions |\n",
    "| Rationale Extraction | Transparency, debugging, complex decisions | Speed-critical applications |\n",
    "| Multi-Agent Reasoning | Strategic decisions, complex problems | Well-defined, single-domain questions |\n",
    "\n",
    "### Cost Impact\n",
    "\n",
    "- **Thinking budget**: 2-3x token cost\n",
    "- **Rationale extraction**: 1.5-2x token cost (explicit reasoning)\n",
    "- **Combined**: 3-5x baseline cost\n",
    "\n",
    "**Recommendation**: Use for high-value decisions where accuracy matters more than cost.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Test on your specific problem domains\n",
    "2. Tune `thinking_budget` (500-4000 tokens)\n",
    "3. Measure accuracy improvement vs. cost increase\n",
    "4. Use rationale to improve prompts and strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
