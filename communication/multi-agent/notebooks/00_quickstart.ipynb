{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Layer Quickstart\n",
    "\n",
    "This notebook verifies your environment and demonstrates the core harness APIs.\n",
    "Feel free to edit and save under a new name once everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# 1. Add repository root to path to import harness\nimport sys\nsys.path.append('../../../')  # Go up to repo root from notebooks/\n\n# 2. Import required modules\nimport os\nimport shutil\nfrom datetime import datetime\n\n# 3. Import harness core functionality\nfrom harness import ExperimentConfig, ExperimentResult, get_tracker, llm_call, run_strategy\nfrom harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n\n# 4. Detect which providers are available on this system\nproviders = []\n\n# 4a. Check if Ollama CLI is installed\nif shutil.which(\"ollama\"):\n    providers.append(\"ollama\")\n\n# 4b. Check if MLX (Apple Silicon) is available\ntry:\n    import mlx.core  # noqa: F401\n    providers.append(\"mlx\")\nexcept Exception:\n    pass\n\n# 4c. Check if Anthropic API key is set\nif os.getenv(\"ANTHROPIC_API_KEY\"):\n    providers.append(\"anthropic\")\n\n# 4d. Check if OpenAI API key is set  \nif os.getenv(\"OPENAI_API_KEY\"):\n    providers.append(\"openai\")\n\n# 5. If no providers detected, fall back to default\nif not providers:\n    providers.append(DEFAULT_PROVIDER)\n\n# 6. Print detected configuration\nprint(\"Detected providers:\", providers)\nprint(\"Default model:\", DEFAULT_MODEL)\n\n# 7. Select the first available provider for testing\nselected_provider = providers[0]\nprint(\"Using provider for smoke test:\", selected_provider)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# 1. Attempt a simple LLM call to verify the provider works\ntry:\n    # 2. Make test call with simple prompt\n    response = llm_call(\n        \"Say 'Hidden Layer online.'\",  # Simple verification prompt\n        provider=selected_provider,     # Use detected provider\n        model=None                      # Use default model for provider\n    )\n    \n    # 3. Print response details if successful\n    print(\"Model:\", response.model)\n    print(\"Latency (s):\", f\"{response.latency_s:.2f}\")\n    print(\"Response:\", response.text.strip())\n    \n# 4. If call fails, raise informative error\nexcept Exception as exc:\n    raise RuntimeError(\"LLM smoke test failed. Check provider configuration.\") from exc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# 1. Test the strategy system with a simple single-model strategy\nstrategy_result = run_strategy(\n    \"single\",  # Use single-model strategy (simplest strategy)\n    \"List two reasons multi-agent strategies can outperform single models.\",  # Test prompt\n    provider=selected_provider,  # Use detected provider\n    model=None,                  # Use default model\n    verbose=False               # Don't print intermediate steps\n)\n\n# 2. Extract model name from result for use in tracking\nselected_model = strategy_result.metadata.get(\"model\") if strategy_result.metadata else DEFAULT_MODEL\n\n# 3. Print strategy execution results\nprint(\"Strategy:\", strategy_result.strategy_name)\nprint(\"Latency (s):\", f\"{strategy_result.latency_s:.2f}\")\nprint(\"Output:\\n\", strategy_result.output.strip())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# 1. Create experiment configuration for tracking\nconfig = ExperimentConfig(\n    experiment_name=\"quickstart_smoke\",  # Identifier for this experiment\n    task_type=\"demo\",                    # Type of task\n    strategy=\"single\",                   # Strategy used\n    provider=selected_provider,          # Provider used\n    model=selected_model,                # Model used\n)\n\n# 2. Initialize tracker and start experiment\ntracker = get_tracker()\nrun_dir = tracker.start_experiment(config)\n\n# 3. Create result record with all execution details\nresult_record = ExperimentResult(\n    config=config,\n    task_input=\"Quickstart notebook demo task\",\n    output=strategy_result.output,\n    latency_s=strategy_result.latency_s,\n    tokens_in=strategy_result.tokens_in,\n    tokens_out=strategy_result.tokens_out,\n    cost_usd=strategy_result.cost_usd,\n    eval_scores={},           # No evaluation scores for smoke test\n    eval_metadata={},         # No evaluation metadata\n    success=True,             # Test passed\n    error=None,               # No errors\n)\n\n# 4. Log the result to tracker\ntracker.log_result(result_record)\n\n# 5. Finish experiment and get summary\nsummary = tracker.finish_experiment()\n\n# 6. Print experiment summary if available\nif summary:\n    print(\"Logged experiment in:\", run_dir)\n    print(\"Summary runs:\", summary.get(\"total_runs\"))\n    print(\"Average latency (s):\", f\"{(summary.get('avg_latency_s') or 0):.2f}\")\n    print(\"Timestamp:\", datetime.fromisoformat(summary[\"timestamp\"]))\nelse:\n    print(\"No summary generated (no results logged).\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}