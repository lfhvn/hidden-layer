{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Layer Quickstart\n",
    "\n",
    "This notebook verifies your environment and demonstrates the core harness APIs.\n",
    "Feel free to edit and save under a new name once everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# SETUP: Imports and Provider Detection\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Don't edit this cell - just run it to see what's available\n\nimport sys\nimport os\nimport shutil\nfrom datetime import datetime\n\n# Add paths for imports\nsys.path.append('../../../')  # Go up to repo root (for harness)\nsys.path.append('../')        # Go up to multi-agent/ (for multi_agent module)\n\n# Import multi-agent specific functions\nfrom multi_agent import run_strategy, STRATEGIES\n\n# Import core harness functionality\nfrom harness import ExperimentConfig, ExperimentResult, get_tracker, llm_call\nfrom harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n\nprint(\"âœ“ Imports successful\\n\")\n\n# Detect what providers are available\navailable_providers = {}\n\n# Check MLX (Apple Silicon local inference)\ntry:\n    import mlx.core\n    available_providers['mlx'] = {\n        'name': 'MLX (Apple Silicon)',\n        'default_model': 'mlx-community/Llama-3.2-3B-Instruct-4bit',\n        'available': True\n    }\nexcept ImportError:\n    available_providers['mlx'] = {'name': 'MLX (Apple Silicon)', 'available': False}\n\n# Check Ollama (local inference)\nif shutil.which(\"ollama\"):\n    available_providers['ollama'] = {\n        'name': 'Ollama (local)',\n        'default_model': 'llama3.2:latest',\n        'available': True\n    }\nelse:\n    available_providers['ollama'] = {'name': 'Ollama (local)', 'available': False}\n\n# Check Anthropic API\nif os.getenv(\"ANTHROPIC_API_KEY\"):\n    available_providers['anthropic'] = {\n        'name': 'Anthropic Claude',\n        'default_model': 'claude-3-5-sonnet-20241022',\n        'available': True\n    }\nelse:\n    available_providers['anthropic'] = {'name': 'Anthropic Claude', 'available': False}\n\n# Check OpenAI API\nif os.getenv(\"OPENAI_API_KEY\"):\n    available_providers['openai'] = {\n        'name': 'OpenAI',\n        'default_model': 'gpt-4o',\n        'available': True\n    }\nelse:\n    available_providers['openai'] = {'name': 'OpenAI', 'available': False}\n\n# Print what's available\nprint(\"Available Providers:\")\nfor key, info in available_providers.items():\n    status = \"âœ“\" if info['available'] else \"âœ—\"\n    model_info = f\" (default: {info['default_model']})\" if info.get('default_model') else \"\"\n    print(f\"  {status} {info['name']}{model_info}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Next: Edit the configuration cell below to choose your settings\")\nprint(\"=\"*70)\n"
  },
  {
   "cell_type": "code",
   "id": "bp8xljfmyyu",
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# âš™ï¸  CONFIGURATION - EDIT THIS CELL TO CHANGE SETTINGS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# 1. Choose your provider (pick one that shows âœ“ above)\nPROVIDER = 'mlx'  # Options: 'mlx', 'ollama', 'anthropic', 'openai'\n\n# 2. Choose your model (or None to use the default)\nMODEL = None  # Examples:\n              # MLX: 'mlx-community/Llama-3.2-3B-Instruct-4bit'\n              # Ollama: 'llama3.2:latest', 'qwen2.5:latest'\n              # Anthropic: 'claude-3-5-sonnet-20241022'\n              # OpenAI: 'gpt-4o'\n\n# 3. Hyperparameters\nTEMPERATURE = 0.7      # Range: 0.0 (focused) to 1.0 (creative)\nMAX_TOKENS = 2048      # Maximum length of response\nSYSTEM_PROMPT = None   # Optional: role/persona for the model\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Validate and set defaults\nif PROVIDER not in available_providers or not available_providers[PROVIDER]['available']:\n    available = [k for k, v in available_providers.items() if v['available']]\n    raise ValueError(f\"Provider '{PROVIDER}' is not available. Choose from: {available}\")\n\nif MODEL is None:\n    MODEL = available_providers[PROVIDER]['default_model']\n\n# Print configuration\nprint(\"Current Configuration:\")\nprint(\"=\"*70)\nprint(f\"ğŸ“ Provider: {PROVIDER}\")\nprint(f\"ğŸ¤– Model: {MODEL}\")\nprint(f\"ğŸŒ¡ï¸  Temperature: {TEMPERATURE}\")\nprint(f\"ğŸ“ Max Tokens: {MAX_TOKENS}\")\nprint(f\"ğŸ’¬ System Prompt: {SYSTEM_PROMPT or '(none)'}\")\nprint(\"=\"*70)\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# SMOKE TEST: Verify the LLM provider works\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ntry:\n    response = llm_call(\n        \"Say 'Hidden Layer online.'\",\n        provider=PROVIDER,\n        model=MODEL,\n        temperature=TEMPERATURE,\n        max_tokens=MAX_TOKENS,\n        system_prompt=SYSTEM_PROMPT\n    )\n    \n    print(\"âœ“ LLM call successful!\")\n    print(f\"Model: {response.model}\")\n    print(f\"Latency: {response.latency_s:.2f}s\")\n    print(f\"Response: {response.text.strip()}\\n\")\n    \nexcept Exception as exc:\n    print(f\"âœ— LLM call failed!\")\n    print(f\"Provider: {PROVIDER}\")\n    print(f\"Model: {MODEL}\")\n    print(f\"Error: {exc}\")\n    raise\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# STRATEGY TEST: Run a multi-agent strategy\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nstrategy_result = run_strategy(\n    \"single\",  # Strategy: 'single', 'debate', 'consensus', 'self_consistency', etc.\n    \"List two reasons multi-agent strategies can outperform single models.\",\n    provider=PROVIDER,\n    model=MODEL,\n    temperature=TEMPERATURE,\n    max_tokens=MAX_TOKENS,\n    verbose=False\n)\n\nprint(\"âœ“ Strategy executed successfully!\")\nprint(f\"Strategy: {strategy_result.strategy_name}\")\nprint(f\"Latency: {strategy_result.latency_s:.2f}s\")\nprint(f\"Output:\\n{strategy_result.output.strip()}\\n\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# EXPERIMENT TRACKING: Log results\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nconfig = ExperimentConfig(\n    experiment_name=\"quickstart_smoke\",\n    task_type=\"demo\",\n    strategy=\"single\",\n    provider=PROVIDER,\n    model=MODEL,\n)\n\ntracker = get_tracker()\nrun_dir = tracker.start_experiment(config)\n\nresult_record = ExperimentResult(\n    config=config,\n    task_input=\"Quickstart notebook demo task\",\n    output=strategy_result.output,\n    latency_s=strategy_result.latency_s,\n    tokens_in=strategy_result.tokens_in,\n    tokens_out=strategy_result.tokens_out,\n    cost_usd=strategy_result.cost_usd,\n    eval_scores={},\n    eval_metadata={},\n    success=True,\n    error=None,\n)\n\ntracker.log_result(result_record)\nsummary = tracker.finish_experiment()\n\nprint(\"âœ“ Experiment logged!\")\nprint(f\"Run directory: {run_dir}\")\nif summary:\n    print(f\"Total runs: {summary.get('total_runs')}\")\n    print(f\"Average latency: {summary.get('avg_latency_s', 0):.2f}s\")\n    print(f\"Timestamp: {summary['timestamp']}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}