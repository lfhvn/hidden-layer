{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hidden Layer Quickstart\n",
        "\n",
        "This notebook verifies your environment and demonstrates the core harness APIs.\n",
        "Feel free to edit and save under a new name once everything is working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "from harness import ExperimentConfig, ExperimentResult, get_tracker, llm_call, run_strategy\n",
        "from harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n",
        "\n",
        "providers = []\n",
        "if shutil.which(\"ollama\"):\n",
        "    providers.append(\"ollama\")\n",
        "try:\n",
        "    import mlx.core  # noqa: F401\n",
        "    providers.append(\"mlx\")\n",
        "except Exception:\n",
        "    pass\n",
        "if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
        "    providers.append(\"anthropic\")\n",
        "if os.getenv(\"OPENAI_API_KEY\"):\n",
        "    providers.append(\"openai\")\n",
        "\n",
        "if not providers:\n",
        "    providers.append(DEFAULT_PROVIDER)\n",
        "\n",
        "print(\"Detected providers:\", providers)\n",
        "print(\"Default model:\", DEFAULT_MODEL)\n",
        "selected_provider = providers[0]\n",
        "print(\"Using provider for smoke test:\", selected_provider)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    response = llm_call(\"Say 'Hidden Layer online.'\", provider=selected_provider, model=None)\n",
        "    print(\"Model:\", response.model)\n",
        "    print(\"Latency (s):\", f\"{response.latency_s:.2f}\")\n",
        "    print(\"Response:\", response.text.strip())\n",
        "except Exception as exc:\n",
        "    raise RuntimeError(\"LLM smoke test failed. Check provider configuration.\") from exc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "strategy_result = run_strategy(\n",
        "            \"single\",\n",
        "            \"List two reasons multi-agent strategies can outperform single models.\",\n",
        "            provider=selected_provider,\n",
        "            model=None,\n",
        "            verbose=False,\n",
        "        )\n",
        "        selected_model = strategy_result.metadata.get(\"model\") if strategy_result.metadata else DEFAULT_MODEL\n",
        "        print(\"Strategy:\", strategy_result.strategy_name)\n",
        "        print(\"Latency (s):\", f\"{strategy_result.latency_s:.2f}\")\n",
        "        print(\"Output:\n",
        "\", strategy_result.output.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "config = ExperimentConfig(\n",
        "    experiment_name=\"quickstart_smoke\",\n",
        "    task_type=\"demo\",\n",
        "    strategy=\"single\",\n",
        "    provider=selected_provider,\n",
        "    model=selected_model,\n",
        ")\n",
        "tracker = get_tracker()\n",
        "run_dir = tracker.start_experiment(config)\n",
        "\n",
        "result_record = ExperimentResult(\n",
        "    config=config,\n",
        "    task_input=\"Quickstart notebook demo task\",\n",
        "    output=strategy_result.output,\n",
        "    latency_s=strategy_result.latency_s,\n",
        "    tokens_in=strategy_result.tokens_in,\n",
        "    tokens_out=strategy_result.tokens_out,\n",
        "    cost_usd=strategy_result.cost_usd,\n",
        "    eval_scores={},\n",
        "    eval_metadata={},\n",
        "    success=True,\n",
        "    error=None,\n",
        ")\n",
        "tracker.log_result(result_record)\n",
        "summary = tracker.finish_experiment()\n",
        "if summary:\n",
        "    print(\"Logged experiment in:\", run_dir)\n",
        "    print(\"Summary runs:\", summary.get(\"total_runs\"))\n",
        "    print(\"Average latency (s):\", f\"{(summary.get('avg_latency_s') or 0):.2f}\")\n",
        "    print(\"Timestamp:\", datetime.fromisoformat(summary[\"timestamp\"]))\n",
        "else:\n",
        "    print(\"No summary generated (no results logged).\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}