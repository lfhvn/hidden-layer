{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Experiments: Debate Strategy\n",
    "\n",
    "**Experiment:** Compare debate-based multi-agent vs single-model baselines  \n",
    "**Date:** [Fill in]  \n",
    "**Author:** Leif Haven Martinson  \n",
    "\n",
    "## Goals\n",
    "- Implement 2-agent debate with judge\n",
    "- Compare debate vs single-model baseline\n",
    "- Measure quality, latency, and cost tradeoffs\n",
    "- Identify when debate helps vs hurts\n",
    "\n",
    "## Hypotheses\n",
    "- Debate improves accuracy on tasks with multiple valid perspectives\n",
    "- Debate adds 2-3x latency but may justify cost with quality gains\n",
    "- Judge quality matters more than debater quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add repository root to path to import harness\nimport sys\nsys.path.append('../../../')  # Go up to repo root from notebooks/\n\n# 2. Import harness functions for running strategies and tracking\nfrom harness import (\n    llm_call,              # Single LLM call wrapper\n    run_strategy,          # Run any strategy (debate, single, etc.)\n    debate_strategy,       # Debate-specific strategy function\n    ExperimentConfig,      # Experiment configuration structure\n    ExperimentResult,      # Result logging structure\n    get_tracker,           # Get experiment tracker instance\n    evaluate_task          # Evaluate task results\n)\nfrom harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n\n# 3. Import data analysis and visualization libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 4. Enable inline plotting in notebook\n%matplotlib inline\n\n# 5. Set visual style for plots\nsns.set_style('whitegrid')\n\n# 6. Show current configuration\nprint(\"=\"*70)\nprint(\"üîß DEBATE NOTEBOOK CONFIGURATION\")\nprint(\"=\"*70)\nprint(f\"üìç Provider: {DEFAULT_PROVIDER}\")\nprint(f\"ü§ñ Model: {DEFAULT_MODEL or '(default for provider)'}\")\nprint(\"=\"*70)\nprint(\"\\nüí° TO CHANGE: Edit cell 7 (Configure Debate Models section)\")\nprint(\"   Set PROVIDER and MODEL variables there\")\nprint(\"=\"*70)\nprint(\"\\n‚úÖ Setup complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debate Strategy Implementation\n",
    "\n",
    "Two agents debate, then a judge decides the best answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Test the debate strategy with a simple question\n# The harness already has debate strategy built-in!\ntest_result = run_strategy(\n    \"debate\",              # Use debate strategy\n    \"What is 2+2?\",       # Simple test question\n    n_debaters=2,          # Use 2 debating agents\n    provider=\"ollama\"      # Use Ollama local provider\n)\n\n# 2. Print the final answer from the judge\nprint(f\"Debate result: {test_result.output}\")\n\n# 3. Print execution time\nprint(f\"Latency: {test_result.latency_s:.2f}s\")\n\n# 4. Print metadata about the debate\nprint(f\"\\nNumber of debaters: {test_result.metadata['n_debaters']}\")\n\n# 5. Confirm system is ready\nprint(\"\\nDebate system ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tasks from Baseline\n",
    "\n",
    "Use the same tasks for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Define the same reasoning tasks used in baseline experiments for fair comparison\nreasoning_tasks = [\n    {\n        \"id\": \"logic_01\",\n        \"category\": \"logical_reasoning\",\n        \"input\": \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n        \"expected\": \"No, this doesn't follow logically.\"  # Tests understanding of logical syllogisms\n    },\n    {\n        \"id\": \"math_01\",\n        \"category\": \"arithmetic\",\n        \"input\": \"A train travels 120 km in 2 hours, then 180 km in 3 hours. What is its average speed for the entire journey?\",\n        \"expected\": \"60 km/h\"  # Tests arithmetic reasoning: total distance / total time\n    },\n    {\n        \"id\": \"reasoning_01\",\n        \"category\": \"causal_reasoning\",\n        \"input\": \"Studies show that people who drink coffee tend to live longer. Does this mean coffee causes longevity?\",\n        \"expected\": \"No, correlation doesn't imply causation.\"  # Tests causal reasoning understanding\n    },\n    {\n        \"id\": \"planning_01\",\n        \"category\": \"planning\",\n        \"input\": \"You need to be at a meeting 30 km away at 2 PM. Traffic is heavy (20 km/h). It's now 1:15 PM. Can you make it on time?\",\n        \"expected\": \"No. Travel time = 1.5 hours.\"  # Tests planning and time calculation\n    },\n    {\n        \"id\": \"pattern_01\",\n        \"category\": \"pattern_recognition\",\n        \"input\": \"What comes next in this sequence: 2, 6, 12, 20, 30, ?\",\n        \"expected\": \"42\"  # Tests pattern recognition: differences of 4, 6, 8, 10, 12...\n    }\n]\n\n# 2. Confirm tasks are loaded\nprint(f\"Loaded {len(reasoning_tasks)} tasks\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Debate Models\n",
    "\n",
    "Start with same model for debaters and judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ==================== CONFIGURE MODEL HERE ====================\n# 1. Import default model/provider settings from harness\nfrom harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n\n# 2. SET YOUR PROVIDER AND MODEL HERE:\n# Uncomment and edit these lines to override:\n# PROVIDER = \"mlx\"  # Options: \"mlx\", \"ollama\", \"anthropic\", \"openai\"\n# MODEL = \"mlx-community/Llama-3.2-3B-Instruct-4bit\"  # Your model name\n\n# Or use defaults:\ntry:\n    PROVIDER\nexcept NameError:\n    PROVIDER = DEFAULT_PROVIDER\n    MODEL = DEFAULT_MODEL\n\n# 3. Configure debate parameters\nNUM_DEBATERS = 2        # Number of agents that will debate (2+ required)\n\n# 4. Print configuration for verification\nprint(\"=\"*70)\nprint(\"üéØ DEBATE CONFIGURATION\")\nprint(\"=\"*70)\nprint(f\"Strategy: {NUM_DEBATERS} debaters + 1 judge\")\nprint(f\"Provider: {PROVIDER}\")\nprint(f\"Model: {MODEL or '(default for provider)'}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Debate Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Configure experiment tracking for debate runs\nconfig = ExperimentConfig(\n    experiment_name=f\"debate_{NUM_DEBATERS}agents\",  # Descriptive experiment name\n    task_type=\"reasoning\",                            # Type of tasks being run\n    strategy=\"debate\",                                # Strategy being tested\n    provider=PROVIDER,                                # Provider used\n    model=MODEL,                                      # Model used\n    n_agents=NUM_DEBATERS,                           # Number of debate agents\n    notes=f\"{NUM_DEBATERS} debaters with judge\"     # Additional context\n)\n\n# 2. Initialize tracker and start experiment\ntracker = get_tracker()\ntracker.start_experiment(config)\n\n# 3. Run debate on each task and track results\nfor i, task in enumerate(reasoning_tasks):\n    print(f\"\\n{'='*60}\")\n    print(f\"Running debate on: {task['id']}\")\n    print(f\"{'='*60}\")\n    \n    # 3a. Run debate strategy on this task\n    result = run_strategy(\n        \"debate\",          # Strategy name\n        task['input'],     # Task input text\n        n_debaters=NUM_DEBATERS,  # Number of debaters\n        provider=PROVIDER,  # Provider to use\n        model=MODEL        # Model to use\n    )\n    \n    # 3b. Display arguments from each debater\n    for j, arg in enumerate(result.metadata['arguments']):\n        print(f\"\\nDebater {j+1}: {arg[:100]}...\")  # First 100 chars\n    \n    # 3c. Display judge's final verdict\n    print(f\"\\nJudge verdict: {result.output[:100]}...\")\n    print(f\"Total latency: {result.latency_s:.2f}s\")\n    \n    # 3d. Create experiment result record\n    exp_result = ExperimentResult(\n        config=config,\n        task_input=task['input'],\n        output=result.output,\n        latency_s=result.latency_s,\n        tokens_in=result.tokens_in,\n        tokens_out=result.tokens_out,\n        cost_usd=result.cost_usd,\n        eval_metadata={\n            'task_id': task['id'],\n            'category': task['category'],\n            'expected': task['expected'],\n            'arguments': result.metadata['arguments']\n        }\n    )\n    \n    # 3e. Evaluate the result against expected answer\n    exp_result.eval_scores = evaluate_task(task, result.output)\n    \n    # 3f. Log result to tracker\n    tracker.log_result(exp_result)\n    print(\"‚úì Logged\")\n\n# 4. Finish experiment and get summary\nsummary = tracker.finish_experiment()\nprint(\"\\n\" + \"=\"*60)\nprint(\"Debate experiment complete!\")\nprint(f\"Saved to: {tracker.current_run_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare: Single vs Debate\n",
    "\n",
    "Load baseline and debate results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare experiments\n",
    "# Note: You'll need to run the baseline experiment first (01_baseline_experiments.ipynb)\n",
    "# Then update the paths below with your actual experiment directories\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# List available experiments\n",
    "exp_dir = Path(\"../experiments\")\n",
    "if exp_dir.exists():\n",
    "    experiments = sorted([d.name for d in exp_dir.iterdir() if d.is_dir()])\n",
    "    print(\"Available experiments:\")\n",
    "    for exp in experiments:\n",
    "        print(f\"  - {exp}\")\n",
    "else:\n",
    "    print(\"No experiments found yet. Run baseline experiments first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison metrics\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': 'Single Model',\n",
    "        'Avg Latency (ms)': baseline_data['summary']['stats']['avg_latency_ms'],\n",
    "        'Total Cost ($)': baseline_data['summary']['stats']['total_cost'],\n",
    "        'Runs': baseline_data['summary']['num_runs']\n",
    "    },\n",
    "    {\n",
    "        'Strategy': f'Debate ({NUM_DEBATERS} agents)',\n",
    "        'Avg Latency (ms)': debate_data['summary']['stats']['avg_latency_ms'],\n",
    "        'Total Cost ($)': debate_data['summary']['stats']['total_cost'],\n",
    "        'Runs': debate_data['summary']['num_runs']\n",
    "    }\n",
    "])\n",
    "\n",
    "# Calculate overhead\n",
    "latency_overhead = (debate_data['summary']['stats']['avg_latency_ms'] / \n",
    "                   baseline_data['summary']['stats']['avg_latency_ms'])\n",
    "cost_overhead = (debate_data['summary']['stats']['total_cost'] / \n",
    "                baseline_data['summary']['stats']['total_cost'])\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\nDebate Overhead:\")\n",
    "print(f\"  Latency: {latency_overhead:.1f}x slower\")\n",
    "print(f\"  Cost: {cost_overhead:.1f}x more expensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency comparison\n",
    "ax = axes[0]\n",
    "comparison_df.plot.bar(x='Strategy', y='Avg Latency (ms)', ax=ax, legend=False, color=['steelblue', 'coral'])\n",
    "ax.set_title('Latency: Single vs Debate', fontsize=14, weight='bold')\n",
    "ax.set_ylabel('Milliseconds')\n",
    "ax.set_xlabel('')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Cost comparison\n",
    "ax = axes[1]\n",
    "comparison_df.plot.bar(x='Strategy', y='Total Cost ($)', ax=ax, legend=False, color=['steelblue', 'coral'])\n",
    "ax.set_title('Cost: Single vs Debate', fontsize=14, weight='bold')\n",
    "ax.set_ylabel('USD')\n",
    "ax.set_xlabel('')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Analysis\n",
    "\n",
    "Compare actual outputs to assess quality differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(task_id: str):\n",
    "    \"\"\"Show baseline vs debate outputs side by side.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Task: {task_id}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Get task\n",
    "    task = next(t for t in reasoning_tasks if t['id'] == task_id)\n",
    "    print(f\"Input: {task['input']}\\n\")\n",
    "    print(f\"Expected: {task['expected']}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get baseline\n",
    "    baseline_run = next(r for r in baseline_data['runs'] if r['metadata']['task_id'] == task_id)\n",
    "    print(f\"\\nBASELINE (Single Model):\")\n",
    "    print(f\"Output: {baseline_run['output']}\")\n",
    "    print(f\"Latency: {baseline_run['latency_ms']:.0f}ms\")\n",
    "    \n",
    "    # Get debate\n",
    "    debate_run = next(r for r in debate_data['runs'] if r['metadata']['task_id'] == task_id)\n",
    "    print(f\"\\nDEBATE ({NUM_DEBATERS} agents):\")\n",
    "    for i, arg in enumerate(debate_run['metadata']['arguments']):\n",
    "        print(f\"  Debater {i+1}: {arg[:80]}...\")\n",
    "    print(f\"\\nJudge Verdict: {debate_run['output']}\")\n",
    "    print(f\"Latency: {debate_run['latency_ms']:.0f}ms\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Compare first task\n",
    "compare_outputs(reasoning_tasks[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse all tasks\n",
    "for task in reasoning_tasks:\n",
    "    compare_outputs(task['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Quantitative\n",
    "- [Fill in after running]\n",
    "- Debate adds Xx latency overhead\n",
    "- Cost increased by X%\n",
    "\n",
    "### Qualitative\n",
    "- [Observations on quality differences]\n",
    "- When did debate help?\n",
    "- When did debate hurt?\n",
    "\n",
    "### Hypotheses\n",
    "- [ ] Debate improves accuracy on multi-perspective tasks\n",
    "- [ ] Latency overhead is 2-3x\n",
    "- [ ] Judge quality matters more than debaters\n",
    "\n",
    "## Next Experiments\n",
    "1. Test with larger judge model (13B or 30B)\n",
    "2. Add 3rd debater\n",
    "3. Try specialized debater roles\n",
    "4. Test on creative/open-ended tasks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}