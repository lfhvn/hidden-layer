{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Model Introspection\n",
    "\n",
    "Test introspection capabilities of frontier models (Claude, GPT-4) using prompt-based steering.\n",
    "\n",
    "## Key Difference from MLX Introspection\n",
    "\n",
    "**MLX Models**: True activation steering - We inject concept vectors directly into model activations.\n",
    "\n",
    "**API Models**: Prompt-based steering - We simulate concept injection using system prompts.\n",
    "\n",
    "## Why Test API Models?\n",
    "\n",
    "1. **Frontier capability comparison** - Claude Opus 4 showed highest introspection in the paper\n",
    "2. **Benchmark local models** - Compare your MLX models against SOTA\n",
    "3. **Natural introspection** - Test if models can self-report without any steering\n",
    "4. **Method comparison** - Does prompt-steering approximate activation steering?\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure you have API keys set:\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=\"your-key\"\n",
    "export OPENAI_API_KEY=\"your-key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add parent directory to path for code imports\nimport sys\nsys.path.append('../code')\n\n# 2. Import OS module to check for API keys\nimport os\n\n# 3. Import harness functions for API-based introspection\nfrom harness import (\n    run_strategy,                        # Run introspection strategy\n    APIIntrospectionTester,              # Test API models for introspection\n    NATURAL_INTROSPECTION_PROMPTS        # Pre-defined introspection prompts\n)\n\n# 4. Check which API keys are available\nhas_anthropic = bool(os.getenv(\"ANTHROPIC_API_KEY\"))\nhas_openai = bool(os.getenv(\"OPENAI_API_KEY\"))\n\n# 5. Print availability status\nprint(f\"Anthropic API: {'✓' if has_anthropic else '✗ (set ANTHROPIC_API_KEY)'}\")\nprint(f\"OpenAI API: {'✓' if has_openai else '✗ (set OPENAI_API_KEY)'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Prompt-Based Steering\n",
    "\n",
    "Simulate concept injection using system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Test prompt-based steering with Claude (if API key available)\nif has_anthropic:\n    print(\"Testing Claude 3.5 Sonnet...\\n\")\n    \n    # 2. Run introspection with prompt-based concept steering\n    result = run_strategy(\n        \"introspection\",                    # Introspection strategy\n        task_input=\"Describe a typical day at work\",  # Neutral task\n        concept=\"happiness\",                 # Concept to simulate via prompting\n        task_type=\"detection\",               # Ask if concept is present\n        provider=\"anthropic\",                # Use Anthropic API\n        model=\"claude-3-5-sonnet-20241022\", # Specific Claude model\n        api_strength=\"moderate\",             # Strength of prompt-based steering\n        steering_style=\"implicit\",           # Subtle steering hints\n        temperature=0.7,                     # Sampling temperature\n        verbose=True                         # Show detailed output\n    )\n    \n    # 3. Print results summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"RESULTS:\")\n    print(\"=\"*60)\n    \n    # 4. Did model detect the simulated injection?\n    print(f\"Detected injection: {result.metadata['introspection_correct']}\")\n    \n    # 5. Confidence level of detection\n    print(f\"Confidence: {result.metadata['introspection_confidence']:.2f}\")\nelse:\n    print(\"Skipping - no Anthropic API key\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with GPT-4\n",
    "if has_openai:\n",
    "    print(\"Testing GPT-4o...\\n\")\n",
    "    \n",
    "    result = run_strategy(\n",
    "        \"introspection\",\n",
    "        task_input=\"Describe a typical day at work\",\n",
    "        concept=\"happiness\",\n",
    "        task_type=\"detection\",\n",
    "        provider=\"openai\",\n",
    "        model=\"gpt-4o\",\n",
    "        api_strength=\"moderate\",\n",
    "        steering_style=\"implicit\",\n",
    "        temperature=0.7,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Detected injection: {result.metadata['introspection_correct']}\")\n",
    "    print(f\"Confidence: {result.metadata['introspection_confidence']:.2f}\")\n",
    "else:\n",
    "    print(\"Skipping - no OpenAI API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Natural Introspection\n",
    "\n",
    "Can models report on their own reasoning without any steering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_anthropic:\n",
    "    tester = APIIntrospectionTester(\n",
    "        provider=\"anthropic\",\n",
    "        model=\"claude-3-5-sonnet-20241022\"\n",
    "    )\n",
    "    \n",
    "    # Test with a few natural introspection prompts\n",
    "    test_prompts = NATURAL_INTROSPECTION_PROMPTS[:5]\n",
    "    \n",
    "    results = tester.test_natural_introspection(\n",
    "        prompts=test_prompts,\n",
    "        temperature=0.7,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Calculate total cost\n",
    "    total_cost = sum(r.get('cost_usd', 0) for r in results)\n",
    "    print(f\"\\nTotal cost: ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Steering Strengths\n",
    "\n",
    "Test subtle, moderate, and strong prompt-based steering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_anthropic:\n",
    "    strengths = [\"subtle\", \"moderate\", \"strong\"]\n",
    "    \n",
    "    for strength in strengths:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing strength: {strength}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = run_strategy(\n",
    "            \"introspection\",\n",
    "            task_input=\"Think about emotions\",\n",
    "            concept=\"anger\",\n",
    "            task_type=\"detection\",\n",
    "            provider=\"anthropic\",\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            api_strength=strength,\n",
    "            steering_style=\"implicit\",\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Detected: {result.metadata['introspection_correct']}\")\n",
    "        print(f\"Confidence: {result.metadata['introspection_confidence']:.2f}\")\n",
    "        print(f\"Response: {result.output[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison\n",
    "\n",
    "Compare introspection across different frontier models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "models_to_test = []\n",
    "\n",
    "if has_anthropic:\n",
    "    models_to_test.extend([\n",
    "        (\"anthropic\", \"claude-3-5-sonnet-20241022\"),\n",
    "        (\"anthropic\", \"claude-3-5-haiku-20241022\"),\n",
    "    ])\n",
    "\n",
    "if has_openai:\n",
    "    models_to_test.extend([\n",
    "        (\"openai\", \"gpt-4o\"),\n",
    "        (\"openai\", \"gpt-4o-mini\"),\n",
    "    ])\n",
    "\n",
    "results = []\n",
    "concepts = [\"happiness\", \"sadness\", \"anger\"]\n",
    "\n",
    "for provider, model in models_to_test:\n",
    "    print(f\"\\nTesting {model}...\")\n",
    "    \n",
    "    for concept in concepts:\n",
    "        result = run_strategy(\n",
    "            \"introspection\",\n",
    "            task_input=\"Describe your current feelings\",\n",
    "            concept=concept,\n",
    "            task_type=\"detection\",\n",
    "            provider=provider,\n",
    "            model=model,\n",
    "            api_strength=\"moderate\",\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'provider': provider,\n",
    "            'model': model,\n",
    "            'concept': concept,\n",
    "            'correct': result.metadata['introspection_correct'],\n",
    "            'confidence': result.metadata['introspection_confidence']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(df.groupby('model').agg({'correct': 'mean', 'confidence': 'mean'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implicit vs Explicit Steering\n",
    "\n",
    "Compare implicit prompting (subtle hints) vs explicit prompting (direct statements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_anthropic:\n",
    "    styles = [\"implicit\", \"explicit\"]\n",
    "    \n",
    "    for style in styles:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing style: {style}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = run_strategy(\n",
    "            \"introspection\",\n",
    "            task_input=\"Describe a memory\",\n",
    "            concept=\"happiness\",\n",
    "            task_type=\"detection\",\n",
    "            provider=\"anthropic\",\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            api_strength=\"moderate\",\n",
    "            steering_style=style,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Detected: {result.metadata['introspection_correct']}\")\n",
    "        print(f\"Steered response: {result.output[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings & Observations\n",
    "\n",
    "### Expected Results (from paper):\n",
    "\n",
    "- **Claude Opus 4**: Highest introspective awareness (~70-80% accuracy)\n",
    "- **GPT-4**: Good introspection (~60-70% accuracy)\n",
    "- **Smaller models**: Lower accuracy (~40-50%)\n",
    "\n",
    "### Limitations of Prompt-Based Steering:\n",
    "\n",
    "1. **Less precise** than activation steering\n",
    "2. **Dependent on instruction following** - model must cooperate with system prompt\n",
    "3. **Can't control layer depth** - only surface-level influence\n",
    "4. **Cost** - API calls are expensive for large experiments\n",
    "\n",
    "### When to Use API Testing:\n",
    "\n",
    "✅ **Good for:**\n",
    "- Benchmarking local models against SOTA\n",
    "- Testing natural introspection (no steering needed)\n",
    "- Quick validation of paper findings\n",
    "- Comparing frontier model capabilities\n",
    "\n",
    "❌ **Not good for:**\n",
    "- Fine-grained layer analysis\n",
    "- Causal mechanistic studies\n",
    "- High-volume experiments (expensive)\n",
    "- Research requiring activation-level control\n",
    "\n",
    "### Research Value:\n",
    "\n",
    "The combination of MLX (activation steering) + API (prompt steering) enables:\n",
    "- **Method validation**: Does prompt steering approximate activation steering?\n",
    "- **Capability ceiling**: How much better are frontier models?\n",
    "- **Local model potential**: Can fine-tuning close the gap?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Compare MLX vs API**: Use `APIIntrospectionTester.compare_with_mlx()` to directly compare\n",
    "2. **Larger study**: Test more models, concepts, and steering strengths\n",
    "3. **Natural introspection**: Explore models' self-awareness without any steering\n",
    "4. **Fine-tuning impact**: Does fine-tuning improve introspection?\n",
    "\n",
    "See `INTROSPECTION.md` for full API documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}