{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introspection Experiments\n",
    "\n",
    "This notebook replicates the methodology from [Emergent Introspective Awareness in Large Language Models](https://transformer-circuits.pub/2025/introspection/index.html).\n",
    "\n",
    "## Key Question\n",
    "Can models accurately report on their internal states when we inject concept vectors into their activations?\n",
    "\n",
    "## Methodology\n",
    "1. **Extract concept vectors** - Get activation representations for specific concepts\n",
    "2. **Inject into activations** - Steer model behavior by adding concept vectors\n",
    "3. **Test introspection** - Ask model to report on its internal state\n",
    "4. **Evaluate accuracy** - Measure if model correctly identifies injected concepts\n",
    "\n",
    "## Requirements\n",
    "- MLX and mlx-lm installed\n",
    "- Local model (e.g., Llama 3.2 3B)\n",
    "- M4 Max or similar Apple Silicon hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlx_lm import load\n",
    "\n",
    "from harness.activation_steering import ActivationSteerer, SteeringConfig, ActivationCache\n",
    "from harness.concept_vectors import ConceptLibrary, build_emotion_library\n",
    "from harness.introspection_tasks import IntrospectionTaskGenerator, IntrospectionEvaluator, IntrospectionTaskType\n",
    "from harness import run_strategy, get_tracker, ExperimentConfig\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Extract Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small model for fast experimentation\n",
    "MODEL_NAME = \"mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model, tokenizer = load(MODEL_NAME)\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize activation steerer\n",
    "steerer = ActivationSteerer(model, tokenizer)\n",
    "\n",
    "# Build emotion concept library\n",
    "print(\"Building emotion concept library...\")\n",
    "emotion_library = build_emotion_library(\n",
    "    steerer=steerer,\n",
    "    layer=15,  # Middle layer\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(emotion_library)} emotion concepts\")\n",
    "print(f\"Concepts: {emotion_library.list_concepts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the library for later use\n",
    "emotion_library.save(\"../concepts/emotions_layer15.pkl\")\n",
    "emotion_library.export_json(\"../concepts/emotions_layer15.json\")\n",
    "print(\"✓ Library saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Basic Steering\n",
    "\n",
    "First, let's verify that activation steering works by comparing baseline vs steered outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test steering with happiness concept\n",
    "happiness_vec = emotion_library.get('happiness').vector\n",
    "\n",
    "prompt = \"Tell me a short story about a day at work.\"\n",
    "\n",
    "result = steerer.compare_with_baseline(\n",
    "    prompt=prompt,\n",
    "    concept_vector=happiness_vec,\n",
    "    config=SteeringConfig(\n",
    "        layer_idx=15,\n",
    "        strength=1.5,\n",
    "        strategy=\"add\"\n",
    "    ),\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE (no steering):\")\n",
    "print(\"=\"*60)\n",
    "print(result['baseline'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEERED (happiness injected):\")\n",
    "print(\"=\"*60)\n",
    "print(result['steered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introspection Tests: Detection\n",
    "\n",
    "Can the model notice when a concept has been injected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test detection with the introspection strategy\n",
    "result = run_strategy(\n",
    "    \"introspection\",\n",
    "    task_input=\"Describe a typical morning routine\",\n",
    "    concept=\"happiness\",\n",
    "    layer=15,\n",
    "    strength=2.0,\n",
    "    task_type=\"detection\",\n",
    "    provider=\"mlx\",\n",
    "    model=MODEL_NAME,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULT SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Introspection Correct: {result.metadata['introspection_correct']}\")\n",
    "print(f\"Confidence: {result.metadata['introspection_confidence']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Introspection Tests: Identification\n",
    "\n",
    "Can the model correctly identify which concept was injected from multiple choices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test identification\n",
    "result = run_strategy(\n",
    "    \"introspection\",\n",
    "    task_input=\"Think about emotions\",\n",
    "    concept=\"anger\",\n",
    "    distractors=[\"happiness\", \"sadness\", \"fear\"],\n",
    "    layer=15,\n",
    "    strength=1.5,\n",
    "    task_type=\"identification\",\n",
    "    provider=\"mlx\",\n",
    "    model=MODEL_NAME,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULT SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Correct Answer: anger\")\n",
    "print(f\"Model Identified Correctly: {result.metadata['introspection_correct']}\")\n",
    "print(f\"Confidence: {result.metadata['introspection_confidence']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Systematic Evaluation: Layer Sensitivity\n",
    "\n",
    "Test how introspection accuracy varies by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test across multiple layers\n",
    "layers_to_test = [5, 10, 15, 20, 25]\n",
    "concepts = [\"happiness\", \"sadness\", \"anger\"]\n",
    "strength = 1.5\n",
    "\n",
    "results = []\n",
    "\n",
    "for layer in layers_to_test:\n",
    "    print(f\"\\nTesting layer {layer}...\")\n",
    "    \n",
    "    for concept in concepts:\n",
    "        result = run_strategy(\n",
    "            \"introspection\",\n",
    "            task_input=\"Describe your current feelings\",\n",
    "            concept=concept,\n",
    "            layer=layer,\n",
    "            strength=strength,\n",
    "            task_type=\"detection\",\n",
    "            provider=\"mlx\",\n",
    "            model=MODEL_NAME,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'layer': layer,\n",
    "            'concept': concept,\n",
    "            'correct': result.metadata['introspection_correct'],\n",
    "            'confidence': result.metadata['introspection_confidence'],\n",
    "            'strength': strength\n",
    "        })\n",
    "\n",
    "df_layers = pd.DataFrame(results)\n",
    "print(\"\\n✓ Layer sweep complete\")\n",
    "df_layers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer sensitivity\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy by layer\n",
    "plt.subplot(1, 2, 1)\n",
    "accuracy_by_layer = df_layers.groupby('layer')['correct'].mean()\n",
    "accuracy_by_layer.plot(kind='bar')\n",
    "plt.title('Introspection Accuracy by Layer')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Confidence by layer\n",
    "plt.subplot(1, 2, 2)\n",
    "confidence_by_layer = df_layers.groupby('layer')['confidence'].mean()\n",
    "confidence_by_layer.plot(kind='bar', color='orange')\n",
    "plt.title('Average Confidence by Layer')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Confidence')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Systematic Evaluation: Steering Strength\n",
    "\n",
    "Test how introspection accuracy varies with steering strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different steering strengths\n",
    "strengths_to_test = [0.5, 1.0, 1.5, 2.0, 3.0]\n",
    "layer = 15  # Use middle layer\n",
    "concept = \"happiness\"\n",
    "\n",
    "strength_results = []\n",
    "\n",
    "for strength in strengths_to_test:\n",
    "    print(f\"Testing strength {strength}...\")\n",
    "    \n",
    "    # Run multiple trials for each strength\n",
    "    for trial in range(3):\n",
    "        result = run_strategy(\n",
    "            \"introspection\",\n",
    "            task_input=\"Describe a day at the beach\",\n",
    "            concept=concept,\n",
    "            layer=layer,\n",
    "            strength=strength,\n",
    "            task_type=\"detection\",\n",
    "            provider=\"mlx\",\n",
    "            model=MODEL_NAME,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        strength_results.append({\n",
    "            'strength': strength,\n",
    "            'trial': trial,\n",
    "            'correct': result.metadata['introspection_correct'],\n",
    "            'confidence': result.metadata['introspection_confidence'],\n",
    "            'concept': concept,\n",
    "            'layer': layer\n",
    "        })\n",
    "\n",
    "df_strength = pd.DataFrame(strength_results)\n",
    "print(\"\\n✓ Strength sweep complete\")\n",
    "df_strength.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize strength sensitivity\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "accuracy_by_strength = df_strength.groupby('strength')['correct'].agg(['mean', 'std'])\n",
    "plt.errorbar(\n",
    "    accuracy_by_strength.index,\n",
    "    accuracy_by_strength['mean'],\n",
    "    yerr=accuracy_by_strength['std'],\n",
    "    marker='o',\n",
    "    capsize=5,\n",
    "    linewidth=2\n",
    ")\n",
    "plt.title('Introspection Accuracy vs Steering Strength')\n",
    "plt.xlabel('Steering Strength')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Concept Specificity\n",
    "\n",
    "Are some concepts easier to detect than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all emotion concepts\n",
    "all_concepts = emotion_library.list_concepts()\n",
    "layer = 15\n",
    "strength = 1.5\n",
    "\n",
    "concept_results = []\n",
    "\n",
    "for concept in all_concepts:\n",
    "    print(f\"Testing concept: {concept}...\")\n",
    "    \n",
    "    # Run multiple trials\n",
    "    for trial in range(3):\n",
    "        result = run_strategy(\n",
    "            \"introspection\",\n",
    "            task_input=\"Reflect on your internal state\",\n",
    "            concept=concept,\n",
    "            layer=layer,\n",
    "            strength=strength,\n",
    "            task_type=\"detection\",\n",
    "            provider=\"mlx\",\n",
    "            model=MODEL_NAME,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        concept_results.append({\n",
    "            'concept': concept,\n",
    "            'trial': trial,\n",
    "            'correct': result.metadata['introspection_correct'],\n",
    "            'confidence': result.metadata['introspection_confidence']\n",
    "        })\n",
    "\n",
    "df_concepts = pd.DataFrame(concept_results)\n",
    "print(\"\\n✓ Concept sweep complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize concept detectability\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "accuracy_by_concept = df_concepts.groupby('concept')['correct'].agg(['mean', 'std'])\n",
    "accuracy_by_concept = accuracy_by_concept.sort_values('mean', ascending=False)\n",
    "\n",
    "plt.bar(range(len(accuracy_by_concept)), accuracy_by_concept['mean'])\n",
    "plt.errorbar(\n",
    "    range(len(accuracy_by_concept)),\n",
    "    accuracy_by_concept['mean'],\n",
    "    yerr=accuracy_by_concept['std'],\n",
    "    fmt='none',\n",
    "    color='black',\n",
    "    capsize=5\n",
    ")\n",
    "plt.xticks(range(len(accuracy_by_concept)), accuracy_by_concept.index, rotation=45)\n",
    "plt.title('Introspection Accuracy by Concept')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. False Positive Rate\n",
    "\n",
    "Do models hallucinate detections when nothing is injected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with zero steering (baseline false positive rate)\n",
    "from harness.introspection_tasks import IntrospectionTaskGenerator\n",
    "\n",
    "task_gen = IntrospectionTaskGenerator()\n",
    "evaluator = IntrospectionEvaluator()\n",
    "\n",
    "false_positive_tests = []\n",
    "\n",
    "for trial in range(10):\n",
    "    # Generate detection task\n",
    "    task = task_gen.detection_task(\n",
    "        concept=\"happiness\",\n",
    "        base_prompt=\"Describe your thoughts\",\n",
    "        layer=15,\n",
    "        strength=0.0  # No steering!\n",
    "    )\n",
    "    \n",
    "    # Generate without steering\n",
    "    from mlx_lm import generate as mlx_generate\n",
    "    prompt = f\"{task.base_prompt}\\n\\n{task.introspection_prompt}\"\n",
    "    response = mlx_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt=prompt,\n",
    "        temp=0.7,\n",
    "        max_tokens=100,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    result = evaluator.evaluate(task, response)\n",
    "    \n",
    "    false_positive_tests.append({\n",
    "        'trial': trial,\n",
    "        'detected': result.is_correct  # If True, it's a false positive!\n",
    "    })\n",
    "\n",
    "fp_rate = sum(t['detected'] for t in false_positive_tests) / len(false_positive_tests)\n",
    "print(f\"\\nFalse Positive Rate: {fp_rate:.1%}\")\n",
    "print(f\"(Lower is better - model should NOT detect concepts when nothing is injected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INTROSPECTION EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"\\nConcepts tested: {len(all_concepts)}\")\n",
    "print(f\"  {', '.join(all_concepts)}\")\n",
    "\n",
    "print(f\"\\n1. Overall Accuracy: {df_concepts['correct'].mean():.1%}\")\n",
    "print(f\"2. Best Layer: {accuracy_by_layer.idxmax()} (accuracy: {accuracy_by_layer.max():.1%})\")\n",
    "print(f\"3. Best Strength: {accuracy_by_strength['mean'].idxmax()} (accuracy: {accuracy_by_strength['mean'].max():.1%})\")\n",
    "print(f\"4. Easiest Concept: {accuracy_by_concept.index[0]} ({accuracy_by_concept['mean'].iloc[0]:.1%})\")\n",
    "print(f\"5. False Positive Rate: {fp_rate:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if accuracy_by_layer.idxmax() > 10:\n",
    "    print(\"✓ Middle/late layers show better introspection (matches paper)\")\n",
    "else:\n",
    "    print(\"⚠ Early layers unexpectedly effective - investigate further\")\n",
    "\n",
    "if accuracy_by_strength['mean'].max() > 0.5:\n",
    "    print(\"✓ Models show introspective awareness above chance\")\n",
    "else:\n",
    "    print(\"⚠ Introspection accuracy near chance - may need tuning\")\n",
    "\n",
    "if fp_rate < 0.3:\n",
    "    print(\"✓ Low false positive rate - model not hallucinating detections\")\n",
    "else:\n",
    "    print(\"⚠ High false positive rate - model may be guessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "### To Improve Results:\n",
    "1. **Test larger models** - 7B and 13B models may show better introspection\n",
    "2. **Tune layer selection** - Fine-tune which layers work best for your model\n",
    "3. **Optimize prompts** - Experiment with different introspection questions\n",
    "4. **Build richer concept library** - Add more concepts beyond emotions\n",
    "\n",
    "### To Extend Research:\n",
    "1. **Cross-model comparison** - Compare introspection across model families\n",
    "2. **Layer analysis** - Study how introspection changes through layers\n",
    "3. **Concept composition** - Test with multiple concepts injected\n",
    "4. **Fine-tuning impact** - Does fine-tuning improve introspection?\n",
    "\n",
    "### For Production Use:\n",
    "1. **Build production library** - Extract comprehensive concept set\n",
    "2. **Calibrate thresholds** - Determine optimal strength per concept\n",
    "3. **Validation suite** - Create standardized introspection benchmarks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
