{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Consensus Strategy: Multi-Agent Without Judge\n",
    "\n",
    "**Experiment:** Agents debate and build consensus amongst themselves (no separate judge)\n",
    "\n",
    "**Date:** [Fill in]  \n",
    "**Author:** Leif Haven Martinson  \n",
    "\n",
    "## Goals\n",
    "- Compare consensus-building vs judge-based debate\n",
    "- Measure convergence across rounds\n",
    "- Understand when agents naturally align vs diverge\n",
    "\n",
    "## Hypotheses\n",
    "- Consensus works better when there's objectively correct answers\n",
    "- Judge-based debate works better for subjective/creative tasks\n",
    "- More rounds lead to better convergence but diminishing returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from harness import (\n",
    "    llm_call,\n",
    "    run_strategy,\n",
    "    consensus_strategy,\n",
    "    ExperimentConfig,\n",
    "    ExperimentResult,\n",
    "    get_tracker,\n",
    "    evaluate_task\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify consensus is working\n",
    "# This uses default settings - see the configuration cell below to customize\n",
    "\n",
    "test_result = run_strategy(\n",
    "    \"consensus\",\n",
    "    \"What is 2+2?\",\n",
    "    n_agents=3,\n",
    "    n_rounds=2,\n",
    "    provider=\"ollama\",\n",
    "    verbose=False  # Set to True to see the full consensus process\n",
    ")\n",
    "\n",
    "print(f\"Consensus result: {test_result.output[:100]}...\")\n",
    "print(f\"Latency: {test_result.latency_s:.2f}s\")\n",
    "print(f\"\\nNumber of agents: {test_result.metadata['n_agents']}\")\n",
    "print(f\"Number of rounds: {test_result.metadata['n_rounds']}\")\n",
    "print(\"\\nâœ… Consensus system ready! Scroll down to customize parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Tasks from Baseline\n",
    "\n",
    "Use the same tasks for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same tasks as baseline and debate experiments\n",
    "reasoning_tasks = [\n",
    "    {\n",
    "        \"id\": \"logic_01\",\n",
    "        \"category\": \"logical_reasoning\",\n",
    "        \"input\": \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n",
    "        \"expected\": \"No, this doesn't follow logically.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"math_01\",\n",
    "        \"category\": \"arithmetic\",\n",
    "        \"input\": \"A train travels 120 km in 2 hours, then 180 km in 3 hours. What is its average speed for the entire journey?\",\n",
    "        \"expected\": \"60 km/h\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"reasoning_01\",\n",
    "        \"category\": \"causal_reasoning\",\n",
    "        \"input\": \"Studies show that people who drink coffee tend to live longer. Does this mean coffee causes longevity?\",\n",
    "        \"expected\": \"No, correlation doesn't imply causation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"planning_01\",\n",
    "        \"category\": \"planning\",\n",
    "        \"input\": \"You need to be at a meeting 30 km away at 2 PM. Traffic is heavy (20 km/h). It's now 1:15 PM. Can you make it on time?\",\n",
    "        \"expected\": \"No. Travel time = 1.5 hours.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"pattern_01\",\n",
    "        \"category\": \"pattern_recognition\",\n",
    "        \"input\": \"What comes next in this sequence: 2, 6, 12, 20, 30, ?\",\n",
    "        \"expected\": \"42\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(reasoning_tasks)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Configure Consensus Parameters\n",
    "\n",
    "**CUSTOMIZE HERE:** Adjust these settings to control the consensus behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONSENSUS CONFIGURATION - EDIT THESE VALUES\n",
    "# ========================================\n",
    "\n",
    "from harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n",
    "\n",
    "# Number of agents working toward consensus\n",
    "NUM_AGENTS = 3\n",
    "\n",
    "# Number of consensus rounds\n",
    "# Each round, agents see others' positions and refine their own\n",
    "# More rounds = more convergence, but also more cost/latency\n",
    "NUM_ROUNDS = 3  # ðŸ”§ CHANGE THIS to adjust consensus depth\n",
    "\n",
    "# Model configuration\n",
    "PROVIDER = DEFAULT_PROVIDER\n",
    "MODEL = DEFAULT_MODEL\n",
    "\n",
    "# ========================================\n",
    "# AGENT PERSPECTIVES - EDIT THESE PROMPTS\n",
    "# ========================================\n",
    "# Give each agent a different perspective or role\n",
    "# Set to None to use default prompts\n",
    "\n",
    "AGENT_PROMPTS = [\n",
    "    # Agent 1: Analytical perspective\n",
    "    \"\"\"You are an analytical thinker who breaks down problems logically and systematically.\n",
    "Focus on facts, data, and structured reasoning.\"\"\",\n",
    "\n",
    "    # Agent 2: Creative perspective\n",
    "    \"\"\"You are a creative thinker who explores unconventional solutions and lateral thinking.\n",
    "Consider multiple perspectives and novel approaches.\"\"\",\n",
    "\n",
    "    # Agent 3: Practical perspective\n",
    "    \"\"\"You are a practical thinker focused on real-world applicability and common sense.\n",
    "Consider what actually works in practice.\"\"\",\n",
    "\n",
    "    # Add more agents if NUM_AGENTS > 3:\n",
    "    # \"\"\"You are a risk-aware thinker who considers potential downsides...\"\"\",\n",
    "]\n",
    "\n",
    "# Set to None to use default neutral prompts:\n",
    "# AGENT_PROMPTS = None\n",
    "\n",
    "print(f\"âœ… Consensus configuration:\")\n",
    "print(f\"   - {NUM_AGENTS} agents\")\n",
    "print(f\"   - {NUM_ROUNDS} round(s) of consensus building\")\n",
    "print(f\"   - Provider: {PROVIDER}\")\n",
    "print(f\"   - Model: {MODEL}\")\n",
    "print(f\"   - Custom agent prompts: {len(AGENT_PROMPTS) if AGENT_PROMPTS else 0}\")\n",
    "print(f\"\\nðŸ’¡ Note: Consensus has NO judge - agents synthesize their own final answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Run Consensus Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run consensus experiment with tracking\n",
    "config = ExperimentConfig(\n",
    "    experiment_name=f\"consensus_{NUM_AGENTS}agents_{NUM_ROUNDS}rounds\",\n",
    "    task_type=\"reasoning\",\n",
    "    strategy=\"consensus\",\n",
    "    provider=PROVIDER,\n",
    "    model=MODEL,\n",
    "    n_agents=NUM_AGENTS,\n",
    "    notes=f\"{NUM_AGENTS} agents, {NUM_ROUNDS} rounds, no judge\"\n",
    ")\n",
    "\n",
    "tracker = get_tracker()\n",
    "tracker.start_experiment(config)\n",
    "\n",
    "for i, task in enumerate(reasoning_tasks):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Building consensus on: {task['id']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Run consensus strategy with custom configuration\n",
    "    result = run_strategy(\n",
    "        \"consensus\",\n",
    "        task['input'],\n",
    "        n_agents=NUM_AGENTS,\n",
    "        n_rounds=NUM_ROUNDS,  # ðŸ”§ Using configured rounds\n",
    "        provider=PROVIDER,\n",
    "        model=MODEL,\n",
    "        agent_prompts=AGENT_PROMPTS,  # ðŸ”§ Using custom agent perspectives\n",
    "        verbose=True  # Set to False to hide streaming output\n",
    "    )\n",
    "    \n",
    "    # Display consensus results\n",
    "    print(f\"\\nðŸ“Š Consensus completed:\")\n",
    "    print(f\"   - Rounds: {NUM_ROUNDS}\")\n",
    "    print(f\"   - Total latency: {result.latency_s:.2f}s\")\n",
    "    \n",
    "    print(f\"\\n   Final positions after {NUM_ROUNDS} rounds:\")\n",
    "    for j, pos in enumerate(result.metadata['all_positions']):\n",
    "        print(f\"\\n   Agent {j+1}: {pos[:100]}...\")\n",
    "    \n",
    "    print(f\"\\n   Synthesized consensus: {result.output[:100]}...\")\n",
    "    \n",
    "    # Log result\n",
    "    exp_result = ExperimentResult(\n",
    "        config=config,\n",
    "        task_input=task['input'],\n",
    "        output=result.output,\n",
    "        latency_s=result.latency_s,\n",
    "        tokens_in=result.tokens_in,\n",
    "        tokens_out=result.tokens_out,\n",
    "        cost_usd=result.cost_usd,\n",
    "        eval_metadata={\n",
    "            'task_id': task['id'],\n",
    "            'category': task['category'],\n",
    "            'expected': task['expected'],\n",
    "            'final_positions': result.metadata['all_positions'],\n",
    "            'n_rounds': NUM_ROUNDS\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Evaluate the result\n",
    "    exp_result.eval_scores = evaluate_task(task, result.output)\n",
    "    \n",
    "    tracker.log_result(exp_result)\n",
    "    print(\"âœ“ Logged\")\n",
    "\n",
    "summary = tracker.finish_experiment()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Consensus experiment complete!\")\n",
    "print(f\"Saved to: {tracker.current_run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Compare: Debate (with Judge) vs Consensus (no Judge)\n",
    "\n",
    "Load results from both strategies for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare experiments\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# List available experiments\n",
    "exp_dir = Path(\"../experiments\")\n",
    "if exp_dir.exists():\n",
    "    experiments = sorted([d.name for d in exp_dir.iterdir() if d.is_dir()])\n",
    "    print(\"Available experiments:\")\n",
    "    \n",
    "    debate_exps = [e for e in experiments if 'debate' in e]\n",
    "    consensus_exps = [e for e in experiments if 'consensus' in e]\n",
    "    \n",
    "    print(\"\\nDebate experiments:\")\n",
    "    for exp in debate_exps:\n",
    "        print(f\"  - {exp}\")\n",
    "    \n",
    "    print(\"\\nConsensus experiments:\")\n",
    "    for exp in consensus_exps:\n",
    "        print(f\"  - {exp}\")\n",
    "else:\n",
    "    print(\"No experiments found yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add comparison code once you have both debate and consensus results\n",
    "# Compare:\n",
    "# - Latency (consensus likely slower - more rounds, no single judge)\n",
    "# - Quality (when does each work better?)\n",
    "# - Convergence (do agents actually agree?)\n",
    "# - Cost (token usage comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Analyze Convergence Over Rounds\n",
    "\n",
    "How much do agent positions change across rounds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze position similarity across rounds\n",
    "# - Extract final positions from metadata\n",
    "# - Measure similarity (e.g., word overlap, embedding similarity)\n",
    "# - Plot convergence over rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Quantitative\n",
    "- [Fill in after running]\n",
    "- Consensus adds Xx latency vs debate\n",
    "- Convergence rate across rounds\n",
    "\n",
    "### Qualitative\n",
    "- [Observations on consensus quality]\n",
    "- When did agents converge well?\n",
    "- When did they diverge or fail to reach agreement?\n",
    "- Comparison to judge-based approach\n",
    "\n",
    "### Hypotheses\n",
    "- [ ] Consensus works better for objective questions\n",
    "- [ ] Debate works better for subjective tasks\n",
    "- [ ] More rounds improve convergence (with diminishing returns)\n",
    "\n",
    "## Next Experiments\n",
    "1. Test with different agent perspectives (domain experts)\n",
    "2. Vary number of rounds (1, 2, 3, 5) to find optimal\n",
    "3. Mix consensus + judge (agents build consensus, then external judge validates)\n",
    "4. Test on creative/open-ended tasks where consensus may struggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
