{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Experiments: Debate Strategy\n",
    "\n",
    "**Experiment:** Compare debate-based multi-agent vs single-model baselines  \n",
    "**Date:** [Fill in]  \n",
    "**Author:** Leif Haven Martinson  \n",
    "\n",
    "## Goals\n",
    "- Implement 2-agent debate with judge\n",
    "- Compare debate vs single-model baseline\n",
    "- Measure quality, latency, and cost tradeoffs\n",
    "- Identify when debate helps vs hurts\n",
    "\n",
    "## Hypotheses\n",
    "- Debate improves accuracy on tasks with multiple valid perspectives\n",
    "- Debate adds 2-3x latency but may justify cost with quality gains\n",
    "- Judge quality matters more than debater quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from llm_providers import create_llm\n",
    "from experiment_logger import experiment, RunResult\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✅ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debate Strategy Implementation\n",
    "\n",
    "Two agents debate, then a judge decides the best answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebateSystem:\n",
    "    \"\"\"Multi-agent debate with judge.\"\"\"\n",
    "    \n",
    "    def __init__(self, debater_model: str, judge_model: str, num_debaters: int = 2):\n",
    "        self.num_debaters = num_debaters\n",
    "        \n",
    "        # Create debaters (all using same model for now)\n",
    "        self.debaters = [\n",
    "            create_llm(\"mlx\", debater_model, temperature=0.8)\n",
    "            for _ in range(num_debaters)\n",
    "        ]\n",
    "        \n",
    "        # Create judge (potentially different/larger model)\n",
    "        self.judge = create_llm(\"mlx\", judge_model, temperature=0.3)\n",
    "    \n",
    "    def run_debate(self, task_input: str) -> dict:\n",
    "        \"\"\"\n",
    "        Run a debate on the task.\n",
    "        \n",
    "        Returns:\n",
    "            {\n",
    "                'arguments': list of debater responses,\n",
    "                'verdict': judge's final answer,\n",
    "                'total_latency_ms': combined time,\n",
    "                'cost_estimate': total cost\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Phase 1: Debaters generate arguments\n",
    "        arguments = []\n",
    "        total_latency = 0\n",
    "        total_cost = 0\n",
    "        \n",
    "        for i, debater in enumerate(self.debaters):\n",
    "            prompt = f\"\"\"\n",
    "You are Debater {i+1} in a debate. Provide your best answer to this question.\n",
    "\n",
    "Question: {task_input}\n",
    "\n",
    "Your argument:\"\"\"\n",
    "            \n",
    "            result = debater.generate(prompt)\n",
    "            arguments.append(result['text'])\n",
    "            total_latency += result['latency_ms']\n",
    "            total_cost += result.get('cost_estimate', 0)\n",
    "        \n",
    "        # Phase 2: Judge evaluates arguments\n",
    "        judge_prompt = f\"\"\"\n",
    "You are a judge evaluating different arguments to a question.\n",
    "\n",
    "Question: {task_input}\n",
    "\n",
    "Arguments:\n",
    "\"\"\"\n",
    "        for i, arg in enumerate(arguments):\n",
    "            judge_prompt += f\"\\n\\nDebater {i+1}:\\n{arg}\"\n",
    "        \n",
    "        judge_prompt += \"\"\"\\n\\n\n",
    "Based on these arguments, provide the best final answer to the question.\n",
    "Consider accuracy, reasoning quality, and completeness.\n",
    "\n",
    "Final answer:\"\"\"\n",
    "        \n",
    "        verdict_result = self.judge.generate(judge_prompt)\n",
    "        total_latency += verdict_result['latency_ms']\n",
    "        total_cost += verdict_result.get('cost_estimate', 0)\n",
    "        \n",
    "        return {\n",
    "            'arguments': arguments,\n",
    "            'verdict': verdict_result['text'],\n",
    "            'total_latency_ms': total_latency,\n",
    "            'cost_estimate': total_cost,\n",
    "            'num_debaters': self.num_debaters\n",
    "        }\n",
    "\n",
    "print(\"Debate system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tasks from Baseline\n",
    "\n",
    "Use the same tasks for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same tasks as baseline experiment\n",
    "reasoning_tasks = [\n",
    "    {\n",
    "        \"id\": \"logic_01\",\n",
    "        \"category\": \"logical_reasoning\",\n",
    "        \"input\": \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n",
    "        \"expected\": \"No, this doesn't follow logically.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"math_01\",\n",
    "        \"category\": \"arithmetic\",\n",
    "        \"input\": \"A train travels 120 km in 2 hours, then 180 km in 3 hours. What is its average speed for the entire journey?\",\n",
    "        \"expected\": \"60 km/h\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"reasoning_01\",\n",
    "        \"category\": \"causal_reasoning\",\n",
    "        \"input\": \"Studies show that people who drink coffee tend to live longer. Does this mean coffee causes longevity?\",\n",
    "        \"expected\": \"No, correlation doesn't imply causation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"planning_01\",\n",
    "        \"category\": \"planning\",\n",
    "        \"input\": \"You need to be at a meeting 30 km away at 2 PM. Traffic is heavy (20 km/h). It's now 1:15 PM. Can you make it on time?\",\n",
    "        \"expected\": \"No. Travel time = 1.5 hours.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"pattern_01\",\n",
    "        \"category\": \"pattern_recognition\",\n",
    "        \"input\": \"What comes next in this sequence: 2, 6, 12, 20, 30, ?\",\n",
    "        \"expected\": \"42\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(reasoning_tasks)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Debate Models\n",
    "\n",
    "Start with same model for debaters and judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEBATER_MODEL = \"mlx-community/Llama-3.2-7B-Instruct-4bit\"\n",
    "JUDGE_MODEL = \"mlx-community/Llama-3.2-7B-Instruct-4bit\"  # Try larger model later\n",
    "NUM_DEBATERS = 2\n",
    "\n",
    "# Create debate system\n",
    "debate_system = DebateSystem(\n",
    "    debater_model=DEBATER_MODEL,\n",
    "    judge_model=JUDGE_MODEL,\n",
    "    num_debaters=NUM_DEBATERS\n",
    ")\n",
    "\n",
    "print(f\"Debate system: {NUM_DEBATERS} debaters + 1 judge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Debate Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run debate experiment\n",
    "model_config = {\n",
    "    \"debater\": DEBATER_MODEL,\n",
    "    \"judge\": JUDGE_MODEL,\n",
    "    \"num_debaters\": NUM_DEBATERS\n",
    "}\n",
    "\n",
    "with experiment(\n",
    "    name=f\"debate_{NUM_DEBATERS}agents\",\n",
    "    task=\"reasoning_suite\",\n",
    "    strategy=\"debate\",\n",
    "    model_config=model_config,\n",
    "    notes=f\"{NUM_DEBATERS} debaters with judge\"\n",
    ") as logger:\n",
    "    \n",
    "    for i, task in enumerate(reasoning_tasks):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Running debate on: {task['id']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Run debate\n",
    "        result = debate_system.run_debate(task['input'])\n",
    "        \n",
    "        # Display arguments\n",
    "        for j, arg in enumerate(result['arguments']):\n",
    "            print(f\"\\nDebater {j+1}: {arg[:100]}...\")\n",
    "        \n",
    "        print(f\"\\nJudge verdict: {result['verdict'][:100]}...\")\n",
    "        print(f\"Total latency: {result['total_latency_ms']:.0f}ms\")\n",
    "        \n",
    "        # Log result\n",
    "        run = RunResult(\n",
    "            run_id=i,\n",
    "            input=task['input'],\n",
    "            output=result['verdict'],\n",
    "            latency_ms=result['total_latency_ms'],\n",
    "            tokens=None,\n",
    "            cost_estimate=result['cost_estimate'],\n",
    "            metadata={\n",
    "                'task_id': task['id'],\n",
    "                'category': task['category'],\n",
    "                'expected': task['expected'],\n",
    "                'arguments': result['arguments'],\n",
    "                'num_debaters': result['num_debaters']\n",
    "            }\n",
    "        )\n",
    "        logger.log_run(run)\n",
    "        print(\"✓ Logged\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Debate experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare: Single vs Debate\n",
    "\n",
    "Load baseline and debate results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_logger import ExperimentComparison\n",
    "\n",
    "comp = ExperimentComparison()\n",
    "\n",
    "# Get most recent baseline and debate experiments\n",
    "all_exps = comp.list_experiments()\n",
    "baseline_exp = next(e for e in all_exps if e['strategy'] == 'single')\n",
    "debate_exp = next(e for e in all_exps if e['strategy'] == 'debate')\n",
    "\n",
    "# Load data\n",
    "baseline_data = comp.load_experiment(baseline_exp['id'])\n",
    "debate_data = comp.load_experiment(debate_exp['id'])\n",
    "\n",
    "print(\"Loaded experiments:\")\n",
    "print(f\"  Baseline: {baseline_exp['name']}\")\n",
    "print(f\"  Debate: {debate_exp['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison metrics\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': 'Single Model',\n",
    "        'Avg Latency (ms)': baseline_data['summary']['stats']['avg_latency_ms'],\n",
    "        'Total Cost ($)': baseline_data['summary']['stats']['total_cost'],\n",
    "        'Runs': baseline_data['summary']['num_runs']\n",
    "    },\n",
    "    {\n",
    "        'Strategy': f'Debate ({NUM_DEBATERS} agents)',\n",
    "        'Avg Latency (ms)': debate_data['summary']['stats']['avg_latency_ms'],\n",
    "        'Total Cost ($)': debate_data['summary']['stats']['total_cost'],\n",
    "        'Runs': debate_data['summary']['num_runs']\n",
    "    }\n",
    "])\n",
    "\n",
    "# Calculate overhead\n",
    "latency_overhead = (debate_data['summary']['stats']['avg_latency_ms'] / \n",
    "                   baseline_data['summary']['stats']['avg_latency_ms'])\n",
    "cost_overhead = (debate_data['summary']['stats']['total_cost'] / \n",
    "                baseline_data['summary']['stats']['total_cost'])\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\nDebate Overhead:\")\n",
    "print(f\"  Latency: {latency_overhead:.1f}x slower\")\n",
    "print(f\"  Cost: {cost_overhead:.1f}x more expensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency comparison\n",
    "ax = axes[0]\n",
    "comparison_df.plot.bar(x='Strategy', y='Avg Latency (ms)', ax=ax, legend=False, color=['steelblue', 'coral'])\n",
    "ax.set_title('Latency: Single vs Debate', fontsize=14, weight='bold')\n",
    "ax.set_ylabel('Milliseconds')\n",
    "ax.set_xlabel('')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Cost comparison\n",
    "ax = axes[1]\n",
    "comparison_df.plot.bar(x='Strategy', y='Total Cost ($)', ax=ax, legend=False, color=['steelblue', 'coral'])\n",
    "ax.set_title('Cost: Single vs Debate', fontsize=14, weight='bold')\n",
    "ax.set_ylabel('USD')\n",
    "ax.set_xlabel('')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Analysis\n",
    "\n",
    "Compare actual outputs to assess quality differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(task_id: str):\n",
    "    \"\"\"Show baseline vs debate outputs side by side.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Task: {task_id}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Get task\n",
    "    task = next(t for t in reasoning_tasks if t['id'] == task_id)\n",
    "    print(f\"Input: {task['input']}\\n\")\n",
    "    print(f\"Expected: {task['expected']}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get baseline\n",
    "    baseline_run = next(r for r in baseline_data['runs'] if r['metadata']['task_id'] == task_id)\n",
    "    print(f\"\\nBASELINE (Single Model):\")\n",
    "    print(f\"Output: {baseline_run['output']}\")\n",
    "    print(f\"Latency: {baseline_run['latency_ms']:.0f}ms\")\n",
    "    \n",
    "    # Get debate\n",
    "    debate_run = next(r for r in debate_data['runs'] if r['metadata']['task_id'] == task_id)\n",
    "    print(f\"\\nDEBATE ({NUM_DEBATERS} agents):\")\n",
    "    for i, arg in enumerate(debate_run['metadata']['arguments']):\n",
    "        print(f\"  Debater {i+1}: {arg[:80]}...\")\n",
    "    print(f\"\\nJudge Verdict: {debate_run['output']}\")\n",
    "    print(f\"Latency: {debate_run['latency_ms']:.0f}ms\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Compare first task\n",
    "compare_outputs(reasoning_tasks[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse all tasks\n",
    "for task in reasoning_tasks:\n",
    "    compare_outputs(task['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Quantitative\n",
    "- [Fill in after running]\n",
    "- Debate adds Xx latency overhead\n",
    "- Cost increased by X%\n",
    "\n",
    "### Qualitative\n",
    "- [Observations on quality differences]\n",
    "- When did debate help?\n",
    "- When did debate hurt?\n",
    "\n",
    "### Hypotheses\n",
    "- [ ] Debate improves accuracy on multi-perspective tasks\n",
    "- [ ] Latency overhead is 2-3x\n",
    "- [ ] Judge quality matters more than debaters\n",
    "\n",
    "## Next Experiments\n",
    "1. Test with larger judge model (13B or 30B)\n",
    "2. Add 3rd debater\n",
    "3. Try specialized debater roles\n",
    "4. Test on creative/open-ended tasks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
