{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Experiments: Debate Strategy\n",
    "\n",
    "**Experiment:** Compare debate-based multi-agent vs single-model baselines  \n",
    "**Date:** [Fill in]  \n",
    "**Author:** Leif Haven Martinson  \n",
    "\n",
    "## Goals\n",
    "- Implement 2-agent debate with judge\n",
    "- Compare debate vs single-model baseline\n",
    "- Measure quality, latency, and cost tradeoffs\n",
    "- Identify when debate helps vs hurts\n",
    "\n",
    "## Hypotheses\n",
    "- Debate improves accuracy on tasks with multiple valid perspectives\n",
    "- Debate adds 2-3x latency but may justify cost with quality gains\n",
    "- Judge quality matters more than debater quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from harness import (\n",
    "    llm_call,\n",
    "    run_strategy,\n",
    "    debate_strategy,\n",
    "    ExperimentConfig,\n",
    "    ExperimentResult,\n",
    "    get_tracker,\n",
    "    evaluate_task\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debate Strategy Implementation\n",
    "\n",
    "Two agents debate, then a judge decides the best answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate result: Answer 1 is best because it not only gives the correct result (4) but also explains why that result is correct, referencing standard arithmetic and providing a brief justification. This additional context makes the answer clearer and more informative than the terse statement in Answerâ€¯2.\n",
      "Latency: 33.73s\n",
      "\n",
      "Number of debaters: 2\n",
      "\n",
      "Debate system ready!\n"
     ]
    }
   ],
   "source": [
    "# The harness already has debate strategy built-in!\n",
    "# Let's test it with a simple example\n",
    "\n",
    "test_result = run_strategy(\n",
    "    \"debate\",\n",
    "    \"What is 2+2?\",\n",
    "    n_debaters=2,\n",
    "    provider=\"ollama\",\n",
    "    model=\"gpt-oss:20b\"\n",
    ")\n",
    "\n",
    "print(f\"Debate result: {test_result.output}\")\n",
    "print(f\"Latency: {test_result.latency_s:.2f}s\")\n",
    "print(f\"\\nNumber of debaters: {test_result.metadata['n_debaters']}\")\n",
    "print(\"\\nDebate system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tasks from Baseline\n",
    "\n",
    "Use the same tasks for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 tasks\n"
     ]
    }
   ],
   "source": [
    "# Same tasks as baseline experiment\n",
    "reasoning_tasks = [\n",
    "    {\n",
    "        \"id\": \"logic_01\",\n",
    "        \"category\": \"logical_reasoning\",\n",
    "        \"input\": \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n",
    "        \"expected\": \"No, this doesn't follow logically.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"math_01\",\n",
    "        \"category\": \"arithmetic\",\n",
    "        \"input\": \"A train travels 120 km in 2 hours, then 180 km in 3 hours. What is its average speed for the entire journey?\",\n",
    "        \"expected\": \"60 km/h\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"reasoning_01\",\n",
    "        \"category\": \"causal_reasoning\",\n",
    "        \"input\": \"Studies show that people who drink coffee tend to live longer. Does this mean coffee causes longevity?\",\n",
    "        \"expected\": \"No, correlation doesn't imply causation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"planning_01\",\n",
    "        \"category\": \"planning\",\n",
    "        \"input\": \"You need to be at a meeting 30 km away at 2 PM. Traffic is heavy (20 km/h). It's now 1:15 PM. Can you make it on time?\",\n",
    "        \"expected\": \"No. Travel time = 1.5 hours.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"pattern_01\",\n",
    "        \"category\": \"pattern_recognition\",\n",
    "        \"input\": \"What comes next in this sequence: 2, 6, 12, 20, 30, ?\",\n",
    "        \"expected\": \"42\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(reasoning_tasks)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Debate Models\n",
    "\n",
    "Start with same model for debaters and judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate configuration: 2 debaters + 1 judge\n",
      "Provider: ollama\n",
      "Model: gpt-oss:20b\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DEBATER_MODEL = \"gpt-oss:20b\"  # Using Ollama for speed\n",
    "JUDGE_MODEL = \"gpt-oss:20b\"     # Can use larger model later\n",
    "NUM_DEBATERS = 2\n",
    "PROVIDER = \"ollama\"\n",
    "\n",
    "print(f\"Debate configuration: {NUM_DEBATERS} debaters + 1 judge\")\n",
    "print(f\"Provider: {PROVIDER}\")\n",
    "print(f\"Model: {DEBATER_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Debate Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Started experiment: debate_2agents_20251026_141040_0e0a6d97\n",
      "ðŸ“ Logging to: experiments/debate_2agents_20251026_141040_0e0a6d97\n",
      "\n",
      "============================================================\n",
      "Running debate on: logic_01\n",
      "============================================================\n",
      "\n",
      "Debater 1: **Answer (Debaterâ€¯1)**  \n",
      "\n",
      "No. The two premises do not allow us to conclude that any roses fade quick...\n",
      "\n",
      "Debater 2: **No â€“ the inference is invalid.**\n",
      "\n",
      "1. **Formal structure**  \n",
      "   - Premiseâ€¯1: All roses are flowers....\n",
      "\n",
      "Judge verdict: **Answerâ€¯1 is best because** it fully explains the logical structure, shows why the inference fails,...\n",
      "Total latency: 149.42s\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model 'llama3.2:latest' not found (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     37\u001b[39m exp_result = ExperimentResult(\n\u001b[32m     38\u001b[39m     config=config,\n\u001b[32m     39\u001b[39m     task_input=task[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     }\n\u001b[32m     51\u001b[39m )\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Evaluate the result\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m exp_result.eval_scores = \u001b[43mevaluate_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m tracker.log_result(exp_result)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Logged\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hidden-layer/notebooks/../code/harness/evals.py:271\u001b[39m, in \u001b[36mevaluate_task\u001b[39m\u001b[34m(task, output, eval_type)\u001b[39m\n\u001b[32m    268\u001b[39m     scores[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m] = numeric_match(output, \u001b[38;5;28mfloat\u001b[39m(task[\u001b[33m'\u001b[39m\u001b[33mexpected\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m eval_type == \u001b[33m'\u001b[39m\u001b[33mllm_judge\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     judge_result = \u001b[43mllm_judge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcriteria\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moverall quality\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     scores[\u001b[33m'\u001b[39m\u001b[33mquality\u001b[39m\u001b[33m'\u001b[39m] = judge_result[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    277\u001b[39m     scores[\u001b[33m'\u001b[39m\u001b[33m_judge_reasoning\u001b[39m\u001b[33m'\u001b[39m] = judge_result[\u001b[33m'\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hidden-layer/notebooks/../code/harness/evals.py:128\u001b[39m, in \u001b[36mllm_judge\u001b[39m\u001b[34m(task_input, output, criteria, provider, model, scale)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03m    Use an LLM to judge the output quality.\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m        Dict with 'score' (normalized 0-1) and 'reasoning'\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    114\u001b[39m     judge_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are an expert evaluator. Rate the following response on a scale of 1-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m    115\u001b[39m \n\u001b[32m    116\u001b[39m \u001b[33mTask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_input\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33mReasoning: [brief explanation]\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     response = \u001b[43mllm_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjudge_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# Parse score\u001b[39;00m\n\u001b[32m    131\u001b[39m     score_match = re.search(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mScore:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+)\u001b[39m\u001b[33m'\u001b[39m, response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hidden-layer/notebooks/../code/harness/llm_provider.py:264\u001b[39m, in \u001b[36mllm_call\u001b[39m\u001b[34m(prompt, provider, model, **kwargs)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm_call\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, provider: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mollama\u001b[39m\u001b[33m\"\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mllama3.2:latest\u001b[39m\u001b[33m\"\u001b[39m, **kwargs) -> LLMResponse:\n\u001b[32m    251\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[33;03m    Convenience function for calling LLMs.\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    262\u001b[39m \u001b[33;03m        response = llm_call(\"What is 2+2?\", provider=\"anthropic\", model=\"claude-3-5-sonnet-20241022\")\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hidden-layer/notebooks/../code/harness/llm_provider.py:57\u001b[39m, in \u001b[36mLLMProvider.call\u001b[39m\u001b[34m(self, prompt, provider, model, temperature, max_tokens, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._call_mlx(prompt, model, temperature, max_tokens, **kwargs)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m provider == \u001b[33m\"\u001b[39m\u001b[33mollama\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m provider == \u001b[33m\"\u001b[39m\u001b[33manthropic\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     59\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._call_anthropic(prompt, model, temperature, max_tokens, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hidden-layer/notebooks/../code/harness/llm_provider.py:111\u001b[39m, in \u001b[36mLLMProvider._call_ollama\u001b[39m\u001b[34m(self, prompt, model, temperature, max_tokens, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mollama\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     response = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_predict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMResponse(\n\u001b[32m    121\u001b[39m         text=response[\u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    122\u001b[39m         model=model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    127\u001b[39m         metadata={\u001b[33m'\u001b[39m\u001b[33mtotal_duration\u001b[39m\u001b[33m'\u001b[39m: response.get(\u001b[33m'\u001b[39m\u001b[33mtotal_duration\u001b[39m\u001b[33m'\u001b[39m)}\n\u001b[32m    128\u001b[39m     )\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hidden-layer/venv/lib/python3.13/site-packages/ollama/_client.py:256\u001b[39m, in \u001b[36mClient.generate\u001b[39m\u001b[34m(self, model, prompt, suffix, system, template, context, stream, think, raw, format, images, options, keep_alive)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    230\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    231\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    245\u001b[39m ) -> Union[GenerateResponse, Iterator[GenerateResponse]]:\n\u001b[32m    246\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m  Create a response using the requested model.\u001b[39;00m\n\u001b[32m    248\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03m  Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGenerateResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/generate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGenerateRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m      \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m      \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m      \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m      \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m      \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hidden-layer/venv/lib/python3.13/site-packages/ollama/_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hidden-layer/venv/lib/python3.13/site-packages/ollama/_client.py:133\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    135\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: model 'llama3.2:latest' not found (status code: 404)"
     ]
    }
   ],
   "source": [
    "# Run debate experiment with tracking\n",
    "config = ExperimentConfig(\n",
    "    experiment_name=f\"debate_{NUM_DEBATERS}agents\",\n",
    "    task_type=\"reasoning\",\n",
    "    strategy=\"debate\",\n",
    "    provider=PROVIDER,\n",
    "    model=DEBATER_MODEL,\n",
    "    n_agents=NUM_DEBATERS,\n",
    "    notes=f\"{NUM_DEBATERS} debaters with judge\"\n",
    ")\n",
    "\n",
    "tracker = get_tracker()\n",
    "tracker.start_experiment(config)\n",
    "\n",
    "for i, task in enumerate(reasoning_tasks):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running debate on: {task['id']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Run debate strategy\n",
    "    result = run_strategy(\n",
    "        \"debate\",\n",
    "        task['input'],\n",
    "        n_debaters=NUM_DEBATERS,\n",
    "        provider=PROVIDER,\n",
    "        model=DEBATER_MODEL\n",
    "    )\n",
    "    \n",
    "    # Display arguments\n",
    "    for j, arg in enumerate(result.metadata['arguments']):\n",
    "        print(f\"\\nDebater {j+1}: {arg[:100]}...\")\n",
    "    \n",
    "    print(f\"\\nJudge verdict: {result.output[:100]}...\")\n",
    "    print(f\"Total latency: {result.latency_s:.2f}s\")\n",
    "    \n",
    "    # Log result\n",
    "    exp_result = ExperimentResult(\n",
    "        config=config,\n",
    "        task_input=task['input'],\n",
    "        output=result.output,\n",
    "        latency_s=result.latency_s,\n",
    "        tokens_in=result.tokens_in,\n",
    "        tokens_out=result.tokens_out,\n",
    "        cost_usd=result.cost_usd,\n",
    "        eval_metadata={\n",
    "            'task_id': task['id'],\n",
    "            'category': task['category'],\n",
    "            'expected': task['expected'],\n",
    "            'arguments': result.metadata['arguments']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Evaluate the result\n",
    "    exp_result.eval_scores = evaluate_task(task, result.output)\n",
    "    \n",
    "    tracker.log_result(exp_result)\n",
    "    print(\"âœ“ Logged\")\n",
    "\n",
    "summary = tracker.finish_experiment()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Debate experiment complete!\")\n",
    "print(f\"Saved to: {tracker.current_run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare: Single vs Debate\n",
    "\n",
    "Load baseline and debate results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare experiments\n",
    "# Note: You'll need to run the baseline experiment first (01_baseline_experiments.ipynb)\n",
    "# Then update the paths below with your actual experiment directories\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# List available experiments\n",
    "exp_dir = Path(\"../experiments\")\n",
    "if exp_dir.exists():\n",
    "    experiments = sorted([d.name for d in exp_dir.iterdir() if d.is_dir()])\n",
    "    print(\"Available experiments:\")\n",
    "    for exp in experiments:\n",
    "        print(f\"  - {exp}\")\n",
    "else:\n",
    "    print(\"No experiments found yet. Run baseline experiments first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison metrics\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': 'Single Model',\n",
    "        'Avg Latency (ms)': baseline_data['summary']['stats']['avg_latency_ms'],\n",
    "        'Total Cost ($)': baseline_data['summary']['stats']['total_cost'],\n",
    "        'Runs': baseline_data['summary']['num_runs']\n",
    "    },\n",
    "    {\n",
    "        'Strategy': f'Debate ({NUM_DEBATERS} agents)',\n",
    "        'Avg Latency (ms)': debate_data['summary']['stats']['avg_latency_ms'],\n",
    "        'Total Cost ($)': debate_data['summary']['stats']['total_cost'],\n",
    "        'Runs': debate_data['summary']['num_runs']\n",
    "    }\n",
    "])\n",
    "\n",
    "# Calculate overhead\n",
    "latency_overhead = (debate_data['summary']['stats']['avg_latency_ms'] / \n",
    "                   baseline_data['summary']['stats']['avg_latency_ms'])\n",
    "cost_overhead = (debate_data['summary']['stats']['total_cost'] / \n",
    "                baseline_data['summary']['stats']['total_cost'])\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\nDebate Overhead:\")\n",
    "print(f\"  Latency: {latency_overhead:.1f}x slower\")\n",
    "print(f\"  Cost: {cost_overhead:.1f}x more expensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency comparison\n",
    "ax = axes[0]\n",
    "comparison_df.plot.bar(x='Strategy', y='Avg Latency (ms)', ax=ax, legend=False, color=['steelblue', 'coral'])\n",
    "ax.set_title('Latency: Single vs Debate', fontsize=14, weight='bold')\n",
    "ax.set_ylabel('Milliseconds')\n",
    "ax.set_xlabel('')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Cost comparison\n",
    "ax = axes[1]\n",
    "comparison_df.plot.bar(x='Strategy', y='Total Cost ($)', ax=ax, legend=False, color=['steelblue', 'coral'])\n",
    "ax.set_title('Cost: Single vs Debate', fontsize=14, weight='bold')\n",
    "ax.set_ylabel('USD')\n",
    "ax.set_xlabel('')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Analysis\n",
    "\n",
    "Compare actual outputs to assess quality differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(task_id: str):\n",
    "    \"\"\"Show baseline vs debate outputs side by side.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Task: {task_id}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Get task\n",
    "    task = next(t for t in reasoning_tasks if t['id'] == task_id)\n",
    "    print(f\"Input: {task['input']}\\n\")\n",
    "    print(f\"Expected: {task['expected']}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get baseline\n",
    "    baseline_run = next(r for r in baseline_data['runs'] if r['metadata']['task_id'] == task_id)\n",
    "    print(f\"\\nBASELINE (Single Model):\")\n",
    "    print(f\"Output: {baseline_run['output']}\")\n",
    "    print(f\"Latency: {baseline_run['latency_ms']:.0f}ms\")\n",
    "    \n",
    "    # Get debate\n",
    "    debate_run = next(r for r in debate_data['runs'] if r['metadata']['task_id'] == task_id)\n",
    "    print(f\"\\nDEBATE ({NUM_DEBATERS} agents):\")\n",
    "    for i, arg in enumerate(debate_run['metadata']['arguments']):\n",
    "        print(f\"  Debater {i+1}: {arg[:80]}...\")\n",
    "    print(f\"\\nJudge Verdict: {debate_run['output']}\")\n",
    "    print(f\"Latency: {debate_run['latency_ms']:.0f}ms\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Compare first task\n",
    "compare_outputs(reasoning_tasks[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse all tasks\n",
    "for task in reasoning_tasks:\n",
    "    compare_outputs(task['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Quantitative\n",
    "- [Fill in after running]\n",
    "- Debate adds Xx latency overhead\n",
    "- Cost increased by X%\n",
    "\n",
    "### Qualitative\n",
    "- [Observations on quality differences]\n",
    "- When did debate help?\n",
    "- When did debate hurt?\n",
    "\n",
    "### Hypotheses\n",
    "- [ ] Debate improves accuracy on multi-perspective tasks\n",
    "- [ ] Latency overhead is 2-3x\n",
    "- [ ] Judge quality matters more than debaters\n",
    "\n",
    "## Next Experiments\n",
    "1. Test with larger judge model (13B or 30B)\n",
    "2. Add 3rd debater\n",
    "3. Try specialized debater roles\n",
    "4. Test on creative/open-ended tasks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
