{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Experiments: Debate Strategy\n",
    "\n",
    "**Experiment:** Compare debate-based multi-agent vs single-model baselines  \n",
    "**Date:** [Fill in]  \n",
    "**Author:** Leif Haven Martinson  \n",
    "\n",
    "## Goals\n",
    "- Implement 2-agent debate with judge\n",
    "- Compare debate vs single-model baseline\n",
    "- Measure quality, latency, and cost tradeoffs\n",
    "- Identify when debate helps vs hurts\n",
    "\n",
    "## Hypotheses\n",
    "- Debate improves accuracy on tasks with multiple valid perspectives\n",
    "- Debate adds 2-3x latency but may justify cost with quality gains\n",
    "- Judge quality matters more than debater quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from harness import (\n",
    "    llm_call,\n",
    "    run_strategy,\n",
    "    debate_strategy,\n",
    "    ExperimentConfig,\n",
    "    ExperimentResult,\n",
    "    get_tracker,\n",
    "    evaluate_task\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debate Strategy Implementation\n",
    "\n",
    "Two agents debate, then a judge decides the best answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick test to verify debate is working\n# This uses default settings - see the configuration cell below to customize\n\ntest_result = run_strategy(\n    \"debate\",\n    \"What is 2+2?\",\n    n_debaters=2,\n    provider=\"ollama\",\n    verbose=False  # Set to True to see the full debate\n)\n\nprint(f\"Debate result: {test_result.output[:100]}...\")\nprint(f\"Latency: {test_result.latency_s:.2f}s\")\nprint(f\"\\nNumber of debaters: {test_result.metadata['n_debaters']}\")\nprint(f\"Number of rounds: {test_result.metadata['n_rounds']}\")\nprint(\"\\nâœ… Debate system ready! Scroll down to customize debate parameters.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tasks from Baseline\n",
    "\n",
    "Use the same tasks for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 tasks\n"
     ]
    }
   ],
   "source": [
    "# Same tasks as baseline experiment\n",
    "reasoning_tasks = [\n",
    "    {\n",
    "        \"id\": \"logic_01\",\n",
    "        \"category\": \"logical_reasoning\",\n",
    "        \"input\": \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n",
    "        \"expected\": \"No, this doesn't follow logically.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"math_01\",\n",
    "        \"category\": \"arithmetic\",\n",
    "        \"input\": \"A train travels 120 km in 2 hours, then 180 km in 3 hours. What is its average speed for the entire journey?\",\n",
    "        \"expected\": \"60 km/h\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"reasoning_01\",\n",
    "        \"category\": \"causal_reasoning\",\n",
    "        \"input\": \"Studies show that people who drink coffee tend to live longer. Does this mean coffee causes longevity?\",\n",
    "        \"expected\": \"No, correlation doesn't imply causation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"planning_01\",\n",
    "        \"category\": \"planning\",\n",
    "        \"input\": \"You need to be at a meeting 30 km away at 2 PM. Traffic is heavy (20 km/h). It's now 1:15 PM. Can you make it on time?\",\n",
    "        \"expected\": \"No. Travel time = 1.5 hours.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"pattern_01\",\n",
    "        \"category\": \"pattern_recognition\",\n",
    "        \"input\": \"What comes next in this sequence: 2, 6, 12, 20, 30, ?\",\n",
    "        \"expected\": \"42\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(reasoning_tasks)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configure Debate Parameters\n\n**CUSTOMIZE HERE:** Adjust these settings to control the debate behavior"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# DEBATE CONFIGURATION - EDIT THESE VALUES\n# ========================================\n\nfrom harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n\n# Number of debating agents\nNUM_DEBATERS = 2\n\n# Number of debate rounds (agents see each other's arguments and refine)\n# 1 = initial arguments only, 2+ = rebuttals and refinements\nNUM_ROUNDS = 2  # ðŸ”§ CHANGE THIS to adjust debate depth\n\n# Model configuration\nPROVIDER = DEFAULT_PROVIDER\nMODEL = DEFAULT_MODEL\n\n# ========================================\n# DEBATER PERSPECTIVES - EDIT THESE PROMPTS\n# ========================================\n# Give each debater a different perspective or role\n# Set to None to use default prompts\n\nDEBATER_PROMPTS = [\n    # Debater 1: Skeptical/Critical perspective\n    \"\"\"You are a critical thinker who questions assumptions and looks for flaws in reasoning.\nApproach the question with healthy skepticism and consider what could go wrong.\"\"\",\n\n    # Debater 2: Optimistic/Constructive perspective\n    \"\"\"You are a constructive thinker who builds on ideas and explores positive possibilities.\nLook for opportunities and creative solutions to the question.\"\"\",\n\n    # Add more debaters if NUM_DEBATERS > 2:\n    # \"\"\"You are a practical thinker focused on real-world implementation...\"\"\",\n]\n\n# ========================================\n# JUDGE PROMPT - EDIT THIS\n# ========================================\n# Custom prompt for the judge (use {task_input} as placeholder for the question)\n# Set to None to use default judge prompt\n\nJUDGE_PROMPT = \"\"\"You are an expert judge evaluating different perspectives on a question.\nConsider the strength of reasoning, evidence, and practical implications of each answer.\n\nQuestion: {task_input}\n\nEvaluate the answers below and provide your final verdict on the best answer, with clear reasoning.\"\"\"\n\n# Set to None to use default:\n# JUDGE_PROMPT = None\n\nprint(f\"âœ… Debate configuration:\")\nprint(f\"   - {NUM_DEBATERS} debaters\")\nprint(f\"   - {NUM_ROUNDS} round(s) of debate\")\nprint(f\"   - Provider: {PROVIDER}\")\nprint(f\"   - Model: {MODEL}\")\nprint(f\"   - Custom debater prompts: {len(DEBATER_PROMPTS) if DEBATER_PROMPTS else 0}\")\nprint(f\"   - Custom judge prompt: {'Yes' if JUDGE_PROMPT else 'No'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Debate Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run debate experiment with tracking\nconfig = ExperimentConfig(\n    experiment_name=f\"debate_{NUM_DEBATERS}agents_{NUM_ROUNDS}rounds\",\n    task_type=\"reasoning\",\n    strategy=\"debate\",\n    provider=PROVIDER,\n    model=MODEL,\n    n_agents=NUM_DEBATERS,\n    notes=f\"{NUM_DEBATERS} debaters, {NUM_ROUNDS} rounds with custom prompts\"\n)\n\ntracker = get_tracker()\ntracker.start_experiment(config)\n\nfor i, task in enumerate(reasoning_tasks):\n    print(f\"\\n{'='*60}\")\n    print(f\"Running debate on: {task['id']}\")\n    print(f\"{'='*60}\")\n    \n    # Run debate strategy with custom configuration\n    result = run_strategy(\n        \"debate\",\n        task['input'],\n        n_debaters=NUM_DEBATERS,\n        n_rounds=NUM_ROUNDS,  # ðŸ”§ Using configured rounds\n        provider=PROVIDER,\n        model=MODEL,\n        debater_prompts=DEBATER_PROMPTS,  # ðŸ”§ Using custom debater perspectives\n        judge_prompt=JUDGE_PROMPT,  # ðŸ”§ Using custom judge prompt\n        verbose=True  # Set to False to hide streaming output\n    )\n    \n    # Display debate results\n    print(f\"\\nðŸ“Š Debate completed:\")\n    print(f\"   - Rounds: {NUM_ROUNDS}\")\n    print(f\"   - Total latency: {result.latency_s:.2f}s\")\n    \n    if NUM_ROUNDS > 1:\n        print(f\"\\n   Final arguments after {NUM_ROUNDS} rounds:\")\n    else:\n        print(f\"\\n   Arguments:\")\n    \n    for j, arg in enumerate(result.metadata['arguments']):\n        print(f\"\\n   Debater {j+1}: {arg[:100]}...\")\n    \n    print(f\"\\n   Judge verdict: {result.output[:100]}...\")\n    \n    # Log result\n    exp_result = ExperimentResult(\n        config=config,\n        task_input=task['input'],\n        output=result.output,\n        latency_s=result.latency_s,\n        tokens_in=result.tokens_in,\n        tokens_out=result.tokens_out,\n        cost_usd=result.cost_usd,\n        eval_metadata={\n            'task_id': task['id'],\n            'category': task['category'],\n            'expected': task['expected'],\n            'arguments': result.metadata['arguments'],\n            'n_rounds': NUM_ROUNDS\n        }\n    )\n    \n    # Evaluate the result\n    exp_result.eval_scores = evaluate_task(task, result.output)\n    \n    tracker.log_result(exp_result)\n    print(\"âœ“ Logged\")\n\nsummary = tracker.finish_experiment()\nprint(\"\\n\" + \"=\"*60)\nprint(\"Debate experiment complete!\")\nprint(f\"Saved to: {tracker.current_run_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare: Single vs Debate\n",
    "\n",
    "Load baseline and debate results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare experiments\n",
    "# Note: You'll need to run the baseline experiment first (01_baseline_experiments.ipynb)\n",
    "# Then update the paths below with your actual experiment directories\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# List available experiments\n",
    "exp_dir = Path(\"../experiments\")\n",
    "if exp_dir.exists():\n",
    "    experiments = sorted([d.name for d in exp_dir.iterdir() if d.is_dir()])\n",
    "    print(\"Available experiments:\")\n",
    "    for exp in experiments:\n",
    "        print(f\"  - {exp}\")\n",
    "else:\n",
    "    print(\"No experiments found yet. Run baseline experiments first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison metrics\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': 'Single Model',\n",
    "        'Avg Latency (ms)': baseline_data['summary']['stats']['avg_latency_ms'],\n",
    "        'Total Cost ($)': baseline_data['summary']['stats']['total_cost'],\n",
    "        'Runs': baseline_data['summary']['num_runs']\n",
    "    },\n",
    "    {\n",
    "        'Strategy': f'Debate ({NUM_DEBATERS} agents)',\n",
    "        'Avg Latency (ms)': debate_data['summary']['stats']['avg_latency_ms'],\n",
    "        'Total Cost ($)': debate_data['summary']['stats']['total_cost'],\n",
    "        'Runs': debate_data['summary']['num_runs']\n",
    "    }\n",
    "])\n",
    "\n",
    "# Calculate overhead\n",
    "latency_overhead = (debate_data['summary']['stats']['avg_latency_ms'] / \n",
    "                   baseline_data['summary']['stats']['avg_latency_ms'])\n",
    "cost_overhead = (debate_data['summary']['stats']['total_cost'] / \n",
    "                baseline_data['summary']['stats']['total_cost'])\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\nDebate Overhead:\")\n",
    "print(f\"  Latency: {latency_overhead:.1f}x slower\")\n",
    "print(f\"  Cost: {cost_overhead:.1f}x more expensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency comparison\n",
    "ax = axes[0]\n",
    "comparison_df.plot.bar(x='Strategy', y='Avg Latency (ms)', ax=ax, legend=False, color=['steelblue', 'coral'])\n",
    "ax.set_title('Latency: Single vs Debate', fontsize=14, weight='bold')\n",
    "ax.set_ylabel('Milliseconds')\n",
    "ax.set_xlabel('')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Cost comparison\n",
    "ax = axes[1]\n",
    "comparison_df.plot.bar(x='Strategy', y='Total Cost ($)', ax=ax, legend=False, color=['steelblue', 'coral'])\n",
    "ax.set_title('Cost: Single vs Debate', fontsize=14, weight='bold')\n",
    "ax.set_ylabel('USD')\n",
    "ax.set_xlabel('')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Analysis\n",
    "\n",
    "Compare actual outputs to assess quality differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(task_id: str):\n",
    "    \"\"\"Show baseline vs debate outputs side by side.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Task: {task_id}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Get task\n",
    "    task = next(t for t in reasoning_tasks if t['id'] == task_id)\n",
    "    print(f\"Input: {task['input']}\\n\")\n",
    "    print(f\"Expected: {task['expected']}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get baseline\n",
    "    baseline_run = next(r for r in baseline_data['runs'] if r['metadata']['task_id'] == task_id)\n",
    "    print(f\"\\nBASELINE (Single Model):\")\n",
    "    print(f\"Output: {baseline_run['output']}\")\n",
    "    print(f\"Latency: {baseline_run['latency_ms']:.0f}ms\")\n",
    "    \n",
    "    # Get debate\n",
    "    debate_run = next(r for r in debate_data['runs'] if r['metadata']['task_id'] == task_id)\n",
    "    print(f\"\\nDEBATE ({NUM_DEBATERS} agents):\")\n",
    "    for i, arg in enumerate(debate_run['metadata']['arguments']):\n",
    "        print(f\"  Debater {i+1}: {arg[:80]}...\")\n",
    "    print(f\"\\nJudge Verdict: {debate_run['output']}\")\n",
    "    print(f\"Latency: {debate_run['latency_ms']:.0f}ms\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Compare first task\n",
    "compare_outputs(reasoning_tasks[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse all tasks\n",
    "for task in reasoning_tasks:\n",
    "    compare_outputs(task['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Quantitative\n",
    "- [Fill in after running]\n",
    "- Debate adds Xx latency overhead\n",
    "- Cost increased by X%\n",
    "\n",
    "### Qualitative\n",
    "- [Observations on quality differences]\n",
    "- When did debate help?\n",
    "- When did debate hurt?\n",
    "\n",
    "### Hypotheses\n",
    "- [ ] Debate improves accuracy on multi-perspective tasks\n",
    "- [ ] Latency overhead is 2-3x\n",
    "- [ ] Judge quality matters more than debaters\n",
    "\n",
    "## Next Experiments\n",
    "1. Test with larger judge model (13B or 30B)\n",
    "2. Add 3rd debater\n",
    "3. Try specialized debater roles\n",
    "4. Test on creative/open-ended tasks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}