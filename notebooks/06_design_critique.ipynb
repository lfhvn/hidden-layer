{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Design Critique Strategy\n",
    "\n",
    "**Mental Model:** Structured feedback process to refine and improve outputs\n",
    "\n",
    "**Date:** [Fill in]  \n",
    "**Author:** Leif Haven Martinson  \n",
    "\n",
    "## Concept\n",
    "\n",
    "In design and creative fields, critique is essential:\n",
    "- Initial draft ‚Üí Peer feedback ‚Üí Revision\n",
    "- Multiple perspectives identify blind spots\n",
    "- Structured critique improves output quality\n",
    "\n",
    "This notebook simulates a design crit session where:\n",
    "1. Generate initial solution\n",
    "2. Multiple agents provide structured feedback\n",
    "3. Refine based on critique\n",
    "4. (Optionally) Iterate\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Initial Draft** - Generate first version\n",
    "2. **Critique Round** - Agents evaluate on different dimensions\n",
    "3. **Synthesis** - Integrate feedback\n",
    "4. **Revision** - Improve the draft\n",
    "5. **Iterate** (optional) - Repeat critique/revision\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- Creative writing (stories, articles, copy)\n",
    "- Technical writing (documentation, proposals)\n",
    "- Code review and improvement\n",
    "- Design reviews (UI, architecture, etc.)\n",
    "- Research paper revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from harness import (\n",
    "    llm_call,\n",
    "    llm_call_stream,\n",
    "    ExperimentConfig,\n",
    "    ExperimentResult,\n",
    "    get_tracker\n",
    ")\n",
    "from harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**CUSTOMIZE HERE:** Define your critique panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CRITIQUE CONFIGURATION - EDIT THESE\n",
    "# ========================================\n",
    "\n",
    "# Model configuration\n",
    "PROVIDER = DEFAULT_PROVIDER\n",
    "MODEL = DEFAULT_MODEL\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "# Number of critique/revision iterations\n",
    "NUM_ITERATIONS = 2  # üîß 1 = single critique, 2+ = iterative refinement\n",
    "\n",
    "# ========================================\n",
    "# DEFINE YOUR CRITIQUE PANEL\n",
    "# ========================================\n",
    "# Each critic evaluates on specific dimensions\n",
    "\n",
    "CRITIQUE_PANEL = [\n",
    "    {\n",
    "        \"name\": \"Clarity Critic\",\n",
    "        \"focus\": \"Clarity and comprehension\",\n",
    "        \"criteria\": \"\"\"Evaluate clarity and comprehension:\n",
    "- Is the writing clear and easy to understand?\n",
    "- Are concepts explained well?\n",
    "- Is the structure logical?\n",
    "- Are there any confusing or ambiguous parts?\n",
    "\n",
    "Provide specific suggestions for improving clarity.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Technical Accuracy Critic\",\n",
    "        \"focus\": \"Technical correctness and accuracy\",\n",
    "        \"criteria\": \"\"\"Evaluate technical accuracy:\n",
    "- Are facts and claims accurate?\n",
    "- Are technical details correct?\n",
    "- Are there any errors or misconceptions?\n",
    "- Are sources and evidence appropriate?\n",
    "\n",
    "Point out any inaccuracies and suggest corrections.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Engagement Critic\",\n",
    "        \"focus\": \"Reader engagement and interest\",\n",
    "        \"criteria\": \"\"\"Evaluate engagement:\n",
    "- Is it interesting and engaging to read?\n",
    "- Does it hook the reader?\n",
    "- Is the tone appropriate?\n",
    "- Are examples compelling?\n",
    "\n",
    "Suggest ways to make it more engaging.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Completeness Critic\",\n",
    "        \"focus\": \"Completeness and coverage\",\n",
    "        \"criteria\": \"\"\"Evaluate completeness:\n",
    "- Are all important points covered?\n",
    "- Are there gaps or missing information?\n",
    "- Is enough context provided?\n",
    "- Are counterarguments addressed?\n",
    "\n",
    "Identify what's missing and should be added.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Style Critic\",\n",
    "        \"focus\": \"Writing style and quality\",\n",
    "        \"criteria\": \"\"\"Evaluate writing style:\n",
    "- Is the writing polished and professional?\n",
    "- Is word choice appropriate?\n",
    "- Are sentences well-constructed?\n",
    "- Is there good variety in sentence structure?\n",
    "\n",
    "Suggest stylistic improvements.\"\"\"\n",
    "    },\n",
    "    \n",
    "    # Add custom critics as needed:\n",
    "    # {\n",
    "    #     \"name\": \"Audience Critic\",\n",
    "    #     \"focus\": \"Audience appropriateness\",\n",
    "    #     \"criteria\": \"Consider the target audience...\"\n",
    "    # },\n",
    "]\n",
    "\n",
    "# ========================================\n",
    "# REVISION PROMPT\n",
    "# ========================================\n",
    "# How revisions incorporate feedback\n",
    "\n",
    "REVISION_PROMPT = \"\"\"You are revising a draft based on structured critique.\n",
    "\n",
    "Your job:\n",
    "1. Carefully review all critiques\n",
    "2. Identify the most important improvements\n",
    "3. Revise the draft to address feedback\n",
    "4. Balance different critique perspectives\n",
    "5. Maintain the core message while improving quality\n",
    "\n",
    "Provide an improved version that addresses the feedback.\"\"\"\n",
    "\n",
    "print(f\"‚úÖ Critique panel configured:\")\n",
    "print(f\"   - {len(CRITIQUE_PANEL)} critics\")\n",
    "print(f\"   - Iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"\\nüé® Your critique panel:\")\n",
    "for critic in CRITIQUE_PANEL:\n",
    "    print(f\"   - {critic['name']}: {critic['focus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Define Task\n",
    "\n",
    "**CUSTOMIZE HERE:** What should be created and critiqued?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TASK DEFINITION - EDIT THIS\n",
    "# ========================================\n",
    "\n",
    "TASK_PROMPT = \"\"\"Write a 300-word blog post explaining what multi-agent AI systems are and why they matter.\n",
    "\n",
    "Target audience: Technical professionals who are curious about AI but not AI experts\n",
    "\n",
    "Goals:\n",
    "- Explain the concept clearly\n",
    "- Provide concrete examples\n",
    "- Make it engaging and accessible\n",
    "- End with why this matters for the future\n",
    "\"\"\"\n",
    "\n",
    "# Alternative tasks to try:\n",
    "#\n",
    "# \"Write API documentation for a new authentication endpoint.\"\n",
    "#\n",
    "# \"Draft a product announcement email for our new feature launch.\"\n",
    "#\n",
    "# \"Create a technical proposal for migrating our database to PostgreSQL.\"\n",
    "#\n",
    "# \"Write a compelling product description for an AI-powered writing assistant.\"\n",
    "\n",
    "print(\"üìù Task defined:\")\n",
    "print(TASK_PROMPT[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Generate Initial Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Track timing and tokens\n",
    "start_time = time.time()\n",
    "total_tokens_in = 0\n",
    "total_tokens_out = 0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìÑ GENERATING INITIAL DRAFT\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Generate initial draft (streaming for visibility)\n",
    "full_text = \"\"\n",
    "response = None\n",
    "\n",
    "for chunk in llm_call_stream(TASK_PROMPT, provider=PROVIDER, model=MODEL, temperature=TEMPERATURE):\n",
    "    if isinstance(chunk, str):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        full_text += chunk\n",
    "    else:\n",
    "        response = chunk\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "current_draft = response.text\n",
    "total_tokens_in += response.tokens_in or 0\n",
    "total_tokens_out += response.tokens_out or 0\n",
    "\n",
    "print(f\"‚úÖ Initial draft generated ({len(current_draft)} characters)\")\n",
    "\n",
    "# Store versions for comparison\n",
    "versions = [{\n",
    "    \"version\": 0,\n",
    "    \"label\": \"Initial Draft\",\n",
    "    \"content\": current_draft\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Critique & Revision Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üé® ITERATION {iteration + 1}: Critique & Revision\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # === CRITIQUE PHASE ===\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"üí¨ CRITIQUE PHASE\")\n",
    "    print(f\"{'‚îÄ'*80}\\n\")\n",
    "    \n",
    "    critiques = []\n",
    "    \n",
    "    for critic in CRITIQUE_PANEL:\n",
    "        print(f\"\\n{critic['name']}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Create critique prompt\n",
    "        critique_prompt = f\"\"\"You are providing critique as: {critic['name']}\n",
    "\n",
    "{critic['criteria']}\n",
    "\n",
    "Draft to critique:\n",
    "\\\"\\\"\\\"\n",
    "{current_draft}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Provide your critique focusing on {critic['focus']}:\"\"\"\n",
    "        \n",
    "        # Get critique (streaming)\n",
    "        full_text = \"\"\n",
    "        response = None\n",
    "        \n",
    "        for chunk in llm_call_stream(critique_prompt, provider=PROVIDER, model=MODEL, temperature=0.7):\n",
    "            if isinstance(chunk, str):\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "                full_text += chunk\n",
    "            else:\n",
    "                response = chunk\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        critiques.append({\n",
    "            \"critic\": critic['name'],\n",
    "            \"focus\": critic['focus'],\n",
    "            \"feedback\": response.text\n",
    "        })\n",
    "        \n",
    "        total_tokens_in += response.tokens_in or 0\n",
    "        total_tokens_out += response.tokens_out or 0\n",
    "    \n",
    "    # === REVISION PHASE ===\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"‚úèÔ∏è  REVISION PHASE\")\n",
    "    print(f\"{'‚îÄ'*80}\\n\")\n",
    "    \n",
    "    # Build revision prompt\n",
    "    revision_prompt = f\"{REVISION_PROMPT}\\n\\n\"\n",
    "    revision_prompt += f\"Current Draft:\\n\\\"\\\"\\\"{current_draft}\\\"\\\"\\\"\\n\\n\"\n",
    "    revision_prompt += \"Critiques:\\n\\n\"\n",
    "    \n",
    "    for critique in critiques:\n",
    "        revision_prompt += f\"{critique['critic']} ({critique['focus']}):\\n\"\n",
    "        revision_prompt += f\"{critique['feedback']}\\n\\n\"\n",
    "        revision_prompt += \"-\" * 60 + \"\\n\\n\"\n",
    "    \n",
    "    revision_prompt += \"Revised draft (addressing the critiques):\"\n",
    "    \n",
    "    # Get revision (streaming)\n",
    "    full_text = \"\"\n",
    "    response = None\n",
    "    \n",
    "    for chunk in llm_call_stream(revision_prompt, provider=PROVIDER, model=MODEL, temperature=0.7):\n",
    "        if isinstance(chunk, str):\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "            full_text += chunk\n",
    "        else:\n",
    "            response = chunk\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    current_draft = response.text\n",
    "    total_tokens_in += response.tokens_in or 0\n",
    "    total_tokens_out += response.tokens_out or 0\n",
    "    \n",
    "    # Store version\n",
    "    versions.append({\n",
    "        \"version\": iteration + 1,\n",
    "        \"label\": f\"After Iteration {iteration + 1}\",\n",
    "        \"content\": current_draft,\n",
    "        \"critiques\": critiques\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Iteration {iteration + 1} complete\")\n",
    "\n",
    "# Calculate total time\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ CRITIQUE PROCESS COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nüìä Stats:\")\n",
    "print(f\"   Critics: {len(CRITIQUE_PANEL)}\")\n",
    "print(f\"   Iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"   Versions: {len(versions)}\")\n",
    "print(f\"   Total time: {total_time:.1f}s\")\n",
    "print(f\"   Total tokens: {total_tokens_in + total_tokens_out:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Compare Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìä VERSION COMPARISON\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for version in versions:\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Version {version['version']}: {version['label']}\")\n",
    "    print(f\"{'‚îÄ'*80}\\n\")\n",
    "    print(version['content'])\n",
    "    print(f\"\\n(Length: {len(version['content'])} characters)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìÑ FINAL OUTPUT\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(current_draft)\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Quality Analysis\n",
    "\n",
    "Measure improvement across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LLM to evaluate quality progression\n",
    "from harness import llm_call\n",
    "\n",
    "print(f\"\\nüìä Evaluating quality progression...\\n\")\n",
    "\n",
    "comparison_prompt = f\"\"\"Compare these versions of the same content and rate how much the quality improved.\n",
    "\n",
    "Original (Version 0):\n",
    "\\\"\\\"\\\"\n",
    "{versions[0]['content']}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Final (Version {len(versions)-1}):\n",
    "\\\"\\\"\\\"\n",
    "{versions[-1]['content']}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Rate the improvement on a scale of 1-10 for each dimension:\n",
    "- Clarity\n",
    "- Technical Accuracy\n",
    "- Engagement\n",
    "- Completeness\n",
    "- Writing Style\n",
    "\n",
    "Also provide an overall quality improvement score (1-10) and brief analysis.\n",
    "\n",
    "Format:\n",
    "Clarity: X/10\n",
    "Technical Accuracy: X/10\n",
    "Engagement: X/10\n",
    "Completeness: X/10\n",
    "Writing Style: X/10\n",
    "Overall Improvement: X/10\n",
    "\n",
    "Analysis: [Brief explanation of key improvements]\n",
    "\"\"\"\n",
    "\n",
    "evaluation = llm_call(comparison_prompt, provider=PROVIDER, model=MODEL, temperature=0.3)\n",
    "\n",
    "print(evaluation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"../experiments/design_critique\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create result object\n",
    "result = {\n",
    "    \"task\": TASK_PROMPT,\n",
    "    \"critics\": [{\"name\": c[\"name\"], \"focus\": c[\"focus\"]} for c in CRITIQUE_PANEL],\n",
    "    \"versions\": versions,\n",
    "    \"final_output\": current_draft,\n",
    "    \"num_iterations\": NUM_ITERATIONS,\n",
    "    \"stats\": {\n",
    "        \"total_time_s\": total_time,\n",
    "        \"total_tokens\": total_tokens_in + total_tokens_out,\n",
    "        \"model\": MODEL,\n",
    "        \"provider\": PROVIDER\n",
    "    },\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "filename = results_dir / f\"result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Try Different Configurations\n",
    "\n",
    "1. **Different tasks** - Try various writing types (technical, creative, business)\n",
    "2. **More iterations** - See if quality keeps improving (diminishing returns?)\n",
    "3. **Different critics** - Add domain-specific critics for your use case\n",
    "4. **Fewer critics** - Test if 2-3 focused critics work as well as 5\n",
    "5. **Compare to single pass** - Does critique actually help?\n",
    "\n",
    "### Custom Critique Panels\n",
    "\n",
    "**For code review:**\n",
    "- Functionality critic\n",
    "- Performance critic\n",
    "- Security critic\n",
    "- Maintainability critic\n",
    "\n",
    "**For academic writing:**\n",
    "- Rigor critic (methodology)\n",
    "- Clarity critic (communication)\n",
    "- Novelty critic (contribution)\n",
    "- References critic (citations)\n",
    "\n",
    "**For marketing copy:**\n",
    "- Brand voice critic\n",
    "- Conversion critic\n",
    "- SEO critic\n",
    "- Emotional appeal critic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
