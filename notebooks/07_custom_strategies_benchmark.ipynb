{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Custom Strategies Benchmark\n\n**Focus:** Compare adaptive multi-agent strategies against single-model baseline\n\n**Date:** [Fill in]  \n**Author:** Leif Haven Martinson  \n\n## Purpose\n\nThis notebook focuses on benchmarking custom mental model strategies:\n- **Single Model** - Baseline performance\n- **Design Critique** - 5 designer archetypes provide iterative feedback\n- **Interdisciplinary Team** - PM, Engineer, Designer collaborate  \n- **Adaptive Team** - ‚≠ê **NEW!** Dynamically generates custom experts tailored to each specific problem\n\nCompare these approaches on established benchmarks with **latest 2025 SOTA baselines**.\n\n## Adaptive Team Strategy\n\nThe **adaptive_team** strategy is a meta-strategy that:\n1. Analyzes each problem to understand what expertise is needed\n2. Dynamically generates custom expert personas specifically for that problem\n3. Runs collaborative analysis with the problem-specific experts\n\n**Example:** \n- Math problem ‚Üí Mathematician, Math Teacher, Applied Scientist\n- Business question ‚Üí Market Analyst, CFO, Operations Manager\n- Medical question ‚Üí Doctor, Pharmacist, Research Scientist\n\nThis allows the expert team to be **perfectly tailored** to each individual problem!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from harness import (\n",
    "    load_benchmark,\n",
    "    get_baseline_scores,\n",
    "    BENCHMARKS,\n",
    "    run_strategy,\n",
    "    ExperimentConfig,\n",
    "    ExperimentResult,\n",
    "    get_tracker\n",
    ")\n",
    "from harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(f\"\\nAvailable benchmarks: {list(BENCHMARKS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Agent Persona Configuration\n",
    "\n",
    "**CUSTOMIZE HERE:** Edit the personas for rapid experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DESIGN CRITIQUE PANEL - EDIT THESE\n",
    "# ========================================\n",
    "\n",
    "CRITIQUE_PANEL = [\n",
    "    {\n",
    "        \"name\": \"Systems Designer\",\n",
    "        \"focus\": \"System architecture and holistic design\",\n",
    "        \"criteria\": \"\"\"You are a Systems Designer who thinks about the big picture and interconnections.\n",
    "Evaluate:\n",
    "- Does the solution consider the whole system and its parts?\n",
    "- Are relationships between components clear?\n",
    "- Is there coherence between different elements?\n",
    "- Does the design scale and adapt to different contexts?\n",
    "Focus on holistic thinking, interconnections, and systemic coherence.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Visual Craft Specialist\",\n",
    "        \"focus\": \"Visual clarity, aesthetics, and presentation\",\n",
    "        \"criteria\": \"\"\"You are a Visual Craft Specialist focused on how information is presented and perceived.\n",
    "Evaluate:\n",
    "- Is information presented clearly and visually comprehensible?\n",
    "- Is there good hierarchy and structure in the presentation?\n",
    "- Are concepts illustrated or explained in ways that are easy to visualize?\n",
    "- Does the format enhance understanding?\n",
    "Focus on clarity of presentation, visual thinking, and aesthetic quality.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"AI Specialist\",\n",
    "        \"focus\": \"AI/ML capabilities, limitations, and best practices\",\n",
    "        \"criteria\": \"\"\"You are an AI Specialist with deep knowledge of AI systems, capabilities, and limitations.\n",
    "Evaluate:\n",
    "- Are claims about AI accurate and grounded in current capabilities?\n",
    "- Are limitations and potential issues with AI acknowledged?\n",
    "- Are AI-related recommendations practical and informed?\n",
    "- Is the approach aligned with AI best practices?\n",
    "Focus on technical accuracy regarding AI/ML, practical feasibility, and responsible AI considerations.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Human-Computer Interaction Expert\",\n",
    "        \"focus\": \"User experience, usability, and human factors\",\n",
    "        \"criteria\": \"\"\"You are an HCI Expert focused on how humans interact with systems and information.\n",
    "Evaluate:\n",
    "- Is the solution usable and accessible to the target audience?\n",
    "- Are user needs and cognitive limitations considered?\n",
    "- Is interaction intuitive and aligned with mental models?\n",
    "- Are there potential usability issues or barriers?\n",
    "Focus on user-centered design, accessibility, cognitive ergonomics, and interaction patterns.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"IDEO Design Thinking Facilitator\",\n",
    "        \"focus\": \"Human-centered innovation and creative problem-solving\",\n",
    "        \"criteria\": \"\"\"You are an IDEO-trained Design Thinking practitioner emphasizing empathy, ideation, and iteration.\n",
    "Evaluate:\n",
    "- Does the solution demonstrate empathy for user needs and pain points?\n",
    "- Is there creative thinking and exploration of possibilities?\n",
    "- Are assumptions tested or validated?\n",
    "- Is the approach iterative and open to refinement?\n",
    "Focus on empathy, creative exploration, prototyping mindset, and bias toward action.\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# ========================================\n",
    "# INTERDISCIPLINARY TEAM - EDIT THESE\n",
    "# ========================================\n",
    "\n",
    "EXPERT_TEAM = [\n",
    "    {\n",
    "        \"name\": \"Product Manager\",\n",
    "        \"role\": \"Product Management\",\n",
    "        \"perspective\": \"User needs, business value, roadmap, and strategic priorities\",\n",
    "        \"system_prompt\": \"\"\"You are a Product Manager responsible for defining what to build and why.\n",
    "\n",
    "Your mission: Ensure the solution creates real user value while achieving business objectives.\n",
    "\n",
    "Focus on:\n",
    "- User needs, pain points, and jobs-to-be-done\n",
    "- Business impact, ROI, and strategic alignment\n",
    "- Feature prioritization and tradeoffs\n",
    "- Market fit and competitive positioning\n",
    "- Success metrics and measurable outcomes\n",
    "- Feasibility vs. value vs. risk assessment\n",
    "\n",
    "Analyze problems by asking:\n",
    "- What user problem does this solve?\n",
    "- What is the business value?\n",
    "- How do we measure success?\n",
    "- What are the must-haves vs. nice-to-haves?\n",
    "- What are the risks and mitigations?\n",
    "\n",
    "Bring a balanced perspective that bridges user needs, business goals, and technical reality.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Software Engineer\",\n",
    "        \"role\": \"Engineering\",\n",
    "        \"perspective\": \"Technical implementation, architecture, scalability, and feasibility\",\n",
    "        \"system_prompt\": \"\"\"You are a Software Engineer responsible for building and shipping reliable systems.\n",
    "\n",
    "Your mission: Deliver technically sound solutions that are maintainable, scalable, and feasible within constraints.\n",
    "\n",
    "Focus on:\n",
    "- Technical feasibility and implementation complexity\n",
    "- System architecture and design patterns\n",
    "- Scalability, performance, and reliability\n",
    "- Security, data integrity, and edge cases\n",
    "- Technical debt and long-term maintainability\n",
    "- Development velocity and engineering resources\n",
    "- Integration with existing systems\n",
    "\n",
    "Analyze problems by asking:\n",
    "- Is this technically feasible?\n",
    "- What is the implementation complexity?\n",
    "- What are the technical risks?\n",
    "- How does this scale?\n",
    "- What are the dependencies and blockers?\n",
    "- What technical debt are we taking on?\n",
    "\n",
    "Bring a pragmatic engineering perspective focused on what we can actually build and ship.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Product Designer\",\n",
    "        \"role\": \"Design\",\n",
    "        \"perspective\": \"User experience, interaction design, usability, and design quality\",\n",
    "        \"system_prompt\": \"\"\"You are a Product Designer responsible for crafting intuitive, delightful user experiences.\n",
    "\n",
    "Your mission: Ensure the solution is usable, accessible, and provides a great user experience.\n",
    "\n",
    "Focus on:\n",
    "- User experience and interaction design\n",
    "- Usability, learnability, and accessibility\n",
    "- User flows and mental models\n",
    "- Information architecture and navigation\n",
    "- Visual design and brand consistency\n",
    "- Edge cases and error states\n",
    "- User research insights and validation\n",
    "\n",
    "Analyze problems by asking:\n",
    "- Is this intuitive for users?\n",
    "- What is the user flow?\n",
    "- Are there usability issues or friction points?\n",
    "- Is it accessible to all users?\n",
    "- How do we handle edge cases and errors?\n",
    "- Does this match user mental models?\n",
    "\n",
    "Bring a user-centered design perspective that ensures solutions are not just functional but delightful to use.\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Agent personas configured:\")\n",
    "print(f\"   Design Critique Panel: {len(CRITIQUE_PANEL)} critics\")\n",
    "print(f\"   Interdisciplinary Team: {len(EXPERT_TEAM)} experts\")\n",
    "print(f\"\\nüí° Edit these personas above to experiment with different perspectives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Benchmark Configuration\n",
    "\n",
    "**CUSTOMIZE HERE:** Choose benchmark and strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# BENCHMARK CONFIGURATION\n# ========================================\n\n# Which benchmark to run\nBENCHMARK_NAME = \"gsm8k\"  # üîß CHANGE: \"gsm8k\", \"mmlu\", \"truthfulqa\", \"arc\", \"humaneval\", \"gpqa\"\n\n# How many tasks to evaluate\nNUM_TASKS = 10  # üîß CHANGE: Start with 10-20, increase to 100+ for full eval\n\n# Random seed for reproducibility\nSEED = 42\n\n# Model configuration\nPROVIDER = DEFAULT_PROVIDER\nMODEL = DEFAULT_MODEL\n\n# ========================================\n# STRATEGIES TO COMPARE\n# ========================================\n\nSTRATEGIES_TO_TEST = [\n    # Baseline\n    (\"single\", {\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n    }),\n\n    # Design Critique - 5 designer archetypes (fixed panel)\n    (\"design_critique\", {\n        \"n_iterations\": 2,  # üîß Number of critique/revision cycles\n        \"critique_panel\": CRITIQUE_PANEL,\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False  # üîß Set to True to see streaming\n    }),\n\n    # Interdisciplinary Team - PM, Engineer, Designer (fixed team)\n    (\"interdisciplinary_team\", {\n        \"refinement_rounds\": 1,  # üîß Number of refinement iterations\n        \"expert_team\": EXPERT_TEAM,\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False  # üîß Set to True to see streaming\n    }),\n\n    # ========================================\n    # ‚≠ê ADAPTIVE TEAM - DYNAMICALLY GENERATED EXPERTS\n    # ========================================\n    # This strategy analyzes EACH problem and generates custom experts!\n    # Different problems get different expert teams tailored to that specific question\n    \n    (\"adaptive_team\", {\n        \"n_experts\": 3,  # üîß How many experts to generate per problem\n        \"refinement_rounds\": 1,  # üîß Number of refinement iterations\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": True  # üîß Recommended: True to see which experts are generated!\n    }),\n]\n\nprint(f\"‚úÖ Configuration:\")\nprint(f\"   Benchmark: {BENCHMARK_NAME}\")\nprint(f\"   Tasks: {NUM_TASKS}\")\nprint(f\"   Strategies: {len(STRATEGIES_TO_TEST)}\")\nprint(f\"   Model: {MODEL} ({PROVIDER})\")\nprint(f\"\\nüí° Adaptive team will generate custom experts for each task!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load Benchmark\n",
    "\n",
    "Load benchmark tasks and show current SOTA baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Load benchmark\nbenchmark = load_benchmark(BENCHMARK_NAME)\nbenchmark.load()  # Make sure benchmark is loaded\ntasks = benchmark.get_tasks(n=NUM_TASKS, seed=SEED)\n\nprint(f\"üìä Loaded {len(tasks)} tasks from {BENCHMARK_NAME}\")\nprint(f\"   (Requested: {NUM_TASKS}, Actual: {len(tasks)})\")\n\nif len(tasks) == 0:\n    print(\"\\n‚ö†Ô∏è  WARNING: No tasks loaded! Check benchmark.load() method.\")\nelse:\n    print(f\"\\nSample task:\")\n    print(f\"  Input: {tasks[0].input[:100]}...\")\n    print(f\"  Expected: {tasks[0].expected}\")\n    print(f\"  Category: {tasks[0].category}\")\n\n# Show baseline scores from literature\nbaselines = get_baseline_scores(BENCHMARK_NAME)\nif baselines:\n    print(f\"\\nüìà Published SOTA baselines on {BENCHMARK_NAME} (Jan 2025):\")\n    print(\"‚îÄ\" * 60)\n    for model, score in sorted(baselines.items(), key=lambda x: x[1], reverse=True)[:15]:\n        print(f\"   {model:30s}: {score:.1%}\")\n    print(\"‚îÄ\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Test each strategy on the benchmark tasks\n",
    "\n",
    "**Note:** This may take a while depending on NUM_TASKS and whether verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for each strategy\n",
    "results = []\n",
    "\n",
    "for strategy_name, strategy_kwargs in STRATEGIES_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîÑ Running: {strategy_name}\")\n",
    "    print(f\"   Config: {strategy_kwargs}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create experiment tracker\n",
    "    config = ExperimentConfig(\n",
    "        experiment_name=f\"custom_benchmark_{BENCHMARK_NAME}_{strategy_name}\",\n",
    "        task_type=BENCHMARK_NAME,\n",
    "        strategy=strategy_name,\n",
    "        provider=PROVIDER,\n",
    "        model=MODEL,\n",
    "        notes=f\"Custom strategies benchmark on {BENCHMARK_NAME}\"\n",
    "    )\n",
    "    \n",
    "    tracker = get_tracker()\n",
    "    tracker.start_experiment(config)\n",
    "    \n",
    "    # Run on each task\n",
    "    for task in tqdm(tasks, desc=f\"{strategy_name}\"):\n",
    "        # Run strategy\n",
    "        result = run_strategy(strategy_name, task.input, **strategy_kwargs)\n",
    "        \n",
    "        # Evaluate with benchmark-specific eval\n",
    "        eval_scores = benchmark.evaluate(task, result.output)\n",
    "        \n",
    "        # Log result\n",
    "        exp_result = ExperimentResult(\n",
    "            config=config,\n",
    "            task_input=task.input,\n",
    "            output=result.output,\n",
    "            latency_s=result.latency_s,\n",
    "            tokens_in=result.tokens_in,\n",
    "            tokens_out=result.tokens_out,\n",
    "            cost_usd=result.cost_usd,\n",
    "            eval_scores=eval_scores,\n",
    "            eval_metadata={\n",
    "                'task_id': task.id,\n",
    "                'expected': task.expected,\n",
    "                'benchmark': BENCHMARK_NAME\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        tracker.log_result(exp_result)\n",
    "        results.append({\n",
    "            'strategy': strategy_name,\n",
    "            'task_id': task.id,\n",
    "            'accuracy': eval_scores.get('accuracy', 0),\n",
    "            'latency_s': result.latency_s,\n",
    "            'cost_usd': result.cost_usd or 0\n",
    "        })\n",
    "    \n",
    "    # Finish experiment\n",
    "    summary = tracker.finish_experiment()\n",
    "    \n",
    "    print(f\"\\n‚úÖ {strategy_name} complete:\")\n",
    "    print(f\"   Accuracy: {summary['eval_scores'].get('accuracy', {}).get('mean', 0):.1%}\")\n",
    "    print(f\"   Avg latency: {summary['avg_latency_s']:.2f}s\")\n",
    "    print(f\"   Total cost: ${summary['total_cost_usd']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ All strategies evaluated!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Aggregate by strategy\n",
    "strategy_summary = df.groupby('strategy').agg({\n",
    "    'accuracy': ['mean', 'std'],\n",
    "    'latency_s': 'mean',\n",
    "    'cost_usd': 'sum'\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nüìä Strategy Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(strategy_summary)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Accuracy ranking\n",
    "accuracy_ranking = df.groupby('strategy')['accuracy'].mean().sort_values(ascending=False)\n",
    "print(\"\\nüèÜ Accuracy Ranking:\")\n",
    "for i, (strategy, acc) in enumerate(accuracy_ranking.items(), 1):\n",
    "    print(f\"   {i}. {strategy:25s}: {acc:.1%}\")\n",
    "\n",
    "# Cost analysis\n",
    "cost_ranking = df.groupby('strategy')['cost_usd'].sum().sort_values()\n",
    "print(\"\\nüí∞ Cost Ranking (lowest to highest):\")\n",
    "for i, (strategy, cost) in enumerate(cost_ranking.items(), 1):\n",
    "    print(f\"   {i}. {strategy:25s}: ${cost:.4f}\")\n",
    "\n",
    "# Latency analysis\n",
    "latency_ranking = df.groupby('strategy')['latency_s'].mean().sort_values()\n",
    "print(\"\\n‚è±Ô∏è  Latency Ranking (fastest to slowest):\")\n",
    "for i, (strategy, latency) in enumerate(latency_ranking.items(), 1):\n",
    "    print(f\"   {i}. {strategy:25s}: {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i473m1sm4ro",
   "source": "## Task-by-Task Breakdown\n\n**Detailed view:** See which strategies succeeded/failed on each individual question",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5gyczu9zsgo",
   "source": "# Create detailed task-by-task breakdown\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìã DETAILED TASK-BY-TASK RESULTS\")\nprint(\"=\"*80)\n\n# Get list of strategies tested\nstrategies = df['strategy'].unique()\n\n# Build a lookup for task results\ntask_results = {}\nfor _, row in df.iterrows():\n    task_id = row['task_id']\n    if task_id not in task_results:\n        task_results[task_id] = {}\n    task_results[task_id][row['strategy']] = {\n        'accuracy': row['accuracy'],\n        'latency': row['latency_s'],\n        'cost': row['cost_usd']\n    }\n\n# Display each task with strategy results\nfor i, task in enumerate(tasks, 1):\n    print(f\"\\n{'‚îÄ'*80}\")\n    print(f\"üìù Task {i}/{len(tasks)}: {task.input[:150]}{'...' if len(task.input) > 150 else ''}\")\n    print(f\"\\n‚úì Expected Answer: {task.expected}\")\n    \n    if task.category:\n        print(f\"üè∑Ô∏è  Category: {task.category}\")\n    \n    print(f\"\\n Strategy Results:\")\n    print(f\" {'‚îÄ'*78}\")\n    \n    # Sort by accuracy (descending) to show best first\n    results_for_task = task_results.get(task.id, {})\n    sorted_strategies = sorted(strategies, \n                              key=lambda s: results_for_task.get(s, {}).get('accuracy', 0), \n                              reverse=True)\n    \n    for strategy in sorted_strategies:\n        if strategy in results_for_task:\n            result = results_for_task[strategy]\n            correct = result['accuracy'] == 1.0\n            symbol = \"‚úÖ\" if correct else \"‚ùå\"\n            status = \"CORRECT\" if correct else \"INCORRECT\"\n            \n            # Color code the status\n            print(f\"   {symbol} {strategy:25s}: {status:10s} | \" + \n                  f\"Latency: {result['latency']:6.2f}s | Cost: ${result['cost']:.4f}\")\n\n# Summary statistics\nprint(f\"\\n\\n{'='*80}\")\nprint(\"üìä TASK DIFFICULTY ANALYSIS\")\nprint(\"=\"*80)\n\n# Group tasks by how many strategies got them right\ntask_difficulty = df.groupby('task_id')['accuracy'].sum()\n\nprint(f\"\\n Tasks by difficulty (# of strategies that got it right):\")\nprint(f\" {'‚îÄ'*78}\")\n\nfor num_correct in sorted(task_difficulty.unique(), reverse=True):\n    count = (task_difficulty == num_correct).sum()\n    difficulty = \"üü¢ Easy\" if num_correct == len(strategies) else \\\n                 \"üü° Medium\" if num_correct >= len(strategies) / 2 else \\\n                 \"üî¥ Hard\"\n    print(f\"   {difficulty} - {int(num_correct)}/{len(strategies)} strategies correct: {count} tasks\")\n\n# Show which tasks only some strategies got right (multi-agent value!)\nprint(f\"\\n\\n{'='*80}\")\nprint(\"üí° TASKS WHERE MULTI-AGENT STRATEGIES HELPED\")\nprint(\"=\"*80)\n\nsingle_results = df[df['strategy'] == 'single'].set_index('task_id')['accuracy']\nother_results = df[df['strategy'] != 'single'].groupby('task_id')['accuracy'].max()\n\nhelped_tasks = []\nfor task_id in single_results.index:\n    if single_results[task_id] == 0 and other_results.get(task_id, 0) == 1:\n        helped_tasks.append(task_id)\n\nif helped_tasks:\n    print(f\"\\n‚úÖ Found {len(helped_tasks)} tasks where multi-agent strategies succeeded but single model failed:\")\n    for task_id in helped_tasks:\n        task = [t for t in tasks if t.id == task_id][0]\n        # Find which strategies got it right\n        right_strategies = df[(df['task_id'] == task_id) & (df['accuracy'] == 1.0)]['strategy'].tolist()\n        print(f\"\\n  üìù {task.input[:100]}{'...' if len(task.input) > 100 else ''}\")\n        print(f\"     ‚úì Solved by: {', '.join(right_strategies)}\")\nelse:\n    print(\"\\n‚ö†Ô∏è  No tasks found where multi-agent strategies outperformed single model\")\n\n# Show tasks that stumped everyone\nall_failed = df.groupby('task_id')['accuracy'].max()\nstumped_tasks = all_failed[all_failed == 0].index.tolist()\n\nif stumped_tasks:\n    print(f\"\\n\\n{'='*80}\")\n    print(f\"üî¥ TASKS THAT STUMPED ALL STRATEGIES ({len(stumped_tasks)} tasks)\")\n    print(\"=\"*80)\n    for task_id in stumped_tasks:\n        task = [t for t in tasks if t.id == task_id][0]\n        print(f\"\\n  ‚ùå {task.input[:150]}{'...' if len(task.input) > 150 else ''}\")\n        print(f\"     Expected: {task.expected}\")\nelse:\n    print(\"\\n\\n‚úÖ At least one strategy solved every task!\")\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Create comparison plots\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Color mapping for strategies\ncolor_map = {\n    'single': '#1f77b4',  # Blue\n    'design_critique': '#ff7f0e',  # Orange\n    'interdisciplinary_team': '#2ca02c',  # Green\n    'adaptive_team': '#d62728'  # Red (NEW!)\n}\n\n# 1. Accuracy by strategy\nax = axes[0, 0]\nstrategy_acc = df.groupby('strategy')['accuracy'].mean().sort_values(ascending=True)\ncolors = [color_map.get(s, '#gray') for s in strategy_acc.index]\nstrategy_acc.plot(kind='barh', ax=ax, color=colors)\nax.set_title(f'Accuracy on {BENCHMARK_NAME}', fontsize=14, weight='bold')\nax.set_xlabel('Accuracy')\nax.set_xlim(0, 1.0)\nax.grid(True, alpha=0.3)\n\n# 2. Latency by strategy\nax = axes[0, 1]\nstrategy_latency = df.groupby('strategy')['latency_s'].mean().sort_values()\ncolors = [color_map.get(s, '#gray') for s in strategy_latency.index]\nstrategy_latency.plot(kind='barh', ax=ax, color=colors)\nax.set_title('Average Latency', fontsize=14, weight='bold')\nax.set_xlabel('Seconds')\nax.grid(True, alpha=0.3)\n\n# 3. Cost by strategy\nax = axes[1, 0]\nstrategy_cost = df.groupby('strategy')['cost_usd'].sum().sort_values()\ncolors = [color_map.get(s, '#gray') for s in strategy_cost.index]\nstrategy_cost.plot(kind='barh', ax=ax, color=colors)\nax.set_title('Total Cost', fontsize=14, weight='bold')\nax.set_xlabel('USD')\nax.grid(True, alpha=0.3)\n\n# 4. Accuracy vs Latency scatter\nax = axes[1, 1]\nstrategy_stats = df.groupby('strategy').agg({\n    'accuracy': 'mean',\n    'latency_s': 'mean'\n})\nfor strategy, row in strategy_stats.iterrows():\n    ax.scatter(row['latency_s'], row['accuracy'], s=300, alpha=0.6, \n              color=color_map.get(strategy, '#gray'))\n    ax.annotate(strategy, (row['latency_s'], row['accuracy']), \n                xytext=(5, 5), textcoords='offset points', fontsize=9)\nax.set_xlabel('Avg Latency (s)', fontsize=11)\nax.set_ylabel('Accuracy', fontsize=11)\nax.set_title('Accuracy vs Latency Tradeoff', fontsize=14, weight='bold')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(f'../experiments/custom_strategies_{BENCHMARK_NAME}_comparison.png', \n           dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nüíæ Saved plot to: experiments/custom_strategies_{BENCHMARK_NAME}_comparison.png\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Compare to Published Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare your results to published baselines\n",
    "baselines = get_baseline_scores(BENCHMARK_NAME)\n",
    "\n",
    "if baselines:\n",
    "    # Get your best strategy\n",
    "    your_best = df.groupby('strategy')['accuracy'].mean().max()\n",
    "    your_best_strategy = df.groupby('strategy')['accuracy'].mean().idxmax()\n",
    "    \n",
    "    print(f\"\\nüìä Comparison to Published Baselines on {BENCHMARK_NAME}:\")\n",
    "    print(f\"\\nYour best: {your_best_strategy} = {your_best:.1%}\\n\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Add top 10 baselines\n",
    "    for model, score in sorted(baselines.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        comparison_data.append({\n",
    "            'Model/Strategy': model,\n",
    "            'Accuracy': score,\n",
    "            'Type': 'Published Baseline'\n",
    "        })\n",
    "    \n",
    "    # Add your results\n",
    "    for strategy, score in df.groupby('strategy')['accuracy'].mean().items():\n",
    "        comparison_data.append({\n",
    "            'Model/Strategy': f\"{MODEL} ({strategy})\",\n",
    "            'Accuracy': score,\n",
    "            'Type': 'Your Results'\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data).sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = ['steelblue' if t == 'Published Baseline' else 'coral' \n",
    "              for t in comparison_df['Type']]\n",
    "    \n",
    "    plt.barh(range(len(comparison_df)), comparison_df['Accuracy'], color=colors)\n",
    "    plt.yticks(range(len(comparison_df)), comparison_df['Model/Strategy'])\n",
    "    plt.xlabel('Accuracy', fontsize=12)\n",
    "    plt.title(f'{BENCHMARK_NAME} - Your Results vs Published Baselines (Jan 2025)', \n",
    "              fontsize=14, weight='bold')\n",
    "    plt.xlim(0, 1.0)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='steelblue', label='Published Baseline (Jan 2025)'),\n",
    "        Patch(facecolor='coral', label='Your Results')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    print(f\"   - Your best strategy: {your_best_strategy} ({your_best:.1%})\")\n",
    "    \n",
    "    # Find closest baseline\n",
    "    baseline_scores = list(baselines.values())\n",
    "    closest_baseline = min(baseline_scores, key=lambda x: abs(x - your_best))\n",
    "    closest_model = [k for k, v in baselines.items() if v == closest_baseline][0]\n",
    "    \n",
    "    print(f\"   - Closest published baseline: {closest_model} ({closest_baseline:.1%})\")\n",
    "    \n",
    "    if your_best > closest_baseline:\n",
    "        print(f\"   - ‚úÖ You're {(your_best - closest_baseline):.1%} better!\")\n",
    "    else:\n",
    "        print(f\"   - Room for improvement: {(closest_baseline - your_best):.1%} gap\")\n",
    "    \n",
    "    # Compare to top baseline\n",
    "    top_baseline = max(baseline_scores)\n",
    "    top_model = [k for k, v in baselines.items() if v == top_baseline][0]\n",
    "    print(f\"   - Top baseline: {top_model} ({top_baseline:.1%})\")\n",
    "    print(f\"   - Gap to SOTA: {(top_baseline - your_best):.1%}\")\n",
    "else:\n",
    "    print(\"No published baselines available for this benchmark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Summary\n",
    "\n",
    "[Fill in after running]\n",
    "\n",
    "- **Best strategy:** _____ (___% accuracy)\n",
    "- **Single-model baseline:** ___% accuracy\n",
    "- **Design Critique:** ___% accuracy (___% vs baseline)\n",
    "- **XFN Team:** ___% accuracy (___% vs baseline)\n",
    "- **Latency overhead:** Design Critique ___x, XFN Team ___x\n",
    "- **Cost overhead:** Design Critique $_____, XFN Team $_____\n",
    "\n",
    "### Insights\n",
    "\n",
    "- **When did custom strategies help?**\n",
    "- **Was the cost/latency tradeoff worth it?**\n",
    "- **How do we compare to published SOTA?**\n",
    "- **Which tasks benefited most from multi-agent approaches?**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Test on different benchmarks\n",
    "2. Tune agent personas for specific domains\n",
    "3. Adjust n_iterations / refinement_rounds\n",
    "4. Try with larger models\n",
    "5. Analyze failure cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Examine specific failures to understand where strategies struggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find tasks where all strategies failed\n",
    "task_accuracy = df.groupby('task_id')['accuracy'].mean()\n",
    "hard_tasks = task_accuracy[task_accuracy == 0].index.tolist()\n",
    "\n",
    "if hard_tasks:\n",
    "    print(f\"\\n‚ùå Tasks where all strategies failed: {len(hard_tasks)}\")\n",
    "    print(f\"\\nSample hard task:\")\n",
    "    hard_task = [t for t in tasks if t.id == hard_tasks[0]][0]\n",
    "    print(f\"  {hard_task.input[:200]}...\")\n",
    "    print(f\"  Expected: {hard_task.expected}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ At least one strategy got each task correct!\")\n",
    "\n",
    "# Find tasks where custom strategies helped most\n",
    "single_results = df[df['strategy'] == 'single'].set_index('task_id')['accuracy']\n",
    "multi_results = df[df['strategy'] != 'single'].groupby('task_id')['accuracy'].max()\n",
    "\n",
    "improvement = multi_results - single_results\n",
    "best_improvements = improvement.nlargest(3)\n",
    "\n",
    "if len(best_improvements) > 0 and best_improvements.max() > 0:\n",
    "    print(f\"\\n‚úÖ Tasks where custom strategies helped most:\")\n",
    "    for task_id, improvement_val in best_improvements.items():\n",
    "        if improvement_val > 0:\n",
    "            task = [t for t in tasks if t.id == task_id][0]\n",
    "            print(f\"\\n  Task: {task.input[:100]}...\")\n",
    "            print(f\"  Single: {single_results[task_id]:.0%} ‚Üí Custom: {multi_results[task_id]:.0%}\")\n",
    "            print(f\"  Improvement: +{improvement_val:.0%}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Custom strategies didn't outperform single model on any tasks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}