{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Benchmark Evaluation: Standard Datasets\n",
    "\n",
    "**Goal:** Evaluate single-model and multi-agent strategies on established benchmarks\n",
    "\n",
    "**Date:** [Fill in]  \n",
    "**Author:** Leif Haven Martinson  \n",
    "\n",
    "## Why Benchmarks?\n",
    "\n",
    "- **Reproducibility**: Compare to published baselines\n",
    "- **Validity**: Prove multi-agent helps on well-studied tasks\n",
    "- **Generalization**: Test across diverse problem types\n",
    "- **Calibration**: Understand if improvements are meaningful\n",
    "\n",
    "## Available Benchmarks\n",
    "\n",
    "1. **GSM8K** - Grade school math word problems (multi-step reasoning)\n",
    "2. **MMLU** - Multitask language understanding (57 subjects)\n",
    "3. **TruthfulQA** - Truthfulness and avoiding misconceptions\n",
    "4. **ARC** - Science reasoning questions\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "- Does debate improve accuracy on math reasoning (GSM8K)?\n",
    "- Does consensus reduce hallucinations (TruthfulQA)?\n",
    "- How do local models compare to API baselines?\n",
    "- What's the cost/performance tradeoff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "from harness import (\n",
    "    load_benchmark,\n",
    "    get_baseline_scores,\n",
    "    BENCHMARKS,\n",
    "    run_strategy,\n",
    "    ExperimentConfig,\n",
    "    ExperimentResult,\n",
    "    get_tracker\n",
    ")\n",
    "from harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… Setup complete\")\n",
    "print(f\"\\nAvailable benchmarks: {list(BENCHMARKS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**CUSTOMIZE HERE:** Choose benchmark and strategies to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ys3i8x9wh4i",
   "source": "# ========================================\n# BENCHMARK CONFIGURATION\n# ========================================\n\n# Which benchmark to run\nBENCHMARK_NAME = \"gsm8k\"  # ðŸ”§ CHANGE: \"gsm8k\", \"mmlu\", \"truthfulqa\", \"arc\"\n\n# How many tasks to evaluate (None = all, start with 10-20 for testing)\nNUM_TASKS = 5  # ðŸ”§ CHANGE: Start small, increase for full eval\n\n# Random seed for reproducibility\nSEED = 42\n\n# Model configuration\nPROVIDER = DEFAULT_PROVIDER\nMODEL = DEFAULT_MODEL\n\n# ========================================\n# STRATEGIES TO COMPARE\n# ========================================\n\n# Define which strategies to test\n# Each entry: (strategy_name, kwargs)\nSTRATEGIES_TO_TEST = [\n    # Baseline\n    (\"single\", {\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n    }),\n\n    # Debate with 2 agents, 1 round\n    (\"debate\", {\n        \"n_debaters\": 2,\n        \"n_rounds\": 1,\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n    }),\n\n    # Debate with 3 agents, 2 rounds\n    (\"debate\", {\n        \"n_debaters\": 3,\n        \"n_rounds\": 2,\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n    }),\n\n    # Consensus (no judge)\n    (\"consensus\", {\n        \"n_agents\": 3,\n        \"n_rounds\": 2,\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n    }),\n\n    # Self-consistency\n    (\"self_consistency\", {\n        \"n_samples\": 5,\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"temperature\": 0.8\n    }),\n\n    # ========================================\n    # CUSTOM MENTAL MODEL STRATEGIES (ACTIVE)\n    # ========================================\n\n    # Design Critique - Uses custom CRITIQUE_PANEL from cell above\n    # ðŸ”§ Edit the CRITIQUE_PANEL cell above to customize critics\n    (\"design_critique\", {\n        \"n_iterations\": 2,\n        \"critique_panel\": CRITIQUE_PANEL,  # Uses your custom panel!\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": True  # ðŸ”§ Set to True to see streaming output\n    }),\n\n    # Interdisciplinary Team - Uses custom EXPERT_TEAM from cell above\n    # ðŸ”§ Edit the EXPERT_TEAM cell above to customize team members\n    (\"interdisciplinary_team\", {\n        \"refinement_rounds\": 1,\n        \"expert_team\": EXPERT_TEAM,  # Uses your custom team!\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": True  # ðŸ”§ Set to True to see streaming output\n    }),\n]\n\nprint(f\"âœ… Configuration:\")\nprint(f\"   Benchmark: {BENCHMARK_NAME}\")\nprint(f\"   Tasks: {NUM_TASKS if NUM_TASKS else 'all'}\")\nprint(f\"   Strategies to test: {len(STRATEGIES_TO_TEST)}\")\nprint(f\"   Model: {MODEL} ({PROVIDER})\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ttk0pn8i63",
   "source": "# ========================================\n# DESIGN CRITIQUE PANEL - EDIT THESE PERSONAS\n# ========================================\n\nCRITIQUE_PANEL = [\n    {\n        \"name\": \"Systems Designer\",\n        \"focus\": \"System architecture and holistic design\",\n        \"criteria\": \"\"\"You are a Systems Designer who thinks about the big picture and interconnections.\nEvaluate:\n- Does the solution consider the whole system and its parts?\n- Are relationships between components clear?\n- Is there coherence between different elements?\n- Does the design scale and adapt to different contexts?\nFocus on holistic thinking, interconnections, and systemic coherence.\"\"\"\n    },\n    {\n        \"name\": \"Visual Craft Specialist\",\n        \"focus\": \"Visual clarity, aesthetics, and presentation\",\n        \"criteria\": \"\"\"You are a Visual Craft Specialist focused on how information is presented and perceived.\nEvaluate:\n- Is information presented clearly and visually comprehensible?\n- Is there good hierarchy and structure in the presentation?\n- Are concepts illustrated or explained in ways that are easy to visualize?\n- Does the format enhance understanding?\nFocus on clarity of presentation, visual thinking, and aesthetic quality.\"\"\"\n    },\n    {\n        \"name\": \"AI Specialist\",\n        \"focus\": \"AI/ML capabilities, limitations, and best practices\",\n        \"criteria\": \"\"\"You are an AI Specialist with deep knowledge of AI systems, capabilities, and limitations.\nEvaluate:\n- Are claims about AI accurate and grounded in current capabilities?\n- Are limitations and potential issues with AI acknowledged?\n- Are AI-related recommendations practical and informed?\n- Is the approach aligned with AI best practices?\nFocus on technical accuracy regarding AI/ML, practical feasibility, and responsible AI considerations.\"\"\"\n    },\n    {\n        \"name\": \"Human-Computer Interaction Expert\",\n        \"focus\": \"User experience, usability, and human factors\",\n        \"criteria\": \"\"\"You are an HCI Expert focused on how humans interact with systems and information.\nEvaluate:\n- Is the solution usable and accessible to the target audience?\n- Are user needs and cognitive limitations considered?\n- Is interaction intuitive and aligned with mental models?\n- Are there potential usability issues or barriers?\nFocus on user-centered design, accessibility, cognitive ergonomics, and interaction patterns.\"\"\"\n    },\n    {\n        \"name\": \"IDEO Design Thinking Facilitator\",\n        \"focus\": \"Human-centered innovation and creative problem-solving\",\n        \"criteria\": \"\"\"You are an IDEO-trained Design Thinking practitioner emphasizing empathy, ideation, and iteration.\nEvaluate:\n- Does the solution demonstrate empathy for user needs and pain points?\n- Is there creative thinking and exploration of possibilities?\n- Are assumptions tested or validated?\n- Is the approach iterative and open to refinement?\nFocus on empathy, creative exploration, prototyping mindset, and bias toward action.\"\"\"\n    },\n]\n\n# ========================================\n# INTERDISCIPLINARY TEAM - EDIT THESE PERSONAS\n# ========================================\n\nEXPERT_TEAM = [\n    {\n        \"name\": \"Product Manager\",\n        \"role\": \"Product Management\",\n        \"perspective\": \"User needs, business value, roadmap, and strategic priorities\",\n        \"system_prompt\": \"\"\"You are a Product Manager responsible for defining what to build and why.\n\nYour mission: Ensure the solution creates real user value while achieving business objectives.\n\nFocus on:\n- User needs, pain points, and jobs-to-be-done\n- Business impact, ROI, and strategic alignment\n- Feature prioritization and tradeoffs\n- Market fit and competitive positioning\n- Success metrics and measurable outcomes\n- Feasibility vs. value vs. risk assessment\n\nAnalyze problems by asking:\n- What user problem does this solve?\n- What is the business value?\n- How do we measure success?\n- What are the must-haves vs. nice-to-haves?\n- What are the risks and mitigations?\n\nBring a balanced perspective that bridges user needs, business goals, and technical reality.\"\"\"\n    },\n    {\n        \"name\": \"Software Engineer\",\n        \"role\": \"Engineering\",\n        \"perspective\": \"Technical implementation, architecture, scalability, and feasibility\",\n        \"system_prompt\": \"\"\"You are a Software Engineer responsible for building and shipping reliable systems.\n\nYour mission: Deliver technically sound solutions that are maintainable, scalable, and feasible within constraints.\n\nFocus on:\n- Technical feasibility and implementation complexity\n- System architecture and design patterns\n- Scalability, performance, and reliability\n- Security, data integrity, and edge cases\n- Technical debt and long-term maintainability\n- Development velocity and engineering resources\n- Integration with existing systems\n\nAnalyze problems by asking:\n- Is this technically feasible?\n- What is the implementation complexity?\n- What are the technical risks?\n- How does this scale?\n- What are the dependencies and blockers?\n- What technical debt are we taking on?\n\nBring a pragmatic engineering perspective focused on what we can actually build and ship.\"\"\"\n    },\n    {\n        \"name\": \"Product Designer\",\n        \"role\": \"Design\",\n        \"perspective\": \"User experience, interaction design, usability, and design quality\",\n        \"system_prompt\": \"\"\"You are a Product Designer responsible for crafting intuitive, delightful user experiences.\n\nYour mission: Ensure the solution is usable, accessible, and provides a great user experience.\n\nFocus on:\n- User experience and interaction design\n- Usability, learnability, and accessibility\n- User flows and mental models\n- Information architecture and navigation\n- Visual design and brand consistency\n- Edge cases and error states\n- User research insights and validation\n\nAnalyze problems by asking:\n- Is this intuitive for users?\n- What is the user flow?\n- Are there usability issues or friction points?\n- Is it accessible to all users?\n- How do we handle edge cases and errors?\n- Does this match user mental models?\n\nBring a user-centered design perspective that ensures solutions are not just functional but delightful to use.\"\"\"\n    },\n]\n\nprint(f\"âœ… Agent personas configured:\")\nprint(f\"   Design Critique Panel: {len(CRITIQUE_PANEL)} critics\")\nprint(f\"   Interdisciplinary Team: {len(EXPERT_TEAM)} experts\")\nprint(f\"\\nðŸ’¡ Edit these personas above to experiment with different perspectives!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# BENCHMARK CONFIGURATION\n# ========================================\n\n# Which benchmark to run\nBENCHMARK_NAME = \"gsm8k\"  # ðŸ”§ CHANGE: \"gsm8k\", \"mmlu\", \"truthfulqa\", \"arc\"\n\n# How many tasks to evaluate (None = all, start with 10-20 for testing)\nNUM_TASKS = 5  # ðŸ”§ CHANGE: Start small, increase for full eval\n\n# Random seed for reproducibility\nSEED = 42\n\n# Model configuration\nPROVIDER = DEFAULT_PROVIDER\nMODEL = DEFAULT_MODEL\n\n# ========================================\n# STRATEGIES TO COMPARE\n# ========================================\n\n# Define which strategies to test\n# Each entry: (strategy_name, kwargs)\nSTRATEGIES_TO_TEST = [\n    # Baseline\n    (\"single\", {\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n    }),\n\n    # Debate with 2 agents, 1 round\n    (\"debate\", {\n        \"n_debaters\": 2,\n        \"n_rounds\": 1,\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n    }),\n\n    # Debate with 3 agents, 2 rounds\n    (\"debate\", {\n        \"n_debaters\": 3,\n        \"n_rounds\": 2,\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n    }),\n\n    # Consensus (no judge)\n    (\"consensus\", {\n        \"n_agents\": 3,\n        \"n_rounds\": 2,\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n    }),\n\n    # Self-consistency\n    (\"self_consistency\", {\n        \"n_samples\": 5,\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"temperature\": 0.8\n    }),\n\n    # ========================================\n    # CUSTOM MENTAL MODEL STRATEGIES (ACTIVE)\n    # ========================================\n\n    # Design Critique - 5 designer archetypes provide feedback\n    # Designers: Systems, Visual Craft, AI Specialist, HCI, IDEO Design Thinking\n    (\"design_critique\", {\n        \"n_iterations\": 2,  # Number of critique/revision cycles\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n        # Uses default design panel (can customize with critique_panel param)\n    }),\n\n    # Interdisciplinary Team - Classic tech team collaboration\n    # Team: Product Manager, Software Engineer, Product Designer\n    (\"interdisciplinary_team\", {\n        \"refinement_rounds\": 1,  # Number of refinement iterations\n        \"provider\": PROVIDER,\n        \"model\": MODEL,\n        \"verbose\": False\n        # Uses default tech team (can customize with expert_team param)\n    }),\n]\n\nprint(f\"âœ… Configuration:\")\nprint(f\"   Benchmark: {BENCHMARK_NAME}\")\nprint(f\"   Tasks: {NUM_TASKS if NUM_TASKS else 'all'}\")\nprint(f\"   Strategies to test: {len(STRATEGIES_TO_TEST)}\")\nprint(f\"   Model: {MODEL} ({PROVIDER})\")"
  },
  {
   "cell_type": "markdown",
   "id": "2l5lhrxjus8",
   "source": "## Custom Mental Model Strategies\n\nThe benchmark framework now supports **design critique** and **interdisciplinary team** strategies from notebooks 05 and 06!\n\n### Design Critique Strategy\n\n**5 Designer Archetypes** provide structured feedback â†’ Iterative refinement\n\n**Default Design Panel:**\n1. **Systems Designer** - Holistic design, interconnections, systemic coherence\n2. **Visual Craft Specialist** - Presentation clarity, visual hierarchy, aesthetics\n3. **AI Specialist** - AI/ML accuracy, capabilities, limitations, best practices\n4. **Human-Computer Interaction Expert** - Usability, accessibility, cognitive ergonomics\n5. **IDEO Design Thinking Facilitator** - Empathy, creative exploration, iteration\n\n**Use cases:**\n- Writing tasks (blog posts, documentation, creative writing)\n- Design and product solutions\n- Tasks where quality improvement through iteration is valuable\n- When you want structured feedback on different quality dimensions\n\n**Configuration:**\n```python\n(\"design_critique\", {\n    \"n_iterations\": 2,  # How many critique/revision cycles\n    \"provider\": PROVIDER,\n    \"model\": MODEL,\n    # Optional: Custom critique panel (see strategies.py for examples)\n})\n```\n\n---\n\n### Interdisciplinary Team Strategy\n\n**Classic Tech Team** (PM, Engineer, Designer) collaborates on solutions\n\n**Default Tech Team:**\n1. **Product Manager** - User needs, business value, priorities, success metrics\n   - Mission: Create real user value while achieving business objectives\n2. **Software Engineer** - Technical feasibility, architecture, scalability, implementation\n   - Mission: Deliver technically sound, maintainable, scalable solutions\n3. **Product Designer** - User experience, interaction design, usability, accessibility\n   - Mission: Craft intuitive, delightful user experiences\n\n**Use cases:**\n- Complex problems requiring multiple perspectives\n- Product and feature planning\n- Strategic decision-making\n- Tasks where PM/Eng/Design perspectives are valuable\n\n**Configuration:**\n```python\n(\"interdisciplinary_team\", {\n    \"refinement_rounds\": 1,  # How many refinement cycles\n    \"provider\": PROVIDER,\n    \"model\": MODEL,\n    # Optional: Custom expert team (see strategies.py for examples)\n})\n```\n\n---\n\n**Status:** Both strategies are **active by default** in the configuration above. To disable, comment them out in `STRATEGIES_TO_TEST`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark\n",
    "benchmark = load_benchmark(BENCHMARK_NAME)\n",
    "tasks = benchmark.get_tasks(n=NUM_TASKS, seed=SEED)\n",
    "\n",
    "print(f\"ðŸ“Š Loaded {len(tasks)} tasks from {BENCHMARK_NAME}\")\n",
    "print(f\"\\nSample task:\")\n",
    "print(f\"  Input: {tasks[0].input[:100]}...\")\n",
    "print(f\"  Expected: {tasks[0].expected}\")\n",
    "print(f\"  Category: {tasks[0].category}\")\n",
    "\n",
    "# Show baseline scores from literature\n",
    "baselines = get_baseline_scores(BENCHMARK_NAME)\n",
    "if baselines:\n",
    "    print(f\"\\nðŸ“ˆ Published baseline scores on {BENCHMARK_NAME}:\")\n",
    "    for model, score in sorted(baselines.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {model:20s}: {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Test each strategy on the benchmark tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for each strategy\n",
    "results = []\n",
    "\n",
    "for strategy_name, strategy_kwargs in STRATEGIES_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ”„ Running: {strategy_name}\")\n",
    "    print(f\"   Config: {strategy_kwargs}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create experiment tracker\n",
    "    config = ExperimentConfig(\n",
    "        experiment_name=f\"benchmark_{BENCHMARK_NAME}_{strategy_name}\",\n",
    "        task_type=BENCHMARK_NAME,\n",
    "        strategy=strategy_name,\n",
    "        provider=PROVIDER,\n",
    "        model=MODEL,\n",
    "        notes=f\"Benchmark eval on {BENCHMARK_NAME}\"\n",
    "    )\n",
    "    \n",
    "    tracker = get_tracker()\n",
    "    tracker.start_experiment(config)\n",
    "    \n",
    "    # Run on each task\n",
    "    for task in tqdm(tasks, desc=f\"{strategy_name}\"):\n",
    "        # Run strategy\n",
    "        result = run_strategy(strategy_name, task.input, **strategy_kwargs)\n",
    "        \n",
    "        # Evaluate with benchmark-specific eval\n",
    "        eval_scores = benchmark.evaluate(task, result.output)\n",
    "        \n",
    "        # Log result\n",
    "        exp_result = ExperimentResult(\n",
    "            config=config,\n",
    "            task_input=task.input,\n",
    "            output=result.output,\n",
    "            latency_s=result.latency_s,\n",
    "            tokens_in=result.tokens_in,\n",
    "            tokens_out=result.tokens_out,\n",
    "            cost_usd=result.cost_usd,\n",
    "            eval_scores=eval_scores,\n",
    "            eval_metadata={\n",
    "                'task_id': task.id,\n",
    "                'expected': task.expected,\n",
    "                'benchmark': BENCHMARK_NAME\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        tracker.log_result(exp_result)\n",
    "        results.append({\n",
    "            'strategy': strategy_name,\n",
    "            'task_id': task.id,\n",
    "            'accuracy': eval_scores.get('accuracy', 0),\n",
    "            'latency_s': result.latency_s,\n",
    "            'cost_usd': result.cost_usd or 0\n",
    "        })\n",
    "    \n",
    "    # Finish experiment\n",
    "    summary = tracker.finish_experiment()\n",
    "    \n",
    "    print(f\"\\nâœ… {strategy_name} complete:\")\n",
    "    print(f\"   Accuracy: {summary['eval_scores'].get('accuracy', {}).get('mean', 0):.1%}\")\n",
    "    print(f\"   Avg latency: {summary['avg_latency_s']:.2f}s\")\n",
    "    print(f\"   Total cost: ${summary['total_cost_usd']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… All strategies evaluated!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Aggregate by strategy\n",
    "strategy_summary = df.groupby('strategy').agg({\n",
    "    'accuracy': ['mean', 'std'],\n",
    "    'latency_s': 'mean',\n",
    "    'cost_usd': 'sum'\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nðŸ“Š Strategy Comparison:\")\n",
    "print(strategy_summary)\n",
    "\n",
    "# Sort by accuracy\n",
    "accuracy_ranking = df.groupby('strategy')['accuracy'].mean().sort_values(ascending=False)\n",
    "print(\"\\nðŸ† Accuracy Ranking:\")\n",
    "for i, (strategy, acc) in enumerate(accuracy_ranking.items(), 1):\n",
    "    print(f\"   {i}. {strategy:20s}: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Accuracy by strategy\n",
    "ax = axes[0, 0]\n",
    "strategy_acc = df.groupby('strategy')['accuracy'].mean().sort_values(ascending=True)\n",
    "strategy_acc.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_title(f'Accuracy on {BENCHMARK_NAME}', fontsize=14, weight='bold')\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_xlim(0, 1.0)\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.3, label='Random (if applicable)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Latency by strategy\n",
    "ax = axes[0, 1]\n",
    "strategy_latency = df.groupby('strategy')['latency_s'].mean().sort_values()\n",
    "strategy_latency.plot(kind='barh', ax=ax, color='coral')\n",
    "ax.set_title('Average Latency', fontsize=14, weight='bold')\n",
    "ax.set_xlabel('Seconds')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cost by strategy\n",
    "ax = axes[1, 0]\n",
    "strategy_cost = df.groupby('strategy')['cost_usd'].sum().sort_values()\n",
    "strategy_cost.plot(kind='barh', ax=ax, color='green')\n",
    "ax.set_title('Total Cost', fontsize=14, weight='bold')\n",
    "ax.set_xlabel('USD')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Accuracy vs Latency scatter\n",
    "ax = axes[1, 1]\n",
    "strategy_stats = df.groupby('strategy').agg({\n",
    "    'accuracy': 'mean',\n",
    "    'latency_s': 'mean'\n",
    "})\n",
    "ax.scatter(strategy_stats['latency_s'], strategy_stats['accuracy'], s=200, alpha=0.6)\n",
    "for strategy, row in strategy_stats.iterrows():\n",
    "    ax.annotate(strategy, (row['latency_s'], row['accuracy']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "ax.set_xlabel('Avg Latency (s)', fontsize=11)\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('Accuracy vs Latency Tradeoff', fontsize=14, weight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../experiments/benchmark_{BENCHMARK_NAME}_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved plot to: experiments/benchmark_{BENCHMARK_NAME}_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Compare to Published Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare your results to published baselines\n",
    "baselines = get_baseline_scores(BENCHMARK_NAME)\n",
    "\n",
    "if baselines:\n",
    "    # Get your best strategy\n",
    "    your_best = df.groupby('strategy')['accuracy'].mean().max()\n",
    "    your_best_strategy = df.groupby('strategy')['accuracy'].mean().idxmax()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Comparison to Published Baselines on {BENCHMARK_NAME}:\")\n",
    "    print(f\"\\nYour best: {your_best_strategy} = {your_best:.1%}\\n\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Add baselines\n",
    "    for model, score in baselines.items():\n",
    "        comparison_data.append({\n",
    "            'Model/Strategy': model,\n",
    "            'Accuracy': score,\n",
    "            'Type': 'Published Baseline'\n",
    "        })\n",
    "    \n",
    "    # Add your results\n",
    "    for strategy, score in df.groupby('strategy')['accuracy'].mean().items():\n",
    "        comparison_data.append({\n",
    "            'Model/Strategy': f\"{MODEL} ({strategy})\",\n",
    "            'Accuracy': score,\n",
    "            'Type': 'Your Results'\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data).sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['steelblue' if t == 'Published Baseline' else 'coral' \n",
    "              for t in comparison_df['Type']]\n",
    "    \n",
    "    plt.barh(range(len(comparison_df)), comparison_df['Accuracy'], color=colors)\n",
    "    plt.yticks(range(len(comparison_df)), comparison_df['Model/Strategy'])\n",
    "    plt.xlabel('Accuracy', fontsize=12)\n",
    "    plt.title(f'{BENCHMARK_NAME} - Your Results vs Published Baselines', \n",
    "              fontsize=14, weight='bold')\n",
    "    plt.xlim(0, 1.0)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='steelblue', label='Published Baseline'),\n",
    "        Patch(facecolor='coral', label='Your Results')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Insights:\")\n",
    "    print(f\"   - Your best strategy achieved {your_best:.1%} accuracy\")\n",
    "    \n",
    "    # Find closest baseline\n",
    "    baseline_scores = list(baselines.values())\n",
    "    closest_baseline = min(baseline_scores, key=lambda x: abs(x - your_best))\n",
    "    closest_model = [k for k, v in baselines.items() if v == closest_baseline][0]\n",
    "    \n",
    "    print(f\"   - Closest published baseline: {closest_model} ({closest_baseline:.1%})\")\n",
    "    \n",
    "    if your_best > closest_baseline:\n",
    "        print(f\"   - âœ… You're {(your_best - closest_baseline):.1%} better!\")\n",
    "    else:\n",
    "        print(f\"   - Room for improvement: {(closest_baseline - your_best):.1%} gap\")\n",
    "else:\n",
    "    print(\"No published baselines available for this benchmark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Quantitative Results\n",
    "\n",
    "[Fill in after running]\n",
    "\n",
    "- Best strategy: _____ (___% accuracy)\n",
    "- Single-model baseline: ___% accuracy\n",
    "- Improvement from multi-agent: ___%\n",
    "- Latency overhead: ___x\n",
    "- Cost overhead: $___\n",
    "\n",
    "### Insights\n",
    "\n",
    "- When did multi-agent help vs hurt?\n",
    "- Was the cost/latency tradeoff worth it?\n",
    "- How do we compare to published baselines?\n",
    "- Which strategy is best for this benchmark?\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Test on different benchmarks\n",
    "2. Tune debate/consensus parameters\n",
    "3. Try with larger models\n",
    "4. Analyze failure cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Examine specific failures to understand where strategies struggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find tasks where all strategies failed\n",
    "task_accuracy = df.groupby('task_id')['accuracy'].mean()\n",
    "hard_tasks = task_accuracy[task_accuracy == 0].index.tolist()\n",
    "\n",
    "if hard_tasks:\n",
    "    print(f\"\\nâŒ Tasks where all strategies failed: {len(hard_tasks)}\")\n",
    "    print(f\"\\nSample hard task:\")\n",
    "    hard_task = [t for t in tasks if t.id == hard_tasks[0]][0]\n",
    "    print(f\"  {hard_task.input[:200]}...\")\n",
    "    print(f\"  Expected: {hard_task.expected}\")\n",
    "else:\n",
    "    print(\"\\nâœ… At least one strategy got each task correct!\")\n",
    "\n",
    "# Find tasks where multi-agent helped most\n",
    "single_results = df[df['strategy'] == 'single'].set_index('task_id')['accuracy']\n",
    "multi_results = df[df['strategy'] != 'single'].groupby('task_id')['accuracy'].max()\n",
    "\n",
    "improvement = multi_results - single_results\n",
    "best_improvements = improvement.nlargest(3)\n",
    "\n",
    "if len(best_improvements) > 0:\n",
    "    print(f\"\\nâœ… Tasks where multi-agent helped most:\")\n",
    "    for task_id, improvement in best_improvements.items():\n",
    "        if improvement > 0:\n",
    "            task = [t for t in tasks if t.id == task_id][0]\n",
    "            print(f\"\\n  Task: {task.input[:100]}...\")\n",
    "            print(f\"  Single: {single_results[task_id]:.0%} â†’ Multi: {multi_results[task_id]:.0%}\")\n",
    "            print(f\"  Improvement: +{improvement:.0%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}