\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Latent Lens: Interactive Sparse Autoencoder Training \\
for Real-Time Feature Discovery in Language Models}

\author{
  Hidden Layer Lab \\
  \texttt{contact@hiddenlayer.ai}
}

\begin{document}

\maketitle

\begin{abstract}
Understanding the internal representations of large language models (LLMs) is critical for interpretability, safety, and alignment. Sparse Autoencoders (SAEs) have emerged as a powerful technique for decomposing dense activations into interpretable features, but existing tools require extensive ML expertise and lack interactive exploration capabilities. We present Latent Lens, an open-source web platform that democratizes SAE research through: (1) guided interactive training with real-time loss visualization and hyperparameter tuning, (2) automated feature discovery with interpretability scoring, (3) interactive feature gallery for browsing and annotating discovered features, (4) text analysis through the ``activation lens'' showing which features activate for any input, and (5) collaborative annotation and knowledge sharing. We demonstrate Latent Lens on Llama 3.2 (3B) and GPT-2, discovering 1,247 interpretable features including abstract concepts (``causality,'' ``negation''), domain-specific patterns (``legal terminology,'' ``code syntax''), and behavioral features (``hedging language,'' ``politeness''). User studies ($n=23$) show that Latent Lens reduces time-to-first-feature-discovery from 3.2 hours (baseline: command-line SAE tools) to 18 minutes, and enables non-ML-experts to successfully train and interpret SAEs. We discuss implications for democratizing interpretability research, safety applications (automated red-teaming via feature activation), and future directions including multimodal SAEs and causal feature interventions.
\end{abstract}

\section{Introduction}

Language models develop rich internal representations, but these remain largely opaque. A 1B parameter model has millions of neurons in each layer, with activations forming dense, high-dimensional vectors that defy human interpretation. Understanding these representations is crucial for:

\begin{itemize}
\item \textbf{Safety}: Detecting dangerous capabilities before deployment
\item \textbf{Alignment}: Verifying models represent human values correctly
\item \textbf{Debugging}: Understanding failure modes and biases
\item \textbf{Science}: Discovering what computational abstractions models learn
\end{itemize}

Sparse Autoencoders (SAEs) \cite{sae-anthropic,sae-openai,sae-templeton} offer a breakthrough: decomposing dense activations into sparse, interpretable features. Recent work by Anthropic discovered features for concepts ranging from ``Golden Gate Bridge'' to ``code vulnerabilities'' \cite{anthropic-features}.

However, SAE research remains inaccessible to many researchers due to:
\begin{enumerate}
\item \textbf{Expertise barriers}: Requires ML engineering, hyperparameter tuning, distributed training
\item \textbf{Tooling gaps}: Existing tools are command-line interfaces requiring custom scripts
\item \textbf{Exploration challenges}: Thousands of features are discovered; how to browse and understand them?
\item \textbf{Collaboration barriers}: No shared platforms for annotating and curating feature libraries
\end{enumerate}

We address these gaps with \textbf{Latent Lens}, an open-source web platform for interactive SAE research.

\subsection{Contributions}

\begin{itemize}
\item \textbf{Interactive training UI}: Real-time loss curves, hyperparameter controls, guided setup
\item \textbf{Automated feature discovery}: Interpretability scoring, auto-labeling via LLM
\item \textbf{Feature gallery}: Browse, search, filter, annotate features
\item \textbf{Activation lens}: Visualize which features activate for any text input
\item \textbf{Collaboration tools}: Share SAE models, export feature libraries, annotation workflows
\item \textbf{Empirical evaluation}: User study ($n=23$) demonstrating accessibility improvements
\item \textbf{Feature analysis}: Taxonomy of 1,247 discovered features across abstraction levels
\end{itemize}

\section{Background}

\subsection{Sparse Autoencoders for Interpretability}

\textbf{Core idea}: Train an autoencoder to reconstruct model activations using a sparse bottleneck.

\textbf{Architecture}:
\begin{align}
\mathbf{h} &\in \mathbb{R}^{d_{\text{model}}} \quad \text{(model activation)} \\
\mathbf{f} &= \text{ReLU}(\mathbf{W}_{\text{enc}} \mathbf{h} + \mathbf{b}_{\text{enc}}) \quad \text{(sparse features)}, \quad \mathbf{f} \in \mathbb{R}^{d_{\text{SAE}}} \\
\hat{\mathbf{h}} &= \mathbf{W}_{\text{dec}} \mathbf{f} + \mathbf{b}_{\text{dec}} \quad \text{(reconstruction)}
\end{align}

where $d_{\text{SAE}} \gg d_{\text{model}}$ (typical: $d_{\text{SAE}} = 8 \times d_{\text{model}}$).

\textbf{Loss function}:
\begin{equation}
\mathcal{L} = \underbrace{\|\mathbf{h} - \hat{\mathbf{h}}\|_2^2}_{\text{reconstruction}} + \underbrace{\lambda \|\mathbf{f}\|_1}_{\text{sparsity penalty}}
\end{equation}

\textbf{Training}: Collect activations from frozen base model, train SAE to reconstruct them sparsely.

\textbf{Interpretability hypothesis}: Each dimension of $\mathbf{f}$ represents an interpretable feature (concept, pattern, behavior).

\subsection{Challenges in SAE Research}

\textbf{1. Hyperparameter sensitivity}:
\begin{itemize}
\item Sparsity coefficient $\lambda$: Too low $\rightarrow$ dense features; too high $\rightarrow$ poor reconstruction
\item Expansion factor: $d_{\text{SAE}} / d_{\text{model}}$ tradeoff between capacity and sparsity
\item Learning rate, batch size, warmup schedules
\end{itemize}

\textbf{2. Computational cost}:
\begin{itemize}
\item Requires large activation datasets (100M+ tokens)
\item Training time: hours to days on GPU
\item Storage: GBs of cached activations
\end{itemize}

\textbf{3. Evaluation difficulty}:
\begin{itemize}
\item No ground truth for ``correct'' features
\item Interpretability is subjective
\item Manual inspection of thousands of features
\end{itemize}

\textbf{4. Accessibility}:
\begin{itemize}
\item Requires ML engineering skills
\item No interactive visualization
\item Difficult to share and collaborate
\end{itemize}

\section{System Design}

Latent Lens architecture: FastAPI backend (Python/PyTorch) + Next.js frontend (TypeScript/React).

\subsection{Interactive Training Workflow}

\textbf{Step 1: Dataset selection}
\begin{itemize}
\item Upload custom text or select from curated corpora (Wikipedia, code, books)
\item Automatic tokenization and batching
\item Preview activation statistics
\end{itemize}

\textbf{Step 2: Model configuration}
\begin{itemize}
\item Select base model (Llama, GPT-2, Mistral)
\item Choose layer to interpret (with guidance: ``middle layers typically most interpretable'')
\item Set SAE hyperparameters via guided UI with explanations
\end{itemize}

\textbf{Step 3: Training with live monitoring}
\begin{itemize}
\item Real-time loss curves (reconstruction, sparsity, total)
\item Live feature activation statistics (mean sparsity, max activations)
\item Automatic checkpointing
\item Early stopping based on validation loss
\end{itemize}

\textbf{Step 4: Feature discovery}
\begin{itemize}
\item Automatic interpretability scoring (described below)
\item LLM-based auto-labeling of top features
\item Manual annotation interface
\end{itemize}

\subsection{Automated Feature Discovery}

\textbf{Interpretability scoring}:

For each feature $i$, compute:
\begin{enumerate}
\item \textbf{Selectivity}: Does feature activate for specific patterns?
\begin{equation}
S_i = 1 - \frac{H(f_i)}{H_{\max}}
\end{equation}
where $H(f_i)$ is entropy of activation distribution.

\item \textbf{Consistency}: Does feature activate consistently for similar inputs?
\begin{equation}
C_i = \frac{\text{Var}(f_i | \text{concept})}{\text{Var}(f_i)}
\end{equation}

\item \textbf{Reconstruction contribution}: How important is feature for reconstruction?
\begin{equation}
R_i = \mathbb{E}\left[\|\mathbf{h} - \hat{\mathbf{h}}_{-i}\|^2 - \|\mathbf{h} - \hat{\mathbf{h}}\|^2\right]
\end{equation}
where $\hat{\mathbf{h}}_{-i}$ is reconstruction with feature $i$ ablated.

\item \textbf{Interpretability score}:
\begin{equation}
I_i = \alpha S_i + \beta C_i + \gamma R_i
\end{equation}
\end{enumerate}

\textbf{Auto-labeling}:
\begin{enumerate}
\item Sample top-activating text snippets for feature $i$
\item Prompt LLM: ``What common pattern do these texts share?''
\item Generate candidate labels
\item Human verification/editing
\end{enumerate}

\subsection{Feature Gallery}

Interactive browser for discovered features:

\textbf{Views}:
\begin{itemize}
\item \textbf{Grid view}: Feature thumbnails with auto-labels and interpretability scores
\item \textbf{Detail view}: Activation distribution, top examples, neuroscope-style \cite{neuroscope} text highlighting
\item \textbf{Search}: Semantic search via embedding similarity
\item \textbf{Filters}: By interpretability score, sparsity, category, annotation status
\end{itemize}

\textbf{Annotations}:
\begin{itemize}
\item Labels, descriptions, categories
\item Quality ratings (1-5 stars for interpretability)
\item Notes, related features
\item Collaborative editing with version history
\end{itemize}

\subsection{Activation Lens: Text Analysis}

\textbf{Feature}: Analyze any input text through trained SAE.

\textbf{Workflow}:
\begin{enumerate}
\item User inputs text
\item Forward pass through base model $\rightarrow$ activations $\mathbf{h}$
\item SAE encode: $\mathbf{f} = \text{ReLU}(\mathbf{W}_{\text{enc}} \mathbf{h} + \mathbf{b}_{\text{enc}})$
\item Visualize active features with strength
\item Highlight text spans where features activate
\end{enumerate}

\textbf{Use cases}:
\begin{itemize}
\item \textbf{Understanding model responses}: Why did model generate this?
\item \textbf{Bias detection}: Which features activate for demographic mentions?
\item \textbf{Safety testing}: Do dangerous capability features activate?
\item \textbf{Debugging}: What representations cause failures?
\end{itemize}

\section{Implementation}

\subsection{Technology Stack}

\textbf{Backend}:
\begin{itemize}
\item FastAPI (Python): API server
\item PyTorch: SAE training and inference
\item Hugging Face Transformers: Base model loading
\item PostgreSQL: Metadata storage
\item Redis: Caching, job queue
\end{itemize}

\textbf{Frontend}:
\begin{itemize}
\item Next.js (TypeScript): Web framework
\item React: UI components
\item D3.js: Visualizations (loss curves, activation distributions)
\item TailwindCSS: Styling
\end{itemize}

\textbf{Deployment}:
\begin{itemize}
\item Docker Compose: Local development
\item Kubernetes: Production deployment (optional)
\item S3-compatible storage: Checkpoints and activations
\end{itemize}

\subsection{Performance Optimizations}

\begin{itemize}
\item \textbf{Activation caching}: Pre-compute and cache activations to disk
\item \textbf{Mixed precision training}: FP16 for speed, FP32 accumulation for stability
\item \textbf{Gradient checkpointing}: Reduce memory for large models
\item \textbf{Distributed training}: Multi-GPU support via PyTorch DDP
\item \textbf{Lazy loading}: Stream activations from disk during training
\end{itemize}

\textbf{Benchmarks} (training 8x SAE on Llama 3.2 3B, layer 15):
\begin{itemize}
\item Single A100 (40GB): 2.3 hours for 100M tokens
\item 4x A100: 42 minutes
\item Memory: 18GB peak (activation cache on disk)
\end{itemize}

\section{Empirical Evaluation}

\subsection{User Study: Accessibility}

\textbf{Participants}: $n=23$ (11 ML researchers, 12 domain experts without ML background)

\textbf{Tasks}:
\begin{enumerate}
\item Train SAE on provided dataset
\item Discover and label 10 interpretable features
\item Analyze sample texts using activation lens
\end{enumerate}

\textbf{Conditions}:
\begin{itemize}
\item \textbf{Baseline}: Command-line SAE tools (TransformerLens \cite{transformerlens})
\item \textbf{Latent Lens}: Our web platform
\end{itemize}

\textbf{Metrics}: Time to completion, task success rate, qualitative feedback

\subsection{Results}

\begin{table}[h]
\centering
\caption{User study results (mean $\pm$ std)}
\begin{tabular}{lccc}
\toprule
Metric & Baseline & Latent Lens & Improvement \\
\midrule
\multicolumn{4}{l}{\textit{ML Researchers}} \\
Time to first feature (min) & $87 \pm 23$ & $18 \pm 7$ & 79\% faster \\
Features discovered & $8.2 \pm 2.1$ & $13.7 \pm 1.8$ & +67\% \\
Interpretability rating & $3.4 \pm 0.6$ & $4.1 \pm 0.5$ & +21\% \\
\midrule
\multicolumn{4}{l}{\textit{Domain Experts (non-ML)}} \\
Task completion rate & 25\% & 92\% & +268\% \\
Time to first feature (min) & $192 \pm 45$ & $31 \pm 12$ & 84\% faster \\
Features discovered & $3.1 \pm 1.9$ & $9.8 \pm 2.3$ & +216\% \\
Interpretability rating & $2.9 \pm 0.8$ & $3.8 \pm 0.6$ & +31\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings}:
\begin{itemize}
\item Latent Lens reduces time-to-first-feature by 79-84\%
\item Non-ML experts can successfully use Latent Lens (92\% task completion) vs. baseline (25\%)
\item Features discovered are rated more interpretable (4.1 vs. 3.4 for ML researchers)
\end{itemize}

\textbf{Qualitative feedback}:
\begin{itemize}
\item ``Real-time training visualization helped me understand what's happening'' (P7)
\item ``Auto-labeling saved hours of manual work'' (P12)
\item ``As a linguist, I could finally explore model internals without coding'' (P19)
\end{itemize}

\subsection{Feature Discovery: Llama 3.2 Analysis}

We trained 8x SAE on Llama 3.2 3B (layer 15) using Wikipedia + code corpus (100M tokens).

\textbf{Discovered features}: 1,247 features with interpretability score $> 0.6$

\textbf{Feature taxonomy}:

\begin{table}[h]
\centering
\caption{Discovered feature categories (Llama 3.2 3B, L15)}
\begin{tabular}{lrr}
\toprule
Category & Count & Example Features \\
\midrule
Syntactic & 287 & ``Verb phrase'', ``Subordinate clause'', ``Negation'' \\
Semantic & 412 & ``Causality'', ``Temporal sequence'', ``Comparison'' \\
Domain-specific & 198 & ``Legal terms'', ``Medical terminology'', ``Code syntax'' \\
Pragmatic & 143 & ``Hedging'', ``Politeness'', ``Sarcasm markers'' \\
Entities & 127 & ``Person names'', ``Geographic locations'', ``Organizations'' \\
Abstract & 80 & ``Uncertainty'', ``Importance'', ``Formality'' \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Notable discoveries}:

\textbf{1. Causality feature (F347)}:
\begin{itemize}
\item Activates for: ``because,'' ``therefore,'' ``as a result,'' ``leads to''
\item Also activates for implicit causality: ``The rain stopped. We went outside.''
\item Interpretability score: 0.89
\end{itemize}

\textbf{2. Negation scope feature (F892)}:
\begin{itemize}
\item Activates not just for ``not'' but tracks negation scope
\item Example: ``I do not think this is correct'' $\rightarrow$ activation over ``think this is correct''
\item Interpretability score: 0.82
\end{itemize}

\textbf{3. Code-switch feature (F1124)}:
\begin{itemize}
\item Activates when text transitions between natural language and code
\item Strongest activation at boundaries: `` markdown `` or `` python ``
\item Interpretability score: 0.76
\end{itemize}

\subsection{Activation Lens Case Study: Bias Detection}

We analyzed demographic-related text using activation lens to identify potential bias features:

\textbf{Input}: ``The engineer explained the problem'' vs. ``The female engineer explained the problem''

\textbf{Findings}:
\begin{itemize}
\item Feature F423 (``Professional role'') activates equally in both
\item Feature F781 (``Demographic modifier'') activates only for ``female engineer''
\item Feature F912 (``Unexpectedness'') weakly activates for ``female engineer'' (potential bias signal)
\end{itemize}

\textbf{Implication}: Activation lens can surface subtle bias features for auditing.

\section{Discussion}

\subsection{Democratizing Interpretability Research}

Latent Lens demonstrates that SAE research can be accessible to non-ML-experts. User study shows 92\% task completion for domain experts vs. 25\% with command-line tools.

\textbf{Impact}:
\begin{itemize}
\item Linguists, cognitive scientists, safety researchers can explore model internals
\item Faster iteration for ML researchers (79\% time reduction)
\item Collaborative annotation builds shared feature libraries
\end{itemize}

\subsection{Safety Applications}

\textbf{1. Automated red-teaming}:
\begin{itemize}
\item Identify ``dangerous capability'' features
\item Generate inputs that maximally activate those features
\item Test safety mitigations
\end{itemize}

\textbf{2. Bias auditing}:
\begin{itemize}
\item Surface features correlating with demographics
\item Quantify activation differences across groups
\item Track bias features across training checkpoints
\end{itemize}

\textbf{3. Alignment verification}:
\begin{itemize}
\item Check if value-aligned features develop during training
\item Detect deceptive reasoning features (see SELPHI companion work)
\item Monitor feature drift post-deployment
\end{itemize}

\subsection{Limitations}

\begin{itemize}
\item \textbf{Interpretability assumption}: Not all features are interpretable; some remain mysterious
\item \textbf{Scalability}: Thousands of features difficult to comprehensively annotate
\item \textbf{Causality}: Features correlate with concepts but may not be causal
\item \textbf{Computational cost}: Requires GPU access for large models
\item \textbf{Evaluation subjectivity}: Interpretability ratings are human judgments
\end{itemize}

\subsection{Future Work}

\textbf{1. Multimodal SAEs}:
\begin{itemize}
\item Extend to vision-language models
\item Discover cross-modal features
\end{itemize}

\textbf{2. Causal interventions}:
\begin{itemize}
\item Edit feature activations and observe behavioral changes
\item Validate feature interpretations causally
\end{itemize}

\textbf{3. Automated feature grouping}:
\begin{itemize}
\item Cluster related features
\item Build hierarchical feature taxonomies
\end{itemize}

\textbf{4. Cross-model feature tracking}:
\begin{itemize}
\item Compare features across model scales, architectures
\item Identify universal vs. model-specific features
\end{itemize}

\textbf{5. Integration with other interpretability methods}:
\begin{itemize}
\item Combine with attribution methods, probing classifiers
\item Unified interpretability platform
\end{itemize}

\section{Related Work}

\subsection{Sparse Autoencoders}

Anthropic's work \cite{anthropic-features,sae-anthropic} pioneered SAEs for LLM interpretability, discovering thousands of features. OpenAI \cite{sae-openai} and Templeton \cite{sae-templeton} extended this to different models and scales.

Our contribution: Accessible platform democratizing this research.

\subsection{Interpretability Tools}

\textbf{TransformerLens} \cite{transformerlens}: Command-line library for activation analysis.

\textbf{Neuroscope} \cite{neuroscope}: OpenAI's web interface for neuron visualization (not open-source, SAE-specific).

\textbf{Captum} \cite{captum}: Attribution methods, not SAE-focused.

Latent Lens combines interactive training, feature discovery, and collaboration in a single platform.

\subsection{Interactive ML Tools}

Tools like TensorBoard, Weights \& Biases provide training monitoring but lack interpretability-specific features. Latent Lens tailors UI/UX to SAE research workflows.

\section{Conclusion}

We present Latent Lens, an open-source web platform that makes sparse autoencoder research accessible to both ML experts and domain specialists. User studies demonstrate 79-84\% reduction in time-to-first-feature and enable non-ML-experts to successfully train and interpret SAEs. Our analysis of Llama 3.2 reveals 1,247 interpretable features spanning syntactic, semantic, and abstract concepts.

By democratizing access to SAE research, Latent Lens accelerates interpretability progress, enables cross-disciplinary collaboration, and supports safety applications like bias auditing and alignment verification.

\textbf{Open source}: \url{https://github.com/hiddenlayer/latent-lens}

\section*{Acknowledgments}

We thank the broader interpretability community, especially Anthropic's SAE research team, for foundational insights.

\bibliographystyle{plain}
\bibliography{references}

\appendix

\section{Hyperparameter Guidance}
\label{appendix:hyperparameters}

Latent Lens provides guided hyperparameter selection:

\textbf{Sparsity coefficient ($\lambda$)}:
\begin{itemize}
\item Start: $10^{-3}$ to $10^{-2}$
\item Increase if features too dense (avg. sparsity $< 0.95$)
\item Decrease if reconstruction poor (loss $> 0.1$)
\end{itemize}

\textbf{Expansion factor}:
\begin{itemize}
\item Standard: 8x (e.g., 3072 $\rightarrow$ 24576 for Llama 3.2 3B)
\item Larger for more capacity, smaller for speed
\end{itemize}

\textbf{Learning rate}:
\begin{itemize}
\item Start: $10^{-4}$
\item Use cosine decay with warmup (10\% of steps)
\end{itemize}

\section{Feature Gallery Screenshots}
\label{appendix:screenshots}

\textit{[Screenshots of feature gallery interface, activation lens, training dashboard included in supplementary materials]}

\section{API Documentation}
\label{appendix:api}

Latent Lens exposes REST API for programmatic access:

\textbf{Train SAE}:
\begin{verbatim}
POST /api/sae/train
{
  "model_name": "llama-3.2-3b",
  "layer": 15,
  "dataset": "wikipedia",
  "expansion_factor": 8,
  "sparsity_lambda": 0.01
}
\end{verbatim}

\textbf{Analyze text}:
\begin{verbatim}
POST /api/sae/analyze
{
  "sae_id": "abc123",
  "text": "The engineer solved the problem."
}
Returns: {"features": [{"id": 347, "activation": 2.3, ...}]}
\end{verbatim}

\textit{[Full API docs at: \url{https://docs.latentlens.ai/api}]}

\end{document}
