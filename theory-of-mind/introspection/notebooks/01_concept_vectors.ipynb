{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Building Concept Vectors\n",
    "\n",
    "**Experiment:** Extract and build concept vector libraries from model activations\n",
    "\n",
    "**Date:** 2025-11-05\n",
    "\n",
    "**Research Question:** Can we extract interpretable concept representations from model activations?\n",
    "\n",
    "**Goals:**\n",
    "- Extract activation patterns for specific concepts (emotions, topics, etc.)\n",
    "- Build concept libraries\n",
    "- Store concepts for reuse across projects\n",
    "- Visualize concept space geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add parent directory to path for code imports\nimport sys\nsys.path.append('../code')\n\n# 2. Import numerical and visualization libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# 3. Import introspection modules\nfrom theory_of_mind.introspection import (\n    ActivationSteerer,     # Capture and steer activations\n    ConceptLibrary,        # Store concept vectors\n    build_emotion_library, # Build pre-defined emotion concepts\n)\n\n# 4. Import harness experiment tracking\nfrom harness import ExperimentConfig, get_tracker\n\n# 5. Confirm setup\nprint(\"Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Model\n",
    "\n",
    "Load a local model for activation extraction (MLX or Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for MLX support\n",
    "try:\n",
    "    from mlx_lm import load\n",
    "    \n",
    "    model_name = \"mlx-community/Llama-3.2-3B-Instruct-4bit\"\n",
    "    model, tokenizer = load(model_name)\n",
    "    \n",
    "    print(f\"Loaded model: {model_name}\")\n",
    "    print(f\"Model layers: {len(model.model.layers)}\")\n",
    "    use_mlx = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"MLX not available. Using Ollama instead.\")\n",
    "    print(\"Note: Activation steering requires local model access (MLX).\")\n",
    "    use_mlx = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Create Activation Steerer\n",
    "\n",
    "Initialize steerer for capturing and manipulating activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_mlx:\n",
    "    steerer = ActivationSteerer(model, tokenizer)\n",
    "    \n",
    "    # Test activation capture\n",
    "    test_prompt = \"I am feeling happy today.\"\n",
    "    activations = steerer.capture_activations(test_prompt, layer_idx=15)\n",
    "    \n",
    "    print(f\"Captured activations shape: {activations.shape}\")\n",
    "    print(f\"Activation statistics:\")\n",
    "    print(f\"  Mean: {np.mean(activations):.4f}\")\n",
    "    print(f\"  Std: {np.std(activations):.4f}\")\n",
    "    print(f\"  Min: {np.min(activations):.4f}\")\n",
    "    print(f\"  Max: {np.max(activations):.4f}\")\n",
    "else:\n",
    "    print(\"Skipping (MLX required for activation steering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Build Emotion Concept Library\n",
    "\n",
    "Extract concept vectors for basic emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_mlx:\n",
    "    # Define emotion prompts\n",
    "    emotion_prompts = {\n",
    "        \"happiness\": [\n",
    "            \"I am feeling extremely happy and joyful!\",\n",
    "            \"This is the best day ever, I'm so excited!\",\n",
    "            \"I'm filled with joy and happiness.\",\n",
    "        ],\n",
    "        \"sadness\": [\n",
    "            \"I feel deeply sad and melancholic.\",\n",
    "            \"Everything feels heavy and sorrowful today.\",\n",
    "            \"I'm overwhelmed with sadness and grief.\",\n",
    "        ],\n",
    "        \"anger\": [\n",
    "            \"I am furious and enraged!\",\n",
    "            \"This makes me so angry and frustrated!\",\n",
    "            \"I'm filled with intense anger.\",\n",
    "        ],\n",
    "        \"fear\": [\n",
    "            \"I'm terrified and scared.\",\n",
    "            \"This is frightening and makes me anxious.\",\n",
    "            \"I feel deep fear and worry.\",\n",
    "        ],\n",
    "        \"surprise\": [\n",
    "            \"Wow, I'm completely surprised!\",\n",
    "            \"This is so unexpected and shocking!\",\n",
    "            \"I'm amazed and surprised by this.\",\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    # Extract concept vectors\n",
    "    library = ConceptLibrary()\n",
    "    target_layer = 15  # Middle layer typically captures semantic info\n",
    "    \n",
    "    for emotion, prompts in emotion_prompts.items():\n",
    "        print(f\"Extracting concept vector for: {emotion}\")\n",
    "        \n",
    "        # Average activations across prompts\n",
    "        activations_list = []\n",
    "        for prompt in prompts:\n",
    "            acts = steerer.capture_activations(prompt, layer_idx=target_layer)\n",
    "            activations_list.append(acts)\n",
    "        \n",
    "        # Average across prompts and sequence positions\n",
    "        avg_activations = np.mean(activations_list, axis=0)\n",
    "        concept_vector = np.mean(avg_activations, axis=0)  # Average over sequence\n",
    "        \n",
    "        library.add_concept(emotion, concept_vector, layer=target_layer)\n",
    "        print(f\"  Vector shape: {concept_vector.shape}\")\n",
    "    \n",
    "    print(f\"\\nBuilt library with {len(library.concepts)} concepts\")\n",
    "    print(f\"Concepts: {list(library.concepts.keys())}\")\n",
    "else:\n",
    "    print(\"Skipping (MLX required)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Visualize Concept Space\n",
    "\n",
    "Use dimensionality reduction to visualize concept relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_mlx:\n",
    "    # Get all concept vectors\n",
    "    concept_names = list(library.concepts.keys())\n",
    "    concept_vectors = [library.get_concept(name) for name in concept_names]\n",
    "    concept_matrix = np.array(concept_vectors)\n",
    "    \n",
    "    # PCA projection to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    coords_2d = pca.fit_transform(concept_matrix)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(coords_2d[:, 0], coords_2d[:, 1], s=100)\n",
    "    \n",
    "    for i, name in enumerate(concept_names):\n",
    "        plt.annotate(\n",
    "            name,\n",
    "            (coords_2d[i, 0], coords_2d[i, 1]),\n",
    "            fontsize=12,\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('Emotion Concept Space (PCA)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Explained variance: {pca.explained_variance_ratio_}\")\n",
    "else:\n",
    "    print(\"Skipping (MLX required)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Compute Concept Similarities\n",
    "\n",
    "Measure cosine similarity between concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_mlx:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(concept_matrix)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(similarity_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.colorbar(label='Cosine Similarity')\n",
    "    plt.xticks(range(len(concept_names)), concept_names, rotation=45)\n",
    "    plt.yticks(range(len(concept_names)), concept_names)\n",
    "    plt.title('Concept Similarity Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print most/least similar pairs\n",
    "    print(\"\\nMost similar pairs:\")\n",
    "    for i in range(len(concept_names)):\n",
    "        for j in range(i+1, len(concept_names)):\n",
    "            sim = similarity_matrix[i, j]\n",
    "            print(f\"  {concept_names[i]} <-> {concept_names[j]}: {sim:.3f}\")\n",
    "else:\n",
    "    print(\"Skipping (MLX required)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Save Concept Library\n",
    "\n",
    "Store concept vectors for use in other projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_mlx:\n",
    "    # Save to shared concepts directory\n",
    "    save_path_pkl = \"/home/user/hidden-layer/shared/concepts/emotions_layer15.pkl\"\n",
    "    save_path_json = \"/home/user/hidden-layer/shared/concepts/emotions_layer15.json\"\n",
    "    \n",
    "    library.save(save_path_pkl)\n",
    "    library.export_json(save_path_json)\n",
    "    \n",
    "    print(f\"Saved library to:\")\n",
    "    print(f\"  {save_path_pkl}\")\n",
    "    print(f\"  {save_path_json}\")\n",
    "    \n",
    "    # Test loading\n",
    "    loaded_library = ConceptLibrary.load(save_path_pkl)\n",
    "    print(f\"\\nSuccessfully loaded library with {len(loaded_library.concepts)} concepts\")\n",
    "else:\n",
    "    print(\"Skipping (MLX required)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Track Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentConfig(\n",
    "    experiment_name=\"concept_vectors_emotions\",\n",
    "    task_type=\"concept_extraction\",\n",
    "    strategy=\"activation_capture\",\n",
    "    provider=\"mlx\" if use_mlx else \"none\",\n",
    "    model=model_name if use_mlx else \"none\",\n",
    ")\n",
    "\n",
    "tracker = get_tracker()\n",
    "run_dir = tracker.start_experiment(config)\n",
    "\n",
    "if use_mlx:\n",
    "    from harness import ExperimentResult\n",
    "    \n",
    "    result = ExperimentResult(\n",
    "        config=config,\n",
    "        task_input=\"Extract emotion concept vectors\",\n",
    "        output=f\"Created library with {len(library.concepts)} concepts\",\n",
    "        eval_scores={},\n",
    "        eval_metadata={\n",
    "            \"concepts\": list(library.concepts.keys()),\n",
    "            \"target_layer\": target_layer,\n",
    "            \"vector_dim\": concept_vector.shape[0],\n",
    "            \"saved_to\": save_path_pkl,\n",
    "        },\n",
    "        success=True,\n",
    "    )\n",
    "    tracker.log_result(result)\n",
    "\n",
    "summary = tracker.finish_experiment()\n",
    "print(f\"Experiment logged in: {run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Activation Steering** (02_activation_steering.ipynb): Apply concept vectors to steer model behavior\n",
    "2. **Introspection Tasks** (03_introspection_eval.ipynb): Test if models can report their internal states\n",
    "3. **API Introspection** (04_api_introspection.ipynb): Test frontier models\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "- Concept vectors capture semantic patterns in activation space\n",
    "- Different layers may encode different levels of abstraction\n",
    "- Concept similarity reflects semantic relationships\n",
    "- Shared concept libraries enable cross-project research"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}