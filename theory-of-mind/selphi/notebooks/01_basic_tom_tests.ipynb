{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Theory of Mind Tests\n",
    "\n",
    "This notebook demonstrates how to test theory of mind (ToM) and epistemology understanding in language models using the selphi toolkit.\n",
    "\n",
    "## What is Theory of Mind?\n",
    "\n",
    "Theory of Mind is the ability to attribute mental states (beliefs, intents, desires, knowledge) to oneself and others. In language models, we're interested in:\n",
    "\n",
    "1. **False Belief Understanding**: Can the model track that different agents have different (possibly incorrect) beliefs?\n",
    "2. **Knowledge Attribution**: Does the model understand who knows what based on observation?\n",
    "3. **Perspective Taking**: Can the model reason from different viewpoints?\n",
    "4. **Belief Updating**: Does the model understand how beliefs change with new information?\n",
    "5. **Second-Order Beliefs**: Can the model reason about beliefs about beliefs?\n",
    "6. **Epistemic States**: Can the model distinguish knowing from believing from guessing?\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add repository root to path to import harness\nimport sys\nsys.path.append('../../../')  # Go up to repo root from notebooks/\n\n# 2. Import SELPHI components for Theory of Mind testing\nfrom selphi import (\n    # Pre-defined ToM scenarios\n    SALLY_ANNE,          # Classic false belief test\n    CHOCOLATE_BAR,       # Chocolate bar false belief variant\n    SURPRISE_PARTY,      # Surprise party scenario\n    BROKEN_VASE,         # Broken vase scenario\n    MOVIE_OPINIONS,      # Movie opinions (perspective-taking)\n    WEATHER_UPDATE,      # Weather update (belief updating)\n    GIFT_SURPRISE,       # Gift surprise (second-order belief)\n    COIN_FLIP,           # Coin flip (epistemic state)\n    DOOR_LOCKED,         # Door locked (knowledge attribution)\n    \n    # Core functions\n    run_scenario,               # Run single ToM scenario\n    run_multiple_scenarios,     # Run multiple scenarios\n    run_all_scenarios,          # Run all pre-defined scenarios\n    evaluate_scenario,          # Evaluate model response\n    evaluate_batch,             # Evaluate multiple responses\n    compare_models_on_scenarios,  # Compare different models\n    results_to_dict_list,       # Convert results to dict format\n    \n    # Types and collections\n    ToMType,              # Enum of ToM types (FALSE_BELIEF, KNOWLEDGE_ATTRIBUTION, etc.)\n    ALL_SCENARIOS,        # All pre-defined scenarios\n    get_scenarios_by_type,       # Filter by ToM type\n    get_scenarios_by_difficulty,  # Filter by difficulty\n)\nfrom harness.defaults import DEFAULT_MODEL, DEFAULT_PROVIDER\n\n# 3. Import JSON and pprint for data display\nimport json\nfrom pprint import pprint\n\n# 4. Show configuration\nprint(\"=\"*70)\nprint(\"üîß SELPHI NOTEBOOK CONFIGURATION\")\nprint(\"=\"*70)\nprint(f\"üìç Provider: {DEFAULT_PROVIDER}\")\nprint(f\"ü§ñ Model: {DEFAULT_MODEL or '(default for provider)'}\")\nprint(\"=\"*70)\nprint(\"\\nüí° TO CHANGE: Add a cell with:\")\nprint(\"   PROVIDER = 'mlx'  # or 'ollama', 'anthropic', 'openai'\")\nprint(\"   MODEL = 'your-model'\")\nprint(\"   Then pass to: run_scenario(..., provider=PROVIDER, model=MODEL)\")\nprint(\"=\"*70 + \"\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Classic Sally-Anne Test\n",
    "\n",
    "The Sally-Anne test is the most famous false belief task. Let's see how a model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Print the Sally-Anne scenario as a prompt\nprint(\"SCENARIO:\")\nprint(SALLY_ANNE.to_prompt())  # Converts scenario to text prompt\n\n# 2. Print separator\nprint(\"\\n\" + \"=\"*60)\n\n# 3. Print the correct answers (what we expect from a model with ToM)\nprint(\"\\nCORRECT ANSWERS:\")\nfor i, answer in enumerate(SALLY_ANNE.correct_answers, 1):\n    print(f\"{i}. {answer}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Run the Sally-Anne scenario with a model\n# Change provider/model as needed\nresult = run_scenario(\n    SALLY_ANNE,        # Scenario to test\n    provider=\"ollama\",  # Provider to use\n    model=None,        # Use default model\n    temperature=0.1    # Low temperature for consistent reasoning\n)\n\n# 2. Print the model's response\nprint(\"MODEL RESPONSE:\")\nprint(result.model_response)\n\n# 3. Print separator and performance metrics\nprint(\"\\n\" + \"=\"*60)\nprint(f\"\\nLatency: {result.latency_s:.2f}s\")\nprint(f\"Tokens: {result.tokens_in} in, {result.tokens_out} out\")\nprint(f\"Cost: ${result.cost_usd:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Evaluate the model's response against correct answers\neval_result = evaluate_scenario(\n    SALLY_ANNE,                # Scenario with correct answers\n    result.model_response,     # Model's response to evaluate\n    method=\"semantic\"          # Use semantic similarity (can also use \"llm_judge\")\n)\n\n# 2. Print evaluation summary\nprint(\"EVALUATION RESULTS:\")\nprint(f\"Average Score: {eval_result['average_score']:.2f}\")\n\n# 3. Print scores for each individual question\nprint(\"\\nIndividual Question Scores:\")\nfor i, score in enumerate(eval_result['individual_scores'], 1):\n    print(f\"  Question {i}: {score:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Run Multiple Scenarios\n",
    "\n",
    "Let's test the model on several scenarios of varying difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all easy scenarios\n",
    "easy_scenarios = get_scenarios_by_difficulty(\"easy\")\n",
    "\n",
    "print(f\"Running {len(easy_scenarios)} easy scenarios...\")\n",
    "results = run_multiple_scenarios(\n",
    "    easy_scenarios,\n",
    "    provider=\"ollama\",\n",
    "    model=None,\n",
    "    verbose=True,\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all results\n",
    "batch_eval = evaluate_batch(\n",
    "    results_to_dict_list(results),\n",
    "    method=\"semantic\"\n",
    ")\n",
    "\n",
    "print(\"BATCH EVALUATION RESULTS:\")\n",
    "print(f\"Overall Average Score: {batch_eval['overall_average']:.2f}\")\n",
    "print(\"\\nScores by Scenario Type:\")\n",
    "for tom_type, score in batch_eval['by_type'].items():\n",
    "    print(f\"  {tom_type}: {score:.2f}\")\n",
    "print(\"\\nScores by Difficulty:\")\n",
    "for difficulty, score in batch_eval['by_difficulty'].items():\n",
    "    print(f\"  {difficulty}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Compare Different Models\n",
    "\n",
    "Compare how different models perform on the same scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models_to_compare = [\n",
    "    {\n",
    "        'name': 'ollama-default',\n",
    "        'provider': 'ollama',\n",
    "        'model': None,\n",
    "        'kwargs': {'temperature': 0.1}\n",
    "    },\n",
    "    # Add more models as needed\n",
    "    # {\n",
    "    #     'name': 'gpt-4o-mini',\n",
    "    #     'provider': 'openai',\n",
    "    #     'model': 'gpt-4o-mini',\n",
    "    #     'kwargs': {'temperature': 0.1}\n",
    "    # },\n",
    "]\n",
    "\n",
    "# Use just false belief scenarios for comparison\n",
    "false_belief_scenarios = get_scenarios_by_type(ToMType.FALSE_BELIEF)\n",
    "\n",
    "print(f\"Comparing {len(models_to_compare)} models on {len(false_belief_scenarios)} scenarios...\")\n",
    "comparison_results = compare_models_on_scenarios(\n",
    "    false_belief_scenarios,\n",
    "    models_to_compare,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to format for comparison evaluation\n",
    "model_results_for_eval = {\n",
    "    model_name: results_to_dict_list(results)\n",
    "    for model_name, results in comparison_results.items()\n",
    "}\n",
    "\n",
    "# Compare\n",
    "from selphi import compare_models\n",
    "comparison_eval = compare_models(model_results_for_eval, method=\"semantic\")\n",
    "\n",
    "print(\"MODEL COMPARISON:\")\n",
    "print(\"\\nOverall Scores:\")\n",
    "for model, score in comparison_eval['overall_scores'].items():\n",
    "    print(f\"  {model}: {score:.2f}\")\n",
    "\n",
    "print(\"\\nScores by Difficulty:\")\n",
    "for difficulty, model_scores in comparison_eval['by_difficulty'].items():\n",
    "    print(f\"  {difficulty}:\")\n",
    "    for model, score in model_scores.items():\n",
    "        print(f\"    {model}: {score:.2f}\")\n",
    "\n",
    "print(\"\\nScores by ToM Type:\")\n",
    "for tom_type, model_scores in comparison_eval['by_type'].items():\n",
    "    print(f\"  {tom_type}:\")\n",
    "    for model, score in model_scores.items():\n",
    "        print(f\"    {model}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: LLM-as-Judge Evaluation\n",
    "\n",
    "For more nuanced evaluation, we can use another LLM as a judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a complex scenario\n",
    "result = run_scenario(\n",
    "    GIFT_SURPRISE,  # Second-order belief scenario\n",
    "    provider=\"ollama\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"SCENARIO:\")\n",
    "print(GIFT_SURPRISE.to_prompt())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nMODEL RESPONSE:\")\n",
    "print(result.model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with LLM judge\n",
    "judge_eval = evaluate_scenario(\n",
    "    GIFT_SURPRISE,\n",
    "    result.model_response,\n",
    "    method=\"llm_judge\",\n",
    "    judge_provider=\"ollama\",\n",
    "    judge_model=None  # Can specify a different model for judging\n",
    ")\n",
    "\n",
    "print(\"LLM JUDGE EVALUATION:\")\n",
    "print(f\"\\nNormalized Score: {judge_eval['normalized_score']:.2f}\")\n",
    "print(f\"Average Score: {judge_eval['average_score']:.1f}/10\")\n",
    "\n",
    "print(\"\\nIndividual Question Scores:\")\n",
    "for i, (score, reasoning) in enumerate(zip(judge_eval['individual_scores'], judge_eval['individual_reasonings']), 1):\n",
    "    print(f\"\\nQuestion {i}: {score}/10\")\n",
    "    print(f\"Reasoning: {reasoning}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nOVERALL ASSESSMENT:\")\n",
    "print(judge_eval['overall_assessment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Test All Scenarios\n",
    "\n",
    "Run a comprehensive test across all ToM scenario types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all scenarios\n",
    "print(f\"Running all {len(ALL_SCENARIOS)} scenarios...\")\n",
    "all_results = run_all_scenarios(\n",
    "    provider=\"ollama\",\n",
    "    temperature=0.2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation\n",
    "comprehensive_eval = evaluate_batch(\n",
    "    results_to_dict_list(all_results),\n",
    "    method=\"semantic\"\n",
    ")\n",
    "\n",
    "print(\"COMPREHENSIVE EVALUATION:\")\n",
    "print(f\"\\nTotal Scenarios Tested: {comprehensive_eval['total_scenarios']}\")\n",
    "print(f\"Overall Average Score: {comprehensive_eval['overall_average']:.2f}\")\n",
    "\n",
    "print(\"\\nScores by ToM Type:\")\n",
    "for tom_type, score in sorted(comprehensive_eval['by_type'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {tom_type}: {score:.2f}\")\n",
    "\n",
    "print(\"\\nScores by Difficulty:\")\n",
    "for difficulty in ['easy', 'medium', 'hard']:\n",
    "    if difficulty in comprehensive_eval['by_difficulty']:\n",
    "        score = comprehensive_eval['by_difficulty'][difficulty]\n",
    "        print(f\"  {difficulty}: {score:.2f}\")\n",
    "\n",
    "# Detailed breakdown\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nDETAILED BREAKDOWN:\")\n",
    "for eval_result in comprehensive_eval['evaluations']:\n",
    "    print(f\"\\n{eval_result['scenario_name']} ({eval_result['scenario_type']}, {eval_result['difficulty']}):\")\n",
    "    print(f\"  Score: {eval_result['average_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration: Create Your Own Scenarios\n",
    "\n",
    "You can create custom ToM scenarios to test specific hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selphi import ToMScenario, ToMType\n",
    "\n",
    "# Create a custom scenario\n",
    "custom_scenario = ToMScenario(\n",
    "    name=\"custom_test\",\n",
    "    tom_type=ToMType.FALSE_BELIEF,\n",
    "    setup=\"Your scenario setup here...\",\n",
    "    events=[\n",
    "        \"Event 1...\",\n",
    "        \"Event 2...\"\n",
    "    ],\n",
    "    test_questions=[\n",
    "        \"Question 1?\",\n",
    "        \"Question 2?\"\n",
    "    ],\n",
    "    correct_answers=[\n",
    "        \"Expected answer 1\",\n",
    "        \"Expected answer 2\"\n",
    "    ],\n",
    "    reasoning=\"Why this tests ToM...\",\n",
    "    difficulty=\"medium\"\n",
    ")\n",
    "\n",
    "# Test it\n",
    "# result = run_scenario(custom_scenario, provider=\"ollama\")\n",
    "# eval_result = evaluate_scenario(custom_scenario, result.model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Experiment with different models**: Compare local models (Ollama, MLX) with API models (Claude, GPT)\n",
    "2. **Vary temperature**: Test if higher temperature affects ToM reasoning\n",
    "3. **Test with different prompt formats**: Try chain-of-thought prompting\n",
    "4. **Create domain-specific scenarios**: Design ToM tests for your specific use case\n",
    "5. **Fine-tune models**: See if fine-tuning improves ToM capabilities\n",
    "6. **Analyze failure modes**: Study which types of ToM reasoning are hardest\n",
    "\n",
    "## Research Questions to Explore\n",
    "\n",
    "- Do larger models have better ToM capabilities?\n",
    "- Which ToM types are easiest/hardest for current models?\n",
    "- Does multi-agent debate improve ToM reasoning?\n",
    "- Can we identify specific architectural features that support ToM?\n",
    "- How does fine-tuning on ToM tasks affect other capabilities?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}