{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Theory of Mind with Established Datasets\n",
    "\n",
    "This notebook shows how to use established ToM benchmarks with SELPHI:\n",
    "\n",
    "1. **ToMBench** (ACL 2024) - 2,860 samples across 8 tasks\n",
    "2. **OpenToM** (2024) - 696 narratives with 16,008 questions\n",
    "3. **SocialIQA** (2019) - 38,000 QA pairs on social reasoning\n",
    "\n",
    "## Why Use Benchmarks?\n",
    "\n",
    "- **Standardized Evaluation**: Compare your models against published results\n",
    "- **Comprehensive Coverage**: Test across diverse ToM scenarios\n",
    "- **Reproducibility**: Use same data as other researchers\n",
    "- **Scale**: Test on thousands of scenarios\n",
    "\n",
    "##  Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Add repository root to path to import harness\nimport sys\nsys.path.append('../../../')  # Go up to repo root from notebooks/\n\n# 2. Import SELPHI benchmark loading functions\nfrom selphi import (\n    # Benchmark loaders\n    load_tombench,        # Load ToMBench dataset (ACL 2024)\n    load_opentom,         # Load OpenToM dataset\n    load_socialiqa,       # Load SocialIQA dataset\n    print_benchmark_info,  # Print info about available benchmarks\n    \n    # Task execution\n    run_scenario,          # Run single scenario\n    run_multiple_scenarios,  # Run multiple scenarios\n    \n    # Evaluation\n    evaluate_scenario,     # Evaluate single response\n    evaluate_batch,        # Evaluate multiple responses\n    results_to_dict_list,  # Convert results to dict format\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Available Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Print information about all available ToM benchmarks\n# This shows which benchmarks are available and how to install them\nprint_benchmark_info()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Load ToMBench\n",
    "\n",
    "First, clone the repository:\n",
    "```bash\n",
    "git clone https://github.com/zhchen18/ToMBench.git\n",
    "```\n",
    "\n",
    "The dataset will be at `./ToMBench/data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ToMBench\n",
    "try:\n",
    "    tombench = load_tombench()\n",
    "    \n",
    "    print(f\"Loaded {tombench.total_count} scenarios from ToMBench\")\n",
    "    print(f\"Source: {tombench.source}\")\n",
    "    print(f\"\\nMetadata: {tombench.metadata}\")\n",
    "    \n",
    "    # Show a sample scenario\n",
    "    print(f\"\\nSample scenario:\")\n",
    "    sample = tombench.scenarios[0]\n",
    "    print(f\"Name: {sample.name}\")\n",
    "    print(f\"Type: {sample.tom_type.value}\")\n",
    "    print(f\"\\nPrompt:\\n{sample.to_prompt()}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nPlease install ToMBench first:\")\n",
    "    print(\"git clone https://github.com/zhchen18/ToMBench.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model on ToMBench Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a subset of ToMBench scenarios\n",
    "try:\n",
    "    # Take first 10 scenarios for quick testing\n",
    "    test_scenarios = tombench.scenarios[:10]\n",
    "    \n",
    "    print(f\"Running {len(test_scenarios)} ToMBench scenarios...\")\n",
    "    \n",
    "    results = run_multiple_scenarios(\n",
    "        test_scenarios,\n",
    "        provider=\"ollama\",\n",
    "        temperature=0.1,  # Low temperature for consistent reasoning\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCompleted {len(results)} scenarios\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"ToMBench not loaded. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ToMBench Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results\n",
    "try:\n",
    "    batch_eval = evaluate_batch(\n",
    "        results_to_dict_list(results),\n",
    "        method=\"semantic\"\n",
    "    )\n",
    "    \n",
    "    print(\"ToMBench Evaluation Results:\")\n",
    "    print(f\"Overall Average: {batch_eval['overall_average']:.3f}\")\n",
    "    print(f\"\\nBy ToM Type:\")\n",
    "    for tom_type, score in batch_eval['by_type'].items():\n",
    "        print(f\"  {tom_type}: {score:.3f}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"No results to evaluate. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Load OpenToM\n",
    "\n",
    "First, clone the repository:\n",
    "```bash\n",
    "git clone https://github.com/seacowx/OpenToM.git\n",
    "```\n",
    "\n",
    "Or install from HuggingFace:\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"SeacowX/OpenToM\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenToM\n",
    "try:\n",
    "    opentom = load_opentom(\n",
    "        include_long=False,  # Exclude long narratives for faster testing\n",
    "        question_types=None  # Load all question types\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {opentom.total_count} scenarios from OpenToM\")\n",
    "    print(f\"Source: {opentom.source}\")\n",
    "    print(f\"\\nMetadata: {opentom.metadata}\")\n",
    "    \n",
    "    # Show a sample scenario\n",
    "    print(f\"\\nSample scenario:\")\n",
    "    sample = opentom.scenarios[0]\n",
    "    print(f\"Name: {sample.name}\")\n",
    "    print(f\"Type: {sample.tom_type.value}\")\n",
    "    print(f\"Question Type: {sample.metadata.get('question_type')}\")\n",
    "    print(f\"\\nNarrative:\\n{sample.setup[:200]}...\")\n",
    "    print(f\"\\nQuestion: {sample.test_questions[0]}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nPlease install OpenToM first:\")\n",
    "    print(\"git clone https://github.com/seacowx/OpenToM.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on OpenToM Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a small sample\n",
    "try:\n",
    "    test_scenarios = opentom.scenarios[:5]  # Just 5 for demo\n",
    "    \n",
    "    print(f\"Running {len(test_scenarios)} OpenToM scenarios...\")\n",
    "    \n",
    "    opentom_results = run_multiple_scenarios(\n",
    "        test_scenarios,\n",
    "        provider=\"ollama\",\n",
    "        temperature=0.2,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    opentom_eval = evaluate_batch(\n",
    "        results_to_dict_list(opentom_results),\n",
    "        method=\"semantic\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nOpenToM Results:\")\n",
    "    print(f\"Average Score: {opentom_eval['overall_average']:.3f}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"OpenToM not loaded. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Load SocialIQA\n",
    "\n",
    "SocialIQA is available on HuggingFace. Install the datasets library:\n",
    "```bash\n",
    "pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SocialIQA (validation split)\n",
    "try:\n",
    "    socialiqa = load_socialiqa(\n",
    "        split=\"validation\",\n",
    "        max_samples=20  # Limit for demo\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {socialiqa.total_count} scenarios from SocialIQA\")\n",
    "    print(f\"Source: {socialiqa.source}\")\n",
    "    print(f\"Full dataset size: {socialiqa.metadata['full_dataset_size']}\")\n",
    "    \n",
    "    # Show a sample\n",
    "    print(f\"\\nSample scenario:\")\n",
    "    sample = socialiqa.scenarios[0]\n",
    "    print(f\"Context: {sample.setup}\")\n",
    "    print(f\"Question: {sample.test_questions[0]}\")\n",
    "    print(f\"Choices: {sample.metadata['choices']}\")\n",
    "    print(f\"Correct Answer: {sample.correct_answers[0]}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Error: HuggingFace datasets library not installed\")\n",
    "    print(\"Install with: pip install datasets\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on SocialIQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SocialIQA scenarios\n",
    "try:\n",
    "    print(f\"Running {len(socialiqa.scenarios)} SocialIQA scenarios...\")\n",
    "    \n",
    "    socialiqa_results = run_multiple_scenarios(\n",
    "        socialiqa.scenarios,\n",
    "        provider=\"ollama\",\n",
    "        temperature=0.1,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    socialiqa_eval = evaluate_batch(\n",
    "        results_to_dict_list(socialiqa_results),\n",
    "        method=\"semantic\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSocialIQA Results:\")\n",
    "    print(f\"Average Score: {socialiqa_eval['overall_average']:.3f}\")\n",
    "    print(f\"\\nNote: GPT-4 achieves ~79% on this benchmark\")\n",
    "    print(f\"      Human baseline is ~84%\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"SocialIQA not loaded. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Compare Across Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance across benchmarks\n",
    "try:\n",
    "    print(\"Performance Comparison Across Benchmarks:\\n\")\n",
    "    print(f\"{'Benchmark':<20} {'Avg Score':<12} {'Scenarios Tested':<20}\")\n",
    "    print(\"=\"*52)\n",
    "    \n",
    "    if 'batch_eval' in locals():\n",
    "        print(f\"{'ToMBench':<20} {batch_eval['overall_average']:<12.3f} {len(results):<20}\")\n",
    "    \n",
    "    if 'opentom_eval' in locals():\n",
    "        print(f\"{'OpenToM':<20} {opentom_eval['overall_average']:<12.3f} {len(opentom_results):<20}\")\n",
    "    \n",
    "    if 'socialiqa_eval' in locals():\n",
    "        print(f\"{'SocialIQA':<20} {socialiqa_eval['overall_average']:<12.3f} {len(socialiqa_results):<20}\")\n",
    "    \n",
    "    print(\"\\nNote: These are small samples. Run on full datasets for accurate comparison.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Not all benchmarks have been run yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Full Benchmark Run (Large Scale)\n",
    "\n",
    "For a complete evaluation, you'd want to run on all scenarios. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full benchmark evaluation (commented out - can be slow!)\n",
    "# Uncomment to run full evaluation\n",
    "\n",
    "# def run_full_benchmark(benchmark_name: str, provider=\"ollama\", model=None):\n",
    "#     \"\"\"Run complete benchmark evaluation\"\"\"\n",
    "#     \n",
    "#     if benchmark_name == \"tombench\":\n",
    "#         benchmark = load_tombench()\n",
    "#     elif benchmark_name == \"opentom\":\n",
    "#         benchmark = load_opentom(include_long=True)\n",
    "#     elif benchmark_name == \"socialiqa\":\n",
    "#         benchmark = load_socialiqa(split=\"validation\", max_samples=None)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown benchmark: {benchmark_name}\")\n",
    "#     \n",
    "#     print(f\"Running full {benchmark_name} benchmark ({benchmark.total_count} scenarios)...\")\n",
    "#     print(\"This may take a while!\\n\")\n",
    "#     \n",
    "#     results = run_multiple_scenarios(\n",
    "#         benchmark.scenarios,\n",
    "#         provider=provider,\n",
    "#         model=model,\n",
    "#         temperature=0.1,\n",
    "#         verbose=True\n",
    "#     )\n",
    "#     \n",
    "#     evaluation = evaluate_batch(\n",
    "#         results_to_dict_list(results),\n",
    "#         method=\"semantic\"\n",
    "#     )\n",
    "#     \n",
    "#     return {\n",
    "#         \"benchmark\": benchmark_name,\n",
    "#         \"total_scenarios\": len(results),\n",
    "#         \"evaluation\": evaluation,\n",
    "#         \"results\": results\n",
    "#     }\n",
    "#\n",
    "# # Run full evaluation\n",
    "# full_results = run_full_benchmark(\"tombench\", provider=\"ollama\")\n",
    "# print(f\"\\nFinal Score: {full_results['evaluation']['overall_average']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Experiment Tracking\n",
    "\n",
    "Track benchmark runs with the harness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harness import get_tracker, ExperimentConfig, ExperimentResult\n",
    "\n",
    "# Example: Track benchmark evaluation\n",
    "def track_benchmark_run(benchmark_name, scenarios, provider=\"ollama\"):\n",
    "    \"\"\"Run and track a benchmark evaluation\"\"\"\n",
    "    \n",
    "    # Start tracking\n",
    "    tracker = get_tracker()\n",
    "    experiment_dir = tracker.start_experiment(ExperimentConfig(\n",
    "        experiment_name=f\"benchmark_{benchmark_name}\",\n",
    "        strategy=\"single\",\n",
    "        provider=provider,\n",
    "        metadata={\n",
    "            \"project\": \"selphi\",\n",
    "            \"benchmark\": benchmark_name,\n",
    "            \"scenario_count\": len(scenarios)\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "    # Run scenarios\n",
    "    results = run_multiple_scenarios(scenarios, provider=provider, verbose=True)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluation = evaluate_batch(results_to_dict_list(results), method=\"semantic\")\n",
    "    \n",
    "    # Log each result\n",
    "    for result, eval_item in zip(results, evaluation['evaluations']):\n",
    "        tracker.log_result(ExperimentResult(\n",
    "            task_input=result.metadata['prompt'],\n",
    "            output=result.model_response,\n",
    "            strategy_name=\"single\",\n",
    "            latency_s=result.latency_s,\n",
    "            tokens_in=result.tokens_in,\n",
    "            tokens_out=result.tokens_out,\n",
    "            cost_usd=result.cost_usd,\n",
    "            metadata={\n",
    "                \"scenario\": result.scenario_name,\n",
    "                \"score\": eval_item['average_score'],\n",
    "                \"benchmark\": benchmark_name\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    # Finish tracking\n",
    "    summary = tracker.finish_experiment()\n",
    "    \n",
    "    print(f\"\\nResults saved to: {experiment_dir}\")\n",
    "    print(f\"Overall Score: {evaluation['overall_average']:.3f}\")\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# Example usage:\n",
    "# evaluation = track_benchmark_run(\"tombench\", tombench.scenarios[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Run Full Benchmarks**: Evaluate on complete datasets for accurate metrics\n",
    "2. **Compare Models**: Test different models on same benchmarks\n",
    "3. **Analyze Failures**: Study which ToM types are hardest\n",
    "4. **Fine-Tune**: Use insights to improve model performance\n",
    "5. **Publish Results**: Share findings with research community\n",
    "\n",
    "## References\n",
    "\n",
    "- ToMBench: Chen et al., ACL 2024 - https://github.com/zhchen18/ToMBench\n",
    "- OpenToM: 2024 - https://github.com/seacowx/OpenToM\n",
    "- SocialIQA: Sap et al., 2019 - https://huggingface.co/datasets/allenai/social_i_qa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}