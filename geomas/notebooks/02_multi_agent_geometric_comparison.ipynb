{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Multi-Agent Geometric Comparison\n",
    "\n",
    "**Goal**: Compare geometric structures across reasoning strategies\n",
    "\n",
    "This notebook:\n",
    "1. Runs the same task through multiple strategies (single, debate, manager-worker)\n",
    "2. Extracts geometric structures from each\n",
    "3. Compares how multi-agent changes geometric properties\n",
    "4. Tests hypothesis: multi-agent helps when single-model geometry is poor\n",
    "\n",
    "**Core Research Question**: Does debate/decomposition create different geometric structures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append('../..')\n",
    "sys.path.append('..')\n",
    "\n",
    "from geomas.code.geometric_probes import GeometricProbe\n",
    "from geomas.code.multi_agent_analyzer import MultiAgentGeometricAnalyzer\n",
    "from geomas.code.tasks import generate_path_finding_task, DifficultyLevel\n",
    "\n",
    "try:\n",
    "    from harness import run_strategy\n",
    "    HARNESS_AVAILABLE = True\n",
    "    print(\"✓ Harness available\")\n",
    "except ImportError:\n",
    "    HARNESS_AVAILABLE = False\n",
    "    print(\"⚠ Harness not available - using simulated data\")\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL = \"llama3.2:latest\"\n",
    "PROVIDER = \"ollama\"\n",
    "\n",
    "# Strategies to compare\n",
    "STRATEGIES = [\"single\", \"debate\"]  # Add \"manager_worker\" later\n",
    "\n",
    "# Task configuration\n",
    "TASK_DIFFICULTY = DifficultyLevel.EASY\n",
    "\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Provider: {PROVIDER}\")\n",
    "print(f\"Strategies: {', '.join(STRATEGIES)}\")\n",
    "print(f\"Task difficulty: {TASK_DIFFICULTY.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Test Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a reasoning task\n",
    "task = generate_path_finding_task(difficulty=TASK_DIFFICULTY)\n",
    "\n",
    "print(\"Test Task:\")\n",
    "print(\"=\" * 60)\n",
    "print(task['prompt'][:500] + \"...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCorrect answer: {task['correct_answer']}\")\n",
    "print(f\"Hops required: {task['n_hops']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run Single-Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HARNESS_AVAILABLE:\n",
    "    print(\"Running SINGLE model strategy...\\n\")\n",
    "    \n",
    "    single_result = run_strategy(\n",
    "        \"single\",\n",
    "        task_input=task['prompt'],\n",
    "        provider=PROVIDER,\n",
    "        model=MODEL,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    print(\"Single Model Response:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(single_result.output)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Check correctness\n",
    "    correct = task['correct_answer'].lower() in single_result.output.lower()\n",
    "    print(f\"\\nResult: {'✓ CORRECT' if correct else '✗ WRONG'}\")\n",
    "    print(f\"Latency: {single_result.latency_s:.2f}s\")\n",
    "    \n",
    "    # TODO: Extract hidden states\n",
    "    # For now, simulate\n",
    "    from sklearn.datasets import make_blobs\n",
    "    \n",
    "    # Simulate different quality based on correctness\n",
    "    if correct:\n",
    "        cluster_std = 0.3  # Good separation\n",
    "    else:\n",
    "        cluster_std = 1.5  # Poor separation\n",
    "    \n",
    "    single_hidden_states, single_labels = make_blobs(\n",
    "        n_samples=100,\n",
    "        n_features=128,\n",
    "        centers=5,\n",
    "        cluster_std=cluster_std,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\n⚠ Using simulated hidden states (real extraction pending)\")\n",
    "else:\n",
    "    print(\"Simulating single model run...\")\n",
    "    # Use simulated data\n",
    "    from sklearn.datasets import make_blobs\n",
    "    single_hidden_states, single_labels = make_blobs(\n",
    "        n_samples=100, n_features=128, centers=5, cluster_std=0.8, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Single-Model Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = GeometricProbe(model=MODEL, provider=PROVIDER)\n",
    "single_analysis = probe.analyze(single_hidden_states, labels=single_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SINGLE MODEL GEOMETRIC ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSpectral Gap:        {single_analysis.spectral_gap:.4f}\")\n",
    "print(f\"Cluster Coherence:   {single_analysis.cluster_coherence:.4f}\")\n",
    "print(f\"Quality Score:       {single_analysis.quality_score:.4f}\")\n",
    "print(f\"Global Structure:    {single_analysis.global_structure_score:.4f}\")\n",
    "\n",
    "# Prediction for multi-agent benefit\n",
    "if single_analysis.quality_score < 0.5:\n",
    "    prediction = \"HIGH - Poor geometric structure suggests multi-agent will help\"\n",
    "    expected_improvement = \"Significant\"\n",
    "elif single_analysis.quality_score < 0.7:\n",
    "    prediction = \"MEDIUM - Moderate structure, multi-agent may help\"\n",
    "    expected_improvement = \"Moderate\"\n",
    "else:\n",
    "    prediction = \"LOW - Strong structure, multi-agent may not be needed\"\n",
    "    expected_improvement = \"Minimal\"\n",
    "\n",
    "print(f\"\\nPredicted Multi-Agent Benefit: {prediction}\")\n",
    "print(f\"Expected Geometric Improvement: {expected_improvement}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Debate Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HARNESS_AVAILABLE:\n",
    "    print(\"Running DEBATE strategy...\\n\")\n",
    "    \n",
    "    debate_result = run_strategy(\n",
    "        \"debate\",\n",
    "        task_input=task['prompt'],\n",
    "        provider=PROVIDER,\n",
    "        model=MODEL,\n",
    "        n_debaters=3,\n",
    "        n_rounds=2,\n",
    "        temperature=0.7  # Slightly higher for diversity\n",
    "    )\n",
    "    \n",
    "    print(\"Debate Result:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(debate_result.output)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Check correctness\n",
    "    debate_correct = task['correct_answer'].lower() in debate_result.output.lower()\n",
    "    print(f\"\\nResult: {'✓ CORRECT' if debate_correct else '✗ WRONG'}\")\n",
    "    print(f\"Latency: {debate_result.latency_s:.2f}s\")\n",
    "    print(f\"Cost multiplier vs single: {debate_result.latency_s / single_result.latency_s:.1f}x\")\n",
    "    \n",
    "    # Simulate improved geometry after debate\n",
    "    # Hypothesis: debate refines geometric structure\n",
    "    debate_hidden_states, debate_labels = make_blobs(\n",
    "        n_samples=100,\n",
    "        n_features=128,\n",
    "        centers=5,\n",
    "        cluster_std=0.25,  # Better separation than single\n",
    "        random_state=43\n",
    "    )\n",
    "    \n",
    "    print(\"\\n⚠ Using simulated hidden states (real extraction pending)\")\n",
    "else:\n",
    "    print(\"Simulating debate run...\")\n",
    "    debate_hidden_states, debate_labels = make_blobs(\n",
    "        n_samples=100, n_features=128, centers=5, cluster_std=0.4, random_state=43\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Debate Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_analysis = probe.analyze(debate_hidden_states, labels=debate_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEBATE GEOMETRIC ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSpectral Gap:        {debate_analysis.spectral_gap:.4f}\")\n",
    "print(f\"Cluster Coherence:   {debate_analysis.cluster_coherence:.4f}\")\n",
    "print(f\"Quality Score:       {debate_analysis.quality_score:.4f}\")\n",
    "print(f\"Global Structure:    {debate_analysis.global_structure_score:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compare Geometric Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute improvements\n",
    "quality_improvement = debate_analysis.quality_score - single_analysis.quality_score\n",
    "spectral_improvement = debate_analysis.spectral_gap - single_analysis.spectral_gap\n",
    "coherence_improvement = debate_analysis.cluster_coherence - single_analysis.cluster_coherence\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GEOMETRIC COMPARISON: SINGLE vs DEBATE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Single':<12} {'Debate':<12} {'Δ Improvement'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Quality Score':<25} {single_analysis.quality_score:<12.4f} \"\n",
    "      f\"{debate_analysis.quality_score:<12.4f} {quality_improvement:+.4f}\")\n",
    "print(f\"{'Spectral Gap':<25} {single_analysis.spectral_gap:<12.4f} \"\n",
    "      f\"{debate_analysis.spectral_gap:<12.4f} {spectral_improvement:+.4f}\")\n",
    "print(f\"{'Cluster Coherence':<25} {single_analysis.cluster_coherence:<12.4f} \"\n",
    "      f\"{debate_analysis.cluster_coherence:<12.4f} {coherence_improvement:+.4f}\")\n",
    "print(f\"{'Global Structure':<25} {single_analysis.global_structure_score:<12.4f} \"\n",
    "      f\"{debate_analysis.global_structure_score:<12.4f} \"\n",
    "      f\"{debate_analysis.global_structure_score - single_analysis.global_structure_score:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Interpretation\n",
    "if quality_improvement > 0.1:\n",
    "    print(\"✓ SIGNIFICANT geometric improvement from debate\")\n",
    "    print(\"  Hypothesis SUPPORTED: Debate refines geometric structure\")\n",
    "elif quality_improvement > 0:\n",
    "    print(\"○ MODERATE geometric improvement\")\n",
    "    print(\"  Debate provides some geometric refinement\")\n",
    "else:\n",
    "    print(\"✗ NO geometric improvement (or degradation)\")\n",
    "    print(\"  Single model already had strong geometry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Row 1: Single model\n",
    "# Eigenvalue spectrum\n",
    "axes[0, 0].plot(single_analysis.eigenvalues[:15], 'o-', color='blue', linewidth=2)\n",
    "axes[0, 0].set_title('Single: Eigenvalue Spectrum')\n",
    "axes[0, 0].set_xlabel('Index')\n",
    "axes[0, 0].set_ylabel('Eigenvalue')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fiedler vector\n",
    "axes[0, 1].scatter(range(len(single_labels)), single_analysis.fiedler_vector,\n",
    "                   c=single_labels, cmap='tab10', s=30, alpha=0.7)\n",
    "axes[0, 1].set_title('Single: Fiedler Vector')\n",
    "axes[0, 1].set_xlabel('Sample')\n",
    "axes[0, 1].set_ylabel('Fiedler Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 2D projection\n",
    "pca = PCA(n_components=2)\n",
    "single_2d = pca.fit_transform(single_hidden_states)\n",
    "axes[0, 2].scatter(single_2d[:, 0], single_2d[:, 1],\n",
    "                   c=single_labels, cmap='tab10', s=50, alpha=0.7, edgecolors='black', linewidths=0.5)\n",
    "axes[0, 2].set_title(f'Single: 2D Projection (Q={single_analysis.quality_score:.3f})')\n",
    "axes[0, 2].set_xlabel('PC1')\n",
    "axes[0, 2].set_ylabel('PC2')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Debate\n",
    "axes[1, 0].plot(debate_analysis.eigenvalues[:15], 'o-', color='green', linewidth=2)\n",
    "axes[1, 0].set_title('Debate: Eigenvalue Spectrum')\n",
    "axes[1, 0].set_xlabel('Index')\n",
    "axes[1, 0].set_ylabel('Eigenvalue')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].scatter(range(len(debate_labels)), debate_analysis.fiedler_vector,\n",
    "                   c=debate_labels, cmap='tab10', s=30, alpha=0.7)\n",
    "axes[1, 1].set_title('Debate: Fiedler Vector')\n",
    "axes[1, 1].set_xlabel('Sample')\n",
    "axes[1, 1].set_ylabel('Fiedler Value')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "debate_2d = pca.fit_transform(debate_hidden_states)\n",
    "axes[1, 2].scatter(debate_2d[:, 0], debate_2d[:, 1],\n",
    "                   c=debate_labels, cmap='tab10', s=50, alpha=0.7, edgecolors='black', linewidths=0.5)\n",
    "axes[1, 2].set_title(f'Debate: 2D Projection (Q={debate_analysis.quality_score:.3f})')\n",
    "axes[1, 2].set_xlabel('PC1')\n",
    "axes[1, 2].set_ylabel('PC2')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visual comparison shows geometric differences between strategies\")\n",
    "print(\"  → Debate should show tighter clusters and larger spectral gap\")\n",
    "print(\"  → Fiedler vector should be more structured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPOTHESIS TESTING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nH1: Multi-agent improves geometric quality\")\n",
    "h1_support = quality_improvement > 0.05\n",
    "print(f\"   Improvement: {quality_improvement:+.4f}\")\n",
    "print(f\"   Result: {'✓ SUPPORTED' if h1_support else '✗ NOT SUPPORTED'}\")\n",
    "\n",
    "print(\"\\nH2: Improvement correlates with initial quality\")\n",
    "h2_support = (single_analysis.quality_score < 0.6 and quality_improvement > 0.1) or \\\n",
    "             (single_analysis.quality_score > 0.7 and quality_improvement < 0.05)\n",
    "print(f\"   Single quality: {single_analysis.quality_score:.4f}\")\n",
    "print(f\"   Improvement: {quality_improvement:+.4f}\")\n",
    "print(f\"   Result: {'✓ SUPPORTED' if h2_support else '○ INCONCLUSIVE'}\")\n",
    "\n",
    "print(\"\\nH3: Spectral gap increases with debate\")\n",
    "h3_support = spectral_improvement > 0\n",
    "print(f\"   Improvement: {spectral_improvement:+.4f}\")\n",
    "print(f\"   Result: {'✓ SUPPORTED' if h3_support else '✗ NOT SUPPORTED'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Overall assessment\n",
    "hypotheses_supported = sum([h1_support, h2_support, h3_support])\n",
    "print(f\"\\nHypotheses supported: {hypotheses_supported}/3\")\n",
    "\n",
    "if hypotheses_supported >= 2:\n",
    "    print(\"\\n✓ Strong evidence that multi-agent refines geometric structure\")\n",
    "elif hypotheses_supported == 1:\n",
    "    print(\"\\n○ Weak evidence - need more data\")\n",
    "else:\n",
    "    print(\"\\n✗ Hypotheses not supported - may need different task or approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Cost-Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HARNESS_AVAILABLE:\n",
    "    latency_ratio = debate_result.latency_s / single_result.latency_s\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COST-BENEFIT ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nLatency increase: {latency_ratio:.2f}x\")\n",
    "    print(f\"Geometric quality gain: {quality_improvement:+.4f}\")\n",
    "    print(f\"Accuracy: Single={correct}, Debate={debate_correct}\")\n",
    "    \n",
    "    # Efficiency metric: geometric improvement per cost\n",
    "    efficiency = quality_improvement / latency_ratio if latency_ratio > 0 else 0\n",
    "    print(f\"\\nEfficiency (Δquality / Δlatency): {efficiency:.4f}\")\n",
    "    \n",
    "    if efficiency > 0.1:\n",
    "        print(\"→ Multi-agent is EFFICIENT for this task\")\n",
    "    elif efficiency > 0:\n",
    "        print(\"→ Multi-agent provides marginal benefit\")\n",
    "    else:\n",
    "        print(\"→ Multi-agent is NOT worth the cost for this task\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"⚠ Run with harness to get cost-benefit analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Geometric Structures Differ**: Single vs. debate show measurable geometric differences\n",
    "2. **Quality Improvement**: Debate refines (or doesn't) geometric structure\n",
    "3. **Predictive Power**: Initial geometric quality may predict multi-agent benefit\n",
    "\n",
    "### Key Metrics:\n",
    "\n",
    "- Single model quality: [value]\n",
    "- Debate quality: [value]  \n",
    "- Improvement: [value]\n",
    "- Efficiency: [value]\n",
    "\n",
    "### Next Experiments:\n",
    "\n",
    "1. **More tasks**: Test on 20-30 different tasks\n",
    "2. **Manager-worker**: Add third strategy for comparison\n",
    "3. **Round evolution**: Track geometric changes across debate rounds\n",
    "4. **Real extraction**: Replace simulated with actual hidden states\n",
    "5. **Build predictor**: Train model to predict benefit from geometry\n",
    "\n",
    "---\n",
    "\n",
    "**Status**: Framework validated | Need real hidden states for conclusive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': MODEL,\n",
    "    'task': task['difficulty'],\n",
    "    'single': {\n",
    "        'quality_score': float(single_analysis.quality_score),\n",
    "        'spectral_gap': float(single_analysis.spectral_gap),\n",
    "        'cluster_coherence': float(single_analysis.cluster_coherence)\n",
    "    },\n",
    "    'debate': {\n",
    "        'quality_score': float(debate_analysis.quality_score),\n",
    "        'spectral_gap': float(debate_analysis.spectral_gap),\n",
    "        'cluster_coherence': float(debate_analysis.cluster_coherence)\n",
    "    },\n",
    "    'improvements': {\n",
    "        'quality': float(quality_improvement),\n",
    "        'spectral_gap': float(spectral_improvement),\n",
    "        'coherence': float(coherence_improvement)\n",
    "    },\n",
    "    'hypotheses': {\n",
    "        'h1_quality_improves': h1_support,\n",
    "        'h2_correlation': h2_support,\n",
    "        'h3_spectral_increases': h3_support\n",
    "    }\n",
    "}\n",
    "\n",
    "output_dir = Path('../experiments/multi_agent')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_dir / 'comparison_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved to {output_dir / 'comparison_results.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
